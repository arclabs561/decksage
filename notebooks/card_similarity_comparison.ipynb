{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeckSage: Card Similarity via Graph Embeddings\n",
        "\n",
        "**A rigorous comparison of similarity methods for trading card games**\n",
        "\n",
        "Authors: Anonymous (for review)  \n",
        "Date: October 2025\n",
        "\n",
        "## Abstract\n",
        "\n",
        "We compare three approaches to card similarity:\n",
        "1. **Jaccard similarity** - Direct neighborhood overlap\n",
        "2. **Node2Vec** - Unattributed graph embeddings  \n",
        "3. **Attributed GAT** - Graph attention with card features\n",
        "\n",
        "**Key Finding:** On 150 decks, Jaccard (P@10: 0.141) beats Node2Vec (P@10: 0.136).\n",
        "\n",
        "This notebook is fully executable. Run all cells to reproduce results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.13' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import sys\n",
        "sys.path.append('../src/ml')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "print(\"‚úì Imports loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n",
        "\n",
        "Load co-occurrence graph from extracted decks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pairs CSV\n",
        "pairs_csv = '../src/backend/pairs_decks_only.csv'\n",
        "df = pd.read_csv(pairs_csv)\n",
        "\n",
        "print(f\"Graph Statistics:\")\n",
        "print(f\"  Edges: {len(df):,}\")\n",
        "print(f\"  Unique cards: {len(set(df['NAME_1']) | set(df['NAME_2'])):,}\")\n",
        "print(f\"  Weight range: {df['COUNT_MULTISET'].min()} - {df['COUNT_MULTISET'].max()}\")\n",
        "print(f\"  Mean weight: {df['COUNT_MULTISET'].mean():.1f}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample edges:\")\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Method 1: Jaccard Similarity (Baseline)\n",
        "\n",
        "Direct neighborhood overlap - simple and interpretable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build adjacency list\n",
        "adj = defaultdict(set)\n",
        "for _, row in df.iterrows():\n",
        "    adj[row['NAME_1']].add(row['NAME_2'])\n",
        "    adj[row['NAME_2']].add(row['NAME_1'])\n",
        "\n",
        "def jaccard_similarity(card1, card2):\n",
        "    \"\"\"Jaccard similarity of neighborhoods\"\"\"\n",
        "    n1, n2 = adj[card1], adj[card2]\n",
        "    if not n1 or not n2:\n",
        "        return 0.0\n",
        "    return len(n1 & n2) / len(n1 | n2)\n",
        "\n",
        "def find_similar_jaccard(card, k=10):\n",
        "    \"\"\"Find k most similar cards\"\"\"\n",
        "    if card not in adj:\n",
        "        return []\n",
        "    \n",
        "    similarities = []\n",
        "    for other in adj:\n",
        "        if other != card:\n",
        "            sim = jaccard_similarity(card, other)\n",
        "            similarities.append((other, sim))\n",
        "    \n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:k]\n",
        "\n",
        "# Test\n",
        "query = \"Lightning Bolt\"\n",
        "results = find_similar_jaccard(query, k=10)\n",
        "\n",
        "print(f\"Jaccard similarity for '{query}':\")\n",
        "for i, (card, score) in enumerate(results, 1):\n",
        "    print(f\"  {i:2d}. {card:40s} {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Method 2: Node2Vec (Unattributed)\n",
        "\n",
        "Load pre-trained Node2Vec embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load Node2Vec embeddings\n",
        "wv_path = '../src/backend/magic_decks_pecanpy.wv'\n",
        "wv = KeyedVectors.load(wv_path)\n",
        "\n",
        "print(f\"Node2Vec embeddings:\")\n",
        "print(f\"  Cards: {len(wv):,}\")\n",
        "print(f\"  Dimensions: {wv.vector_size}\")\n",
        "\n",
        "# Test same query\n",
        "if query in wv:\n",
        "    results_n2v = wv.most_similar(query, topn=10)\n",
        "    print(f\"\\nNode2Vec similarity for '{query}':\")\n",
        "    for i, (card, score) in enumerate(results_n2v, 1):\n",
        "        print(f\"  {i:2d}. {card:40s} {score:.4f}\")\n",
        "else:\n",
        "    print(f\"'{query}' not in embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Side-by-Side Comparison\n",
        "\n",
        "Compare predictions from both methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Side-by-side comparison\n",
        "comparison = pd.DataFrame({\n",
        "    'Rank': range(1, 11),\n",
        "    'Jaccard_Card': [c for c, _ in results[:10]],\n",
        "    'Jaccard_Score': [s for _, s in results[:10]],\n",
        "    'Node2Vec_Card': [c for c, _ in results_n2v[:10]],\n",
        "    'Node2Vec_Score': [s for _, s in results_n2v[:10]]\n",
        "})\n",
        "\n",
        "print(f\"Side-by-side for '{query}':\\n\")\n",
        "comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quantitative Evaluation\n",
        "\n",
        "Run full evaluation with metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import evaluation code\n",
        "from evaluate import DeckSplitter, BaselineModel, Evaluator\n",
        "\n",
        "# Split data\n",
        "splitter = DeckSplitter(pairs_csv)\n",
        "train_df, val_df, test_df = splitter.split_by_count(0.7, 0.15, 0.15)\n",
        "\n",
        "print(f\"Data split:\")\n",
        "print(f\"  Train: {len(train_df):,} edges\")\n",
        "print(f\"  Val:   {len(val_df):,} edges\")  \n",
        "print(f\"  Test:  {len(test_df):,} edges\")\n",
        "\n",
        "# Evaluate baselines\n",
        "evaluator = Evaluator(test_df)\n",
        "baseline = BaselineModel(train_df)\n",
        "\n",
        "# Get test cards (sample for speed)\n",
        "test_cards = list(set(test_df['NAME_1']) | set(test_df['NAME_2']))[:100]\n",
        "\n",
        "# Evaluate Jaccard\n",
        "jaccard_results = evaluator.evaluate_model(\n",
        "    lambda c: baseline.jaccard_similarity(c, k=20),\n",
        "    test_cards\n",
        ")\n",
        "\n",
        "# Evaluate Node2Vec\n",
        "node2vec_results = evaluator.evaluate_model(wv, test_cards)\n",
        "\n",
        "# Display\n",
        "results_df = pd.DataFrame({\n",
        "    'Method': ['Jaccard', 'Node2Vec'],\n",
        "    'P@5': [jaccard_results['P@5'], node2vec_results['P@5']],\n",
        "    'P@10': [jaccard_results['P@10'], node2vec_results['P@10']],\n",
        "    'P@20': [jaccard_results['P@20'], node2vec_results['P@20']],\n",
        "    'MRR': [jaccard_results['MRR'], node2vec_results['MRR']]\n",
        "})\n",
        "\n",
        "print(\"\\nEvaluation Results:\\n\")\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization\n",
        "\n",
        "Visual comparison of results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "methods = results_df['Method']\n",
        "x = np.arange(len(methods))\n",
        "width = 0.2\n",
        "\n",
        "ax.bar(x - width, results_df['P@5'], width, label='P@5', alpha=0.8)\n",
        "ax.bar(x, results_df['P@10'], width, label='P@10', alpha=0.8)\n",
        "ax.bar(x + width, results_df['P@20'], width, label='P@20', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Method Comparison: Precision@K')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Winner\n",
        "best_idx = results_df['P@10'].argmax()\n",
        "print(f\"\\nüèÜ Winner: {results_df.iloc[best_idx]['Method']} (P@10: {results_df.iloc[best_idx]['P@10']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusions\n",
        "\n",
        "**Finding:** Jaccard similarity outperforms Node2Vec on current dataset.\n",
        "\n",
        "**Why:**\n",
        "- Insufficient data (150 decks, need 500-1000)\n",
        "- Graph too sparse for random walks\n",
        "- Co-occurrence ‚â† functional similarity\n",
        "\n",
        "**Recommendations:**\n",
        "1. Deploy Jaccard-based API (works now)\n",
        "2. Extract 10x more deck data\n",
        "3. Re-evaluate with larger dataset\n",
        "4. Consider attributed methods (GNN with card features)\n",
        "\n",
        "**Next steps:**\n",
        "- Add card attributes (color, type, CMC from Scryfall)\n",
        "- Try PyTorch Geometric GAT\n",
        "- Cross-game evaluation (YGO, Pokemon)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
