#!/usr/bin/env bash # /// script # requires-python = ">=3.11" # /// # # Comprehensive downstream task evaluation with task-aware instructions # # This script runs a complete evaluation of all downstream tasks using # task-specific instructions for instruction-tuned embeddings. # # Usage: # ./scripts/run_comprehensive_downstream_eval.sh [--game magic] [--quick] # # Options: # --game: Game to evaluate (magic, pokemon, yugioh) [default: magic] # --quick: Run quick evaluation with limited test data [default: false] # --compare: Compare with previous evaluation results [default: false] # --output: Output directory [default: experiments/downstream_eval_$(date +%Y%m%d_%H%M%S)] set -euo pipefail SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)" PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)" cd "$PROJECT_ROOT" # Default arguments GAME="magic" QUICK=false COMPARE=false OUTPUT_DIR="experiments/downstream_eval_$(date +%Y%m%d_%H%M%S)" # Parse arguments while [[ $# -gt 0 ]]; do case $1 in --game) GAME="$2" shift 2 ;; --quick) QUICK=true shift ;; --compare) COMPARE=true shift ;; --output) OUTPUT_DIR="$2" shift 2 ;; *) echo "Unknown option: $1" exit 1 ;; esac done echo "═══════════════════════════════════════════════════════════════════════" echo "COMPREHENSIVE DOWNSTREAM TASK EVALUATION" echo "═══════════════════════════════════════════════════════════════════════" echo "" echo "Game: $GAME" echo "Quick mode: $QUICK" echo "Compare with previous: $COMPARE" echo "Output directory: $OUTPUT_DIR" echo "" # Create output directory mkdir -p "$OUTPUT_DIR" # Find embeddings EMBEDDINGS_PATH="" for path in \ "data/embeddings/game_specific/${GAME}_128d_test_pecanpy.wv" \ "data/embeddings/trained/${GAME}_128d_test_pecanpy.wv" \ "data/embeddings/trained/${GAME}_production.wv" \ "data/embeddings/trained/multigame_128d.wv" \ "data/embeddings/${GAME}_128d_test_pecanpy.wv" do if [[ -f "$path" ]]; then EMBEDDINGS_PATH="$path" echo "✓ Found embeddings: $EMBEDDINGS_PATH" break fi done if [[ -z "$EMBEDDINGS_PATH" ]]; then echo "Error: No embeddings found. Please train embeddings first." exit 1 fi # Find test sets TEST_SUBSTITUTIONS="" TEST_DECKS="" TEST_CONTEXTUAL="" # Try unified test sets first for path in \ "experiments/test_set_unified_${GAME}.json" \ "data/test_set_unified_${GAME}.json" \ "experiments/test_set_canonical_${GAME}.json" do if [[ -f "$path" ]]; then TEST_SUBSTITUTIONS="$path" echo "✓ Found test set: $TEST_SUBSTITUTIONS" break fi done # Find test decks (JSONL format) for path in \ "data/processed/decks_all_final.jsonl" \ "data/processed/decks_all_enhanced.jsonl" \ "data/processed/decks_${GAME}.jsonl" do if [[ -f "$path" ]]; then TEST_DECKS="$path" echo "✓ Found test decks: $TEST_DECKS" break fi done # Build evaluation command (use uv run for proper environment) EVAL_CMD=( uv run python src/ml/scripts/evaluate_downstream_complete.py --game "$GAME" --embeddings "$EMBEDDINGS_PATH" --output "$OUTPUT_DIR/results.json" ) if [[ -n "$TEST_SUBSTITUTIONS" ]]; then EVAL_CMD+=(--test-substitutions "$TEST_SUBSTITUTIONS") fi if [[ -n "$TEST_DECKS" ]]; then EVAL_CMD+=(--test-decks "$TEST_DECKS") fi if [[ -n "$TEST_CONTEXTUAL" ]]; then EVAL_CMD+=(--test-contextual "$TEST_CONTEXTUAL") fi # Run evaluation echo "" echo "Running evaluation..." echo "Command: ${EVAL_CMD[*]}" echo "" "${EVAL_CMD[@]}" 2>&1 | tee "$OUTPUT_DIR/evaluation.log" EVAL_EXIT_CODE=${PIPESTATUS[0]} if [[ $EVAL_EXIT_CODE -ne 0 ]]; then echo "Error: Evaluation failed with exit code $EVAL_EXIT_CODE" exit $EVAL_EXIT_CODE fi echo "" echo " Evaluation complete!" echo "Results saved to: $OUTPUT_DIR/results.json" echo "Log saved to: $OUTPUT_DIR/evaluation.log" # Compare with previous results if requested if [[ "$COMPARE" == "true" ]]; then echo "" echo "Comparing with previous results..." PREVIOUS_RESULTS="" for path in experiments/downstream_eval_*/results.json; do if [[ -f "$path" && "$path" != "$OUTPUT_DIR/results.json" ]]; then PREVIOUS_RESULTS="$path" break fi done if [[ -n "$PREVIOUS_RESULTS" ]]; then python -c " import json import sys with open('$OUTPUT_DIR/results.json') as f: current = json.load(f) with open('$PREVIOUS_RESULTS') as f: previous = json.load(f) print('Comparison: Current vs Previous') print('=' * 70) for task_name in ['completion', 'refinement', 'substitution', 'contextual_discovery']: if task_name in current.get('tasks', {}) and task_name in previous.get('tasks', {}): curr = current['tasks'][task_name] prev = previous['tasks'][task_name] print(f'\n{task_name.upper()}:') # Compare key metrics for metric in ['p_at_10', 'p_at_5', 'p_at_1', 'avg_rank', 'avg_steps', 'avg_quality_after']: if metric in curr and metric in prev: curr_val = curr[metric] prev_val = prev[metric] if isinstance(curr_val, (int, float)) and isinstance(prev_val, (int, float)): diff = curr_val - prev_val pct = (diff / prev_val * 100) if prev_val != 0 else 0 symbol = '↑' if diff > 0 else '↓' if diff < 0 else '=' print(f' {metric}: {curr_val:.4f} vs {prev_val:.4f} ({symbol} {diff:+.4f}, {pct:+.2f}%)') " else echo "No previous results found for comparison" fi fi # Generate summary report echo "" echo "Generating summary report..." uv run python -c " import json from pathlib import Path with open('$OUTPUT_DIR/results.json') as f: results = json.load(f) report = [] report.append('═══════════════════════════════════════════════════════════════════════') report.append('EVALUATION SUMMARY') report.append('═══════════════════════════════════════════════════════════════════════') report.append('') report.append(f'Game: {results.get(\"game\", \"unknown\")}') report.append('') tasks = results.get('tasks', {}) for task_name, task_results in tasks.items(): report.append(f'{task_name.upper()}:') if 'error' in task_results: report.append(f' Error: Error: {task_results[\"error\"]}') else: # Print key metrics for key, value in task_results.items(): if isinstance(value, (int, float)) and not isinstance(value, bool): if 'p_at' in key or 'avg' in key or 'rank' in key: report.append(f' {key}: {value:.4f}') elif 'total' in key or 'found' in key or 'completed' in key: report.append(f' {key}: {value}') report.append('') critique = results.get('critique', {}) if critique: report.append('PERFORMANCE CRITIQUE:') report.append(f' Overall: {critique.get(\"overall_assessment\", \"N/A\")}') if critique.get('strengths'): report.append(' Strengths:') for s in critique['strengths']: report.append(f' + {s}') if critique.get('weaknesses'): report.append(' Weaknesses:') for w in critique['weaknesses']: report.append(f' - {w}') if critique.get('recommendations'): report.append(' Recommendations:') for r in critique['recommendations']: report.append(f' → {r}') report_text = '\n'.join(report) print(report_text) with open('$OUTPUT_DIR/summary.txt', 'w') as f: f.write(report_text) " echo "" echo " Summary report saved to: $OUTPUT_DIR/summary.txt" echo "" echo "═══════════════════════════════════════════════════════════════════════" echo "EVALUATION COMPLETE" echo "═══════════════════════════════════════════════════════════════════════" echo "" echo "Results: $OUTPUT_DIR/results.json" echo "Summary: $OUTPUT_DIR/summary.txt" echo "Log: $OUTPUT_DIR/evaluation.log" echo ""