#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "gensim>=4.3.0", # ] # /// """ Compare embeddings trained with annotations vs baseline. Generates comprehensive comparison report with: - P@10, MRR metrics - Downstream task performance - Improvement percentages """ from __future__ import annotations import json import logging from pathlib import Path from typing import Any try: from gensim.models import KeyedVectors HAS_GENSIM = True except ImportError: HAS_GENSIM = False from ml.scripts.evaluate_all_embeddings import evaluate_embedding from ml.utils.data_loading import load_test_set from ml.utils.paths import PATHS logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def compare_embeddings( baseline_path: Path, annotated_path: Path, test_set_path: Path, output_path: Path | None = None, ) -> dict[str, Any]: """Compare baseline vs annotated embeddings.""" if not HAS_GENSIM: raise ImportError("gensim required") logger.info("=" * 70) logger.info("EMBEDDING COMPARISON: BASELINE vs ANNOTATED") logger.info("=" * 70) logger.info() # Load test set test_set = load_test_set(test_set_path) logger.info(f"Loaded test set: {len(test_set)} queries") logger.info() # Load embeddings logger.info(f"Loading baseline: {baseline_path}") baseline_wv = KeyedVectors.load(str(baseline_path)) logger.info(f" Vocabulary: {len(baseline_wv)} cards") logger.info(f"Loading annotated: {annotated_path}") annotated_wv = KeyedVectors.load(str(annotated_path)) logger.info(f" Vocabulary: {len(annotated_wv)} cards") logger.info() # Evaluate both logger.info("Evaluating baseline embeddings...") baseline_metrics = evaluate_embedding( baseline_wv, test_set, top_k=10, per_query=False, verbose=True, ) logger.info("") logger.info("Evaluating annotated embeddings...") annotated_metrics = evaluate_embedding( annotated_wv, test_set, top_k=10, per_query=False, verbose=True, ) # Compute improvements p10_improvement = annotated_metrics['p@10'] - baseline_metrics['p@10'] p10_improvement_pct = (p10_improvement / baseline_metrics['p@10'] * 100) if baseline_metrics['p@10'] > 0 else 0.0 mrr_improvement = annotated_metrics['mrr'] - baseline_metrics['mrr'] mrr_improvement_pct = (mrr_improvement / baseline_metrics['mrr'] * 100) if baseline_metrics['mrr'] > 0 else 0.0 comparison = { "baseline": { "path": str(baseline_path), "vocab_size": len(baseline_wv), "p@10": baseline_metrics['p@10'], "mrr": baseline_metrics['mrr'], "num_queries": baseline_metrics['num_evaluated'], "vocab_coverage": baseline_metrics.get('vocab_coverage', 0.0), }, "annotated": { "path": str(annotated_path), "vocab_size": len(annotated_wv), "p@10": annotated_metrics['p@10'], "mrr": annotated_metrics['mrr'], "num_queries": annotated_metrics['num_evaluated'], "vocab_coverage": annotated_metrics.get('vocab_coverage', 0.0), }, "improvements": { "p@10": { "absolute": p10_improvement, "percent": p10_improvement_pct, }, "mrr": { "absolute": mrr_improvement, "percent": mrr_improvement_pct, }, }, "test_set": str(test_set_path), } # Print summary logger.info("") logger.info("=" * 70) logger.info("COMPARISON SUMMARY") logger.info("=" * 70) logger.info() logger.info("Baseline:") logger.info(f" P@10: {baseline_metrics['p@10']:.4f}") logger.info(f" MRR: {baseline_metrics['mrr']:.4f}") logger.info() logger.info("Annotated (with substitution pairs):") logger.info(f" P@10: {annotated_metrics['p@10']:.4f}") logger.info(f" MRR: {annotated_metrics['mrr']:.4f}") logger.info() logger.info("Improvements:") logger.info(f" P@10: {p10_improvement:+.4f} ({p10_improvement_pct:+.1f}%)") logger.info(f" MRR: {mrr_improvement:+.4f} ({mrr_improvement_pct:+.1f}%)") logger.info() # Save results if output_path: with open(output_path, 'w') as f: json.dump(comparison, f, indent=2) logger.info(f" Saved comparison to {output_path}") return comparison def main() -> int: """Main entry point.""" import argparse parser = argparse.ArgumentParser(description="Compare baseline vs annotated embeddings") parser.add_argument( "--baseline", type=Path, default=PATHS.embeddings / "production.wv", help="Path to baseline embeddings", ) parser.add_argument( "--annotated", type=Path, default=PATHS.embeddings / "multitask_with_combined_annotations.wv", help="Path to annotated embeddings", ) parser.add_argument( "--test-set", type=Path, default=PATHS.test_magic, help="Path to test set", ) parser.add_argument( "--output", type=Path, default=PATHS.experiments / "embedding_comparison.json", help="Output path for comparison JSON", ) args = parser.parse_args() try: comparison = compare_embeddings( args.baseline, args.annotated, args.test_set, args.output, ) return 0 except Exception as e: logger.error(f"Error: {e}", exc_info=True) return 1 if __name__ == "__main__": import sys sys.exit(main())