#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pydantic-ai>=0.0.12", # ] # /// """ Generate multi-judge annotations for IAA tracking. Uses multiple LLM perspectives to generate annotations and compute inter-annotator agreement metrics. """ from __future__ import annotations import argparse import json import sys from pathlib import Path from typing import Any # Add src to path script_dir = Path(__file__).parent project_root = script_dir.parent src_dir = project_root / "src" if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) try: from ml.experimental.multi_perspective_judge import MultiPerspectiveJudge HAS_MULTI_JUDGE = True except ImportError: HAS_MULTI_JUDGE = False print("Warning: multi_perspective_judge not available") try: from ml.utils.paths import PATHS EXPERIMENTS_DIR = PATHS.EXPERIMENTS_DIR except ImportError: EXPERIMENTS_DIR = project_root / "experiments" def load_test_set_queries(test_set_path: Path, max_queries: int | None = None) -> list[str]: """Load queries from test set.""" with open(test_set_path) as f: data = json.load(f) queries = data.get("queries", {}) if isinstance(queries, dict): query_list = list(queries.keys()) else: query_list = [q.get("query", "") for q in queries if isinstance(q, dict)] if max_queries: query_list = query_list[:max_queries] return query_list def generate_candidates_for_query(query: str, top_k: int = 20) -> list[str]: """Generate candidate cards for a query (placeholder - should use actual similarity search).""" # TODO: Use actual embedding similarity or API call # For now, return empty list (should be implemented) return [] def main() -> int: parser = argparse.ArgumentParser(description="Generate multi-judge annotations") parser.add_argument("--test-set", type=Path, required=True, help="Test set JSON file") parser.add_argument("--output", type=Path, help="Output JSONL file") parser.add_argument("--num-judges", type=int, default=3, help="Number of judges (perspectives)") parser.add_argument("--max-queries", type=int, help="Maximum queries to process") parser.add_argument("--perspectives", nargs="+", default=["competitive", "rules", "meta"], help="Perspectives to use") args = parser.parse_args() if not HAS_MULTI_JUDGE: print("Error: Multi-perspective judge not available") return 1 # Load queries queries = load_test_set_queries(args.test_set, args.max_queries) print(f"Processing {len(queries)} queries...") # Initialize judge judge = MultiPerspectiveJudge() # Output path if args.output: output_path = args.output else: output_dir = EXPERIMENTS_DIR / "annotations_multi_judge" output_dir.mkdir(parents=True, exist_ok=True) output_path = output_dir / f"multi_judge_{args.test_set.stem}.jsonl" # Generate annotations all_annotations = [] for i, query in enumerate(queries, 1): print(f"[{i}/{len(queries)}] Processing: {query}") # Get candidates (TODO: use actual similarity search) candidates = generate_candidates_for_query(query, top_k=20) if not candidates: print(f" Warning: No candidates for {query}, skipping") continue # Get multi-perspective judgments judgments = judge.judge_multi_perspective( query_card=query, candidates=candidates, perspectives=args.perspectives, ) # Add query metadata judgments["query"] = query judgments["candidates"] = candidates all_annotations.append(judgments) # Save incrementally with open(output_path, "a") as f: f.write(json.dumps(judgments) + "\n") print(f"\n Generated {len(all_annotations)} multi-judge annotations") print(f" Saved to: {output_path}") # Compute IAA summary if all_annotations: print("\nIAA Summary:") print(f" Total queries: {len(all_annotations)}") print(f" Perspectives used: {args.perspectives}") print(f" Average judgments per query: {sum(len(j.get('evaluations', [])) for j in all_annotations) / len(all_annotations):.1f}") return 0 if __name__ == "__main__": sys.exit(main())
