#!/bin/bash
# Final verification that everything works as documented

set -euo pipefail echo "╔═══════════════════════════════════════════════════════════════════╗" echo "║ FINAL VERIFICATION RUN ║" echo "╚═══════════════════════════════════════════════════════════════════╝" echo "" export $(cat .env | grep -v '^#' | xargs) echo "1️⃣ Verify API key loaded..." if [ -z "$OPENROUTER_API_KEY" ]; then echo " Error: OPENROUTER_API_KEY not set" exit 1 fi echo " API key present" echo "" echo "2️⃣ Run fast tests (no LLM calls)..." uv run pytest src/ml/tests/test_integration_complete.py -q STATUS_FAST=$? echo "" echo "3️⃣ Run ONE real LLM test..." uv run pytest src/ml/tests/test_llm_validators_real.py::test_llm_judge_actually_works -v STATUS_LLM=$? echo "" echo "4️⃣ Quick LLM judge smoke test..." cd src/ml && uv run python -c " import sys sys.path.insert(0, '.') from experimental.llm_judge import LLMJudge judge = LLMJudge(model='openai/gpt-4o-mini') result = judge.evaluate_similarity('Test', [('Similar', 0.9)]) assert result['overall_quality'] is not None print(f' LLM Judge works: Quality {result[\"overall_quality\"]}/10') " STATUS_SMOKE=$? cd ../.. echo "" echo "═══════════════════════════════════════════════════════════════════" echo " VERIFICATION RESULTS" echo "═══════════════════════════════════════════════════════════════════" echo "" if [ $STATUS_FAST -eq 0 ] && [ $STATUS_LLM -eq 0 ] && [ $STATUS_SMOKE -eq 0 ]; then echo " ALL VERIFICATIONS PASSED" echo "" echo "System is ready to use:" echo " • LLM validators working" echo " • Tests passing" echo " • OpenRouter connected" echo "" echo "Known limitations:" echo " • No caching (see docs for why)" echo " • ~10s per LLM call" echo " • ~\$0.01 per call" echo "" echo "Grade: B (good enough)" exit 0 else echo "Error: SOME VERIFICATIONS FAILED" echo " Fast tests: $([ $STATUS_FAST -eq 0 ] && echo '' || echo 'Error:')" echo " LLM tests: $([ $STATUS_LLM -eq 0 ] && echo '' || echo 'Error:')" echo " Smoke test: $([ $STATUS_SMOKE -eq 0 ] && echo '' || echo 'Error:')" exit 1 fi