#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas", # ] # /// """ Validate dataset consistency and quality. Checks: - Card names in pairs match card attributes - Test set queries exist in card attributes - Deck files are non-empty - No duplicate pairs - Embeddings match expected vocabulary """ from __future__ import annotations import argparse import csv import json import sys from collections import Counter from pathlib import Path from typing import Any try: import pandas as pd HAS_PANDAS = True except ImportError: HAS_PANDAS = False def check_card_attributes_coverage( pairs_csv: Path, attributes_csv: Path, ) -> list[str]: """Check that card names in pairs exist in attributes.""" issues = [] print("Loading card attributes...") if HAS_PANDAS: attrs_df = pd.read_csv(attributes_csv) card_names = set(attrs_df['name'].str.strip().str.lower()) else: # Fallback: read CSV manually card_names = set() with open(attributes_csv) as f: reader = csv.DictReader(f) for row in reader: name = row.get('name', '').strip().lower() if name: card_names.add(name) print(f"Found {len(card_names)} cards in attributes") print("Checking pairs...") missing_cards = set() with open(pairs_csv) as f: reader = csv.DictReader(f) for i, row in enumerate(reader): if i % 100000 == 0 and i > 0: print(f" Processed {i:,} pairs...") n1 = row.get('NAME_1', '').strip().lower() n2 = row.get('NAME_2', '').strip().lower() if n1 and n1 not in card_names: missing_cards.add(n1) if n2 and n2 not in card_names: missing_cards.add(n2) if i >= 1000000: # Sample break if missing_cards: issues.append(f"Found {len(missing_cards)} cards in pairs not in attributes (sample)") if len(missing_cards) <= 20: issues.append(f" Examples: {', '.join(list(missing_cards)[:10])}") return issues def check_test_set_coverage( test_set_path: Path, attributes_csv: Path, ) -> list[str]: """Check that test set queries exist in card attributes.""" issues = [] print(f"Loading test set: {test_set_path}") with open(test_set_path) as f: test_set = json.load(f) queries = test_set.get("queries", test_set) if isinstance(test_set.get("queries"), dict) else test_set.get("queries", []) if isinstance(queries, dict): query_names = set(queries.keys()) else: query_names = {q.get("query", "") for q in queries if isinstance(q, dict)} print(f"Found {len(query_names)} queries") print("Loading card attributes...") if HAS_PANDAS: attrs_df = pd.read_csv(attributes_csv) card_names = set(attrs_df['name'].str.strip().str.lower()) else: # Fallback: read CSV manually card_names = set() with open(attributes_csv) as f: reader = csv.DictReader(f) for row in reader: name = row.get('name', '').strip().lower() if name: card_names.add(name) missing_queries = {q for q in query_names if q.strip().lower() not in card_names} if missing_queries: issues.append(f"Found {len(missing_queries)} queries not in card attributes") issues.append(f" Missing: {', '.join(list(missing_queries)[:10])}") return issues def check_deck_files() -> list[str]: """Check that deck files are non-empty.""" issues = [] deck_files = [ Path("data/decks/yugioh_decks.jsonl"), Path("data/processed/decks_pokemon.jsonl"), ] for deck_file in deck_files: if not deck_file.exists(): issues.append(f"Deck file missing: {deck_file}") continue with open(deck_file) as f: count = sum(1 for _ in f) if count == 0: issues.append(f"Deck file is empty: {deck_file}") else: print(f"{deck_file.name}: {count} decks") return issues def check_duplicate_pairs(pairs_csv: Path) -> list[str]: """Check for duplicate pairs.""" issues = [] print(f"Checking for duplicate pairs in {pairs_csv.name}...") seen = set() duplicates = [] with open(pairs_csv) as f: reader = csv.DictReader(f) for i, row in enumerate(reader): if i % 100000 == 0 and i > 0: print(f" Processed {i:,} pairs...") n1 = row.get('NAME_1', '').strip() n2 = row.get('NAME_2', '').strip() # Normalize pair (order-independent) pair = tuple(sorted([n1, n2])) if pair in seen: duplicates.append(pair) seen.add(pair) if i >= 1000000: # Sample break if duplicates: issues.append(f"Found {len(duplicates)} duplicate pairs (sample)") return issues def check_multi_game_claim(pairs_csv: Path) -> list[str]: """Check if pairs_multi_game.csv actually contains multiple games.""" issues = [] if "multi_game" not in pairs_csv.name: return issues print(f"Checking game distribution in {pairs_csv.name}...") games = Counter() with open(pairs_csv) as f: reader = csv.DictReader(f) for i, row in enumerate(reader): g1 = row.get('GAME_1', '').strip() g2 = row.get('GAME_2', '').strip() games[g1] += 1 games[g2] += 1 if i >= 100000: # Sample break unique_games = {g for g in games.keys() if g} if len(unique_games) <= 1: issues.append(f"File named 'multi_game' but only contains: {unique_games}") issues.append(f" Consider renaming to reflect single-game content") else: print(f" Found games: {sorted(unique_games)}") return issues def main() -> int: """Run dataset validation.""" parser = argparse.ArgumentParser(description="Validate dataset consistency") parser.add_argument("--pairs", type=Path, help="Pairs CSV to validate") parser.add_argument("--test-set", type=Path, help="Test set JSON to validate") parser.add_argument("--all", action="store_true", help="Run all checks") args = parser.parse_args() all_issues = [] if args.all or not args.pairs: # Run all checks print("=" * 60) print("Dataset Validation") print("=" * 60) # Check deck files print("\n1. Checking deck files...") issues = check_deck_files() all_issues.extend(issues) # Check multi-game claim print("\n2. Checking multi-game data claim...") multi_game = Path("data/processed/pairs_multi_game.csv") if multi_game.exists(): issues = check_multi_game_claim(multi_game) all_issues.extend(issues) # Check pairs coverage print("\n3. Checking pairs coverage...") pairs_large = Path("data/processed/pairs_large.csv") attrs = Path("data/processed/card_attributes_enriched.csv") if pairs_large.exists() and attrs.exists(): issues = check_card_attributes_coverage(pairs_large, attrs) all_issues.extend(issues) # Check test set coverage print("\n4. Checking test set coverage...") test_sets = [ Path("experiments/test_set_canonical_magic.json"), Path("experiments/test_set_canonical_pokemon.json"), Path("experiments/test_set_canonical_yugioh.json"), ] for test_set in test_sets: if test_set.exists() and attrs.exists(): issues = check_test_set_coverage(test_set, attrs) all_issues.extend(issues) # Check duplicates print("\n5. Checking for duplicate pairs...") if pairs_large.exists(): issues = check_duplicate_pairs(pairs_large) all_issues.extend(issues) else: # Run specific checks if args.pairs: attrs = Path("data/processed/card_attributes_enriched.csv") if attrs.exists(): issues = check_card_attributes_coverage(args.pairs, attrs) all_issues.extend(issues) if args.test_set: attrs = Path("data/processed/card_attributes_enriched.csv") if attrs.exists(): issues = check_test_set_coverage(args.test_set, attrs) all_issues.extend(issues) # Report results print("\n" + "=" * 60) print("Validation Results") print("=" * 60) if all_issues: print(f"\nFound {len(all_issues)} issues:") for issue in all_issues: print(f" Warning: {issue}") return 1 else: print("\n No issues found!") return 0 if __name__ == "__main__": sys.exit(main())