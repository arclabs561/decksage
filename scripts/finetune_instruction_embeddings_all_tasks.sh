#!/usr/bin/env bash # Fine-tune instruction embeddings on all downstream tasks set -euo pipefail SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)" PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)" cd "$PROJECT_ROOT" # Configuration BASE_MODEL="${BASE_MODEL:-intfloat/e5-base-v2}" OUTPUT_DIR="${OUTPUT_DIR:-data/embeddings/instruction_finetuned}" TRAINING_DATA="${TRAINING_DATA:-experiments/instruction_finetuning_data.json}" # Training parameters EPOCHS="${EPOCHS:-3}" BATCH_SIZE="${BATCH_SIZE:-16}" LEARNING_RATE="${LEARNING_RATE:-2e-5}" WARMUP_STEPS="${WARMUP_STEPS:-100}" echo "═══════════════════════════════════════════════════════════════════════" echo "FINE-TUNING INSTRUCTION EMBEDDINGS ON ALL TASKS" echo "═══════════════════════════════════════════════════════════════════════" echo "" echo "Base model: $BASE_MODEL" echo "Output directory: $OUTPUT_DIR" echo "Training data: $TRAINING_DATA" echo "Epochs: $EPOCHS, Batch size: $BATCH_SIZE, LR: $LEARNING_RATE" echo "" # Check if training data exists if [ ! -f "$TRAINING_DATA" ]; then echo "Warning: Training data not found: $TRAINING_DATA" echo " Preparing training data first..." python3 scripts/prepare_instruction_finetuning_data.py --output "$TRAINING_DATA" fi # Load training data to check task distribution echo "Checking training data..." python3 -c " import json from pathlib import Path data_path = Path('$TRAINING_DATA') if data_path.exists(): with open(data_path) as f: data = json.load(f) print('Training data distribution:') for task, pairs in data.items(): print(f' {task}: {len(pairs)} pairs') total = sum(len(pairs) for pairs in data.values()) print(f' Total: {total} pairs') else: print('Error: Training data not found') exit(1) " # Fine-tune for each task TASKS=("substitution" "similarity" "completion" "synergy") for task in "${TASKS[@]}"; do echo "" echo "═══════════════════════════════════════════════════════════════════════" echo "FINE-TUNING FOR TASK: $task" echo "═══════════════════════════════════════════════════════════════════════" TASK_OUTPUT_DIR="$OUTPUT_DIR/${task}" mkdir -p "$TASK_OUTPUT_DIR" # Extract task-specific pairs from training data python3 -c " import json from pathlib import Path data_path = Path('$TRAINING_DATA') with open(data_path) as f: all_data = json.load(f) task_pairs = all_data.get('$task', []) if not task_pairs: print(f'Warning: No training data for task: $task') exit(1) # Save task-specific pairs task_pairs_file = Path('experiments/task_${task}_pairs.json') with open(task_pairs_file, 'w') as f: json.dump(task_pairs, f, indent=2) print(f' Extracted {len(task_pairs)} pairs for task: $task') " || continue # Run fine-tuning python3 src/ml/scripts/train_instruction_finetune.py \ --base-model "$BASE_MODEL" \ --output "$TASK_OUTPUT_DIR" \ --multi-task-data "$TRAINING_DATA" \ --task-type "$task" \ --epochs "$EPOCHS" \ --batch-size "$BATCH_SIZE" \ --learning-rate "$LEARNING_RATE" \ --warmup-steps "$WARMUP_STEPS" \ --num-negatives 5 echo " Fine-tuning complete for task: $task" done echo "" echo "═══════════════════════════════════════════════════════════════════════" echo " ALL TASKS FINE-TUNED" echo "═══════════════════════════════════════════════════════════════════════" echo "" echo "Fine-tuned models saved to: $OUTPUT_DIR" echo "" echo "Next steps:" echo " 1. Evaluate fine-tuned models vs zero-shot" echo " 2. Compare performance across tasks" echo " 3. Deploy best model to production"