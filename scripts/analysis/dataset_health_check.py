#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [] # /// """ Comprehensive dataset health check and metrics. Generates a health report with: - Dataset sizes and coverage - Quality metrics - Missing data indicators - Recommendations """ from __future__ import annotations import argparse import json import sys from collections import Counter from datetime import datetime from pathlib import Path from typing import Any def get_file_size_mb(path: Path) -> float: """Get file size in MB.""" if not path.exists(): return 0.0 return path.stat().st_size / (1024 * 1024) def count_jsonl_lines(path: Path) -> int: """Count lines in JSONL file.""" if not path.exists(): return 0 with open(path) as f: return sum(1 for _ in f) def count_csv_rows(path: Path) -> int: """Count rows in CSV file (excluding header).""" if not path.exists(): return 0 import csv with open(path) as f: reader = csv.DictReader(f) return sum(1 for _ in reader) def analyze_test_set(path: Path) -> dict[str, Any]: """Analyze test set structure and coverage.""" if not path.exists(): return {"exists": False} with open(path) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data.get("queries"), dict) else data.get("queries", []) if isinstance(queries, dict): num_queries = len(queries) total_labels = sum( len(v.get("highly_relevant", [])) + len(v.get("relevant", [])) + len(v.get("somewhat_relevant", [])) + len(v.get("marginally_relevant", [])) + len(v.get("irrelevant", [])) for v in queries.values() ) avg_labels_per_query = total_labels / num_queries if num_queries > 0 else 0 else: num_queries = len(queries) total_labels = sum(len(q.get("candidates", [])) for q in queries) avg_labels_per_query = total_labels / num_queries if num_queries > 0 else 0 return { "exists": True, "num_queries": num_queries, "total_labels": total_labels, "avg_labels_per_query": round(avg_labels_per_query, 1), "version": data.get("version", "unknown"), } def check_embedding_health() -> dict[str, Any]: """Check embedding files.""" emb_dir = Path("data/embeddings") if not emb_dir.exists(): return {"exists": False} wv_files = list(emb_dir.glob("*.wv")) pkl_files = list(emb_dir.glob("*.pkl")) total_size_mb = sum(get_file_size_mb(f) for f in wv_files + pkl_files) return { "exists": True, "num_embeddings": len(wv_files) + len(pkl_files), "wv_files": len(wv_files), "pkl_files": len(pkl_files), "total_size_mb": round(total_size_mb, 2), "has_production": any("production" in f.name for f in wv_files), } def generate_health_report() -> dict[str, Any]: """Generate comprehensive health report.""" report = { "timestamp": datetime.now().isoformat(), "datasets": {}, "issues": [], "recommendations": [], } # Check pairs data pairs_large = Path("data/processed/pairs_large.csv") pairs_multi = Path("data/processed/pairs_multi_game.csv") if pairs_large.exists(): rows = count_csv_rows(pairs_large) size_mb = get_file_size_mb(pairs_large) report["datasets"]["pairs_large"] = { "exists": True, "rows": rows, "size_mb": round(size_mb, 2), } else: report["datasets"]["pairs_large"] = {"exists": False} report["issues"].append("pairs_large.csv missing") if pairs_multi.exists(): rows = count_csv_rows(pairs_multi) size_mb = get_file_size_mb(pairs_multi) report["datasets"]["pairs_multi_game"] = { "exists": True, "rows": rows, "size_mb": round(size_mb, 2), } # Check if actually multi-game import csv games = set() with open(pairs_multi) as f: reader = csv.DictReader(f) for i, row in enumerate(reader): games.add(row.get('GAME_1', '').strip()) games.add(row.get('GAME_2', '').strip()) if i >= 10000: break unique_games = {g for g in games if g} if len(unique_games) <= 1: report["issues"].append(f"pairs_multi_game.csv named 'multi_game' but only contains: {unique_games}") else: report["datasets"]["pairs_multi_game"] = {"exists": False} # Check card attributes attrs_enriched = Path("data/processed/card_attributes_enriched.csv") if attrs_enriched.exists(): rows = count_csv_rows(attrs_enriched) size_mb = get_file_size_mb(attrs_enriched) report["datasets"]["card_attributes"] = { "exists": True, "rows": rows, "size_mb": round(size_mb, 2), } else: report["datasets"]["card_attributes"] = {"exists": False} report["issues"].append("card_attributes_enriched.csv missing") # Check deck files yugioh_decks = Path("data/decks/yugioh_decks.jsonl") pokemon_decks = Path("data/processed/decks_pokemon.jsonl") yugioh_count = count_jsonl_lines(yugioh_decks) pokemon_count = count_jsonl_lines(pokemon_decks) report["datasets"]["decks"] = { "yugioh": { "exists": yugioh_decks.exists(), "count": yugioh_count, }, "pokemon": { "exists": pokemon_decks.exists(), "count": pokemon_count, }, } if pokemon_count == 0: report["issues"].append("decks_pokemon.jsonl is empty") if yugioh_count < 100: report["recommendations"].append(f"yugioh_decks.jsonl has only {yugioh_count} decks (consider expanding)") # Check test sets test_sets = { "magic": Path("experiments/test_set_canonical_magic.json"), "pokemon": Path("experiments/test_set_canonical_pokemon.json"), "yugioh": Path("experiments/test_set_canonical_yugioh.json"), } report["datasets"]["test_sets"] = {} targets = {"magic": 50, "pokemon": 25, "yugioh": 25} for game, path in test_sets.items(): analysis = analyze_test_set(path) report["datasets"]["test_sets"][game] = analysis if analysis.get("exists"): current = analysis.get("num_queries", 0) target = targets.get(game, 25) if current < target: report["recommendations"].append( f"{game} test set: {current}/{target} queries (need {target - current} more)") # Check ground truth gt_path = Path("data/processed/ground_truth_v1.json") if gt_path.exists(): with open(gt_path) as f: gt_data = json.load(f) report["datasets"]["ground_truth"] = { "exists": True, "num_queries": len(gt_data), } if len(gt_data) < 20: report["recommendations"].append(f"ground_truth_v1.json has only {len(gt_data)} queries (target: 20+)") else: report["datasets"]["ground_truth"] = {"exists": False} report["issues"].append("ground_truth_v1.json missing") # Check embeddings report["datasets"]["embeddings"] = check_embedding_health() if not report["datasets"]["embeddings"].get("has_production"): report["issues"].append("No production embedding found") # Calculate health score total_checks = 0 passed_checks = 0 # Critical checks if report["datasets"]["pairs_large"].get("exists"): passed_checks += 1 total_checks += 1 if report["datasets"]["card_attributes"].get("exists"): passed_checks += 1 total_checks += 1 if report["datasets"]["test_sets"]["magic"].get("exists"): passed_checks += 1 total_checks += 1 if report["datasets"]["embeddings"].get("exists"): passed_checks += 1 total_checks += 1 health_score = (passed_checks / total_checks) * 100 if total_checks > 0 else 0 report["health_score"] = round(health_score, 1) return report def print_report(report: dict[str, Any]) -> None: """Print formatted health report.""" print("=" * 70) print("Dataset Health Report") print("=" * 70) print(f"Generated: {report['timestamp']}") print(f"Health Score: {report['health_score']}%") print() # Datasets print("Datasets:") print("-" * 70) if "pairs_large" in report["datasets"]: pl = report["datasets"]["pairs_large"] if pl.get("exists"): print(f" pairs_large.csv: {pl['rows']:,} rows, {pl['size_mb']} MB") else: print(" pairs_large.csv: MISSING") if "pairs_multi_game" in report["datasets"]: pm = report["datasets"]["pairs_multi_game"] if pm.get("exists"): print(f" pairs_multi_game.csv: {pm['rows']:,} rows, {pm['size_mb']} MB") else: print(" pairs_multi_game.csv: MISSING") if "card_attributes" in report["datasets"]: ca = report["datasets"]["card_attributes"] if ca.get("exists"): print(f" card_attributes_enriched.csv: {ca['rows']:,} cards, {ca['size_mb']} MB") else: print(" card_attributes_enriched.csv: MISSING") # Decks if "decks" in report["datasets"]: decks = report["datasets"]["decks"] print(f" yugioh_decks.jsonl: {decks['yugioh']['count']} decks") print(f" decks_pokemon.jsonl: {decks['pokemon']['count']} decks") # Test sets if "test_sets" in report["datasets"]: print("\nTest Sets:") print("-" * 70) for game, data in report["datasets"]["test_sets"].items(): if data.get("exists"): print(f" {game}: {data['num_queries']} queries, {data['total_labels']} labels " f"(avg {data['avg_labels_per_query']}/query)") else: print(f" {game}: MISSING") # Embeddings if "embeddings" in report["datasets"]: emb = report["datasets"]["embeddings"] if emb.get("exists"): print(f"\nEmbeddings: {emb['num_embeddings']} files, {emb['total_size_mb']} MB total") if emb.get("has_production"): print(" âœ“ Production embedding found") else: print(" Warning: No production embedding") # Issues if report["issues"]: print("\nIssues:") print("-" * 70) for issue in report["issues"]: print(f" Warning: {issue}") # Recommendations if report["recommendations"]: print("\nRecommendations:") print("-" * 70) for rec in report["recommendations"]: print(f" {rec}") def main() -> int: """Run health check.""" parser = argparse.ArgumentParser(description="Dataset health check") parser.add_argument("--json", type=Path, help="Save report as JSON") parser.add_argument("--quiet", action="store_true", help="Only print health score") args = parser.parse_args() report = generate_health_report() if args.json: with open(args.json, "w") as f: json.dump(report, f, indent=2) print(f"Report saved to {args.json}") if args.quiet: print(f"{report['health_score']}%") else: print_report(report) # Exit code based on health if report["health_score"] < 75: return 1 return 0 if __name__ == "__main__": sys.exit(main())
