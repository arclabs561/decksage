#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # ] # /// """ Evaluate instruction-tuned embeddings (zero-shot vs fine-tuned) on downstream tasks. Compares: - Zero-shot E5-base-v2 (baseline) - Fine-tuned models per task - Task-specific instruction selection impact """ from __future__ import annotations import argparse import json import logging from pathlib import Path from typing import Any try: import pandas as pd import numpy as np HAS_DEPS = True except ImportError: HAS_DEPS = False import sys _script_file = Path(__file__).resolve() _src_dir = _script_file.parent.parent / "src" if str(_src_dir) not in sys.path: sys.path.insert(0, str(_src_dir)) from ml.similarity.instruction_tuned_embeddings import InstructionTunedCardEmbedder from ml.utils.paths import PATHS logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) def evaluate_on_test_set( embedder: InstructionTunedCardEmbedder, test_set_path: Path, instruction_type: str = "substitution", top_k: int = 10, ) -> dict[str, Any]: """ Evaluate embedder on test set. Returns: Dictionary with P@10, MRR, and other metrics """ with open(test_set_path) as f: test_data = json.load(f) queries = test_set_path.get("queries", {}) if not queries: queries = test_data.get("queries", {}) total_queries = 0 found_in_top_10 = 0 reciprocal_ranks = [] for query_card, labels in queries.items(): highly_relevant = labels.get("highly_relevant", []) relevant = labels.get("relevant", []) all_relevant = highly_relevant + relevant if not all_relevant: continue total_queries += 1 # Get similar cards using instruction-tuned embedder try: # Get all cards from test set as candidates all_cards = set() for labels_dict in queries.values(): all_cards.update(labels_dict.get("highly_relevant", [])) all_cards.update(labels_dict.get("relevant", [])) all_cards.update(labels_dict.get("irrelevant", [])) similar = embedder.most_similar( query_card, list(all_cards), topn=top_k, instruction_type=instruction_type, ) similar_cards = [card if isinstance(card, str) else card.get("name", str(card)) for card, _ in similar] # Check if any relevant card is in top-k found = False for i, card in enumerate(similar_cards): if card in all_relevant: found = True reciprocal_ranks.append(1.0 / (i + 1)) break if found: found_in_top_10 += 1 else: reciprocal_ranks.append(0.0) except Exception as e: logger.debug(f"Failed to evaluate query '{query_card}': {e}") reciprocal_ranks.append(0.0) p_at_10 = found_in_top_10 / total_queries if total_queries > 0 else 0.0 mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0 return { "total_queries": total_queries, "found_in_top_10": found_in_top_10, "p_at_10": p_at_10, "mrr": mrr, "instruction_type": instruction_type, } def compare_models( base_model_name: str = "intfloat/e5-base-v2", fine_tuned_path: Path | None = None, test_set_path: Path | None = None, task_type: str = "substitution", ) -> dict[str, Any]: """ Compare zero-shot vs fine-tuned model performance. Returns: Dictionary with comparison results """ if not test_set_path or not test_set_path.exists(): logger.error(f"Test set not found: {test_set_path}") return {} results = { "task_type": task_type, "test_set": str(test_set_path), "models": {}, } # Evaluate zero-shot baseline logger.info(f"Evaluating zero-shot model: {base_model_name}") zero_shot_embedder = InstructionTunedCardEmbedder(model_name=base_model_name) zero_shot_results = evaluate_on_test_set( zero_shot_embedder, test_set_path, instruction_type=task_type, ) results["models"]["zero_shot"] = zero_shot_results # Evaluate fine-tuned model if available if fine_tuned_path and fine_tuned_path.exists(): logger.info(f"Evaluating fine-tuned model: {fine_tuned_path}") try: from sentence_transformers import SentenceTransformer fine_tuned_model = SentenceTransformer(str(fine_tuned_path)) # Create embedder with fine-tuned model fine_tuned_embedder = InstructionTunedCardEmbedder(model_name=str(fine_tuned_path)) fine_tuned_results = evaluate_on_test_set( fine_tuned_embedder, test_set_path, instruction_type=task_type, ) results["models"]["fine_tuned"] = fine_tuned_results # Calculate improvement p10_improvement = fine_tuned_results["p_at_10"] - zero_shot_results["p_at_10"] mrr_improvement = fine_tuned_results["mrr"] - zero_shot_results["mrr"] results["improvement"] = { "p_at_10_delta": p10_improvement, "p_at_10_percent": (p10_improvement / zero_shot_results["p_at_10"] * 100) if zero_shot_results["p_at_10"] > 0 else 0.0, "mrr_delta": mrr_improvement, "mrr_percent": (mrr_improvement / zero_shot_results["mrr"] * 100) if zero_shot_results["mrr"] > 0 else 0.0, } except Exception as e: logger.warning(f"Failed to load fine-tuned model: {e}") return results def main() -> int: """Evaluate instruction embeddings.""" parser = argparse.ArgumentParser(description="Evaluate instruction-tuned embeddings") parser.add_argument("--base-model", type=str, default="intfloat/e5-base-v2", help="Base model name") parser.add_argument("--fine-tuned", type=Path, help="Path to fine-tuned model directory") parser.add_argument("--test-set", type=Path, help="Test set JSON") parser.add_argument("--task-type", type=str, default="substitution", help="Task type to evaluate") parser.add_argument("--output", type=Path, help="Output JSON file") args = parser.parse_args() # Default test set if not args.test_set: test_candidates = [ PATHS.test_magic if hasattr(PATHS, 'test_magic') else None, Path("experiments/test_set_unified_magic.json"), ] for cand in test_candidates: if cand and isinstance(cand, Path) and cand.exists(): args.test_set = cand break if not args.test_set or not args.test_set.exists(): logger.error("Test set not found. Please provide --test-set") return 1 try: results = compare_models( base_model_name=args.base_model, fine_tuned_path=args.fine_tuned, test_set_path=args.test_set, task_type=args.task_type, ) # Print results logger.info("\n" + "="*70) logger.info("EVALUATION RESULTS") logger.info("="*70) if "zero_shot" in results["models"]: zs = results["models"]["zero_shot"] logger.info(f"Zero-shot ({args.base_model}):") logger.info(f" P@10: {zs['p_at_10']:.4f}") logger.info(f" MRR: {zs['mrr']:.4f}") if "fine_tuned" in results["models"]: ft = results["models"]["fine_tuned"] logger.info(f"Fine-tuned ({args.fine_tuned}):") logger.info(f" P@10: {ft['p_at_10']:.4f}") logger.info(f" MRR: {ft['mrr']:.4f}") if "improvement" in results: imp = results["improvement"] logger.info(f"Improvement:") logger.info(f" P@10: {imp['p_at_10_delta']:+.4f} ({imp['p_at_10_percent']:+.2f}%)") logger.info(f" MRR: {imp['mrr_delta']:+.4f} ({imp['mrr_percent']:+.2f}%)") # Save results if args.output: args.output.parent.mkdir(parents=True, exist_ok=True) with open(args.output, 'w') as f: json.dump(results, f, indent=2) logger.info(f"\n Results saved to {args.output}") return 0 except Exception as e: logger.error(f"Error: Evaluation failed: {e}", exc_info=True) return 1 if __name__ == "__main__": exit(main())