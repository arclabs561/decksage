#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [] # /// """ Aggregate user feedback into training data. Converts user feedback (from /v1/feedback endpoint) into: 1. Substitution pairs (for training) 2. Similarity annotations (for evaluation) 3. Quality metrics (for monitoring) """ from __future__ import annotations import argparse import json import sys from collections import defaultdict from pathlib import Path from typing import Any # Add src to path script_dir = Path(__file__).parent project_root = script_dir.parent src_dir = project_root / "src" if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) try: from ml.utils.paths import PATHS DATA_DIR = PATHS.DATA_DIR EXPERIMENTS_DIR = PATHS.EXPERIMENTS_DIR except ImportError: DATA_DIR = project_root / "data" EXPERIMENTS_DIR = project_root / "experiments" def load_user_feedback(feedback_path: Path) -> list[dict[str, Any]]: """Load user feedback from JSONL file.""" if not feedback_path.exists(): return [] feedback = [] with open(feedback_path) as f: for line in f: if line.strip(): try: feedback.append(json.loads(line)) except Exception: continue return feedback def extract_substitution_pairs(feedback: list[dict[str, Any]], min_rating: int = 3) -> list[tuple[str, str]]: """Extract substitution pairs from feedback.""" pairs = [] for f in feedback: # Only include if explicitly marked as substitute or high rating if f.get("is_substitute") is True or (f.get("rating", 0) >= min_rating and f.get("task_type") == "substitution"): query = f.get("query_card", "") suggested = f.get("suggested_card", "") if query and suggested: # Normalize pair (sorted order) pair = tuple(sorted([query, suggested])) pairs.append(pair) # Deduplicate pairs = list(set(pairs)) return pairs def convert_to_similarity_annotations(feedback: list[dict[str, Any]]) -> list[dict[str, Any]]: """Convert user feedback to similarity annotation format.""" annotations = [] for f in feedback: ann = { "card1": f.get("query_card", ""), "card2": f.get("suggested_card", ""), "similarity_score": f.get("rating", 0) / 4.0, # Convert 0-4 to 0-1 "similarity_type": "functional" if f.get("is_substitute") else "synergy", "is_substitute": f.get("is_substitute", False), "reasoning": f.get("feedback_text", ""), "annotator_id": f.get("user_id", "user"), "timestamp": f.get("timestamp", ""), "source": "user_feedback", } annotations.append(ann) return annotations def main() -> int: parser = argparse.ArgumentParser(description="Aggregate user feedback into training data") parser.add_argument("--feedback-file", type=Path, default=DATA_DIR / "annotations" / "user_feedback.jsonl", help="Path to user feedback JSONL file") parser.add_argument("--output-pairs", type=Path, help="Output path for substitution pairs JSON") parser.add_argument("--output-annotations", type=Path, help="Output path for similarity annotations JSONL") parser.add_argument("--min-rating", type=int, default=3, help="Minimum rating for substitution pairs (0-4)") parser.add_argument("--stats", action="store_true", help="Print statistics") args = parser.parse_args() # Load feedback feedback = load_user_feedback(args.feedback_file) if not feedback: print(f"Warning: No feedback found in {args.feedback_file}") return 0 print(f"Loaded {len(feedback)} feedback entries") # Extract substitution pairs pairs = extract_substitution_pairs(feedback, min_rating=args.min_rating) print(f"Extracted {len(pairs)} substitution pairs (min_rating={args.min_rating})") # Convert to similarity annotations annotations = convert_to_similarity_annotations(feedback) print(f"Converted to {len(annotations)} similarity annotations") # Save substitution pairs if args.output_pairs: args.output_pairs.parent.mkdir(parents=True, exist_ok=True) pairs_list = [[card1, card2] for card1, card2 in pairs] with open(args.output_pairs, "w") as f: json.dump(pairs_list, f, indent=2) print(f" Saved substitution pairs to {args.output_pairs}") else: # Default location output_pairs = EXPERIMENTS_DIR / "substitution_pairs_from_user_feedback.json" output_pairs.parent.mkdir(parents=True, exist_ok=True) pairs_list = [[card1, card2] for card1, card2 in pairs] with open(output_pairs, "w") as f: json.dump(pairs_list, f, indent=2) print(f" Saved substitution pairs to {output_pairs}") # Save similarity annotations if args.output_annotations: args.output_annotations.parent.mkdir(parents=True, exist_ok=True) with open(args.output_annotations, "w") as f: for ann in annotations: f.write(json.dumps(ann) + "\n") print(f" Saved similarity annotations to {args.output_annotations}") else: # Default location output_ann = EXPERIMENTS_DIR / "annotations_llm" / "similarity_annotations_user_feedback.jsonl" output_ann.parent.mkdir(parents=True, exist_ok=True) with open(output_ann, "w") as f: for ann in annotations: f.write(json.dumps(ann) + "\n") print(f" Saved similarity annotations to {output_ann}") # Print statistics if args.stats: print("\nStatistics:") print(f" Total feedback: {len(feedback)}") by_task = defaultdict(int) by_rating = defaultdict(int) substitutions = 0 for f in feedback: by_task[f.get("task_type", "unknown")] += 1 by_rating[f.get("rating", 0)] += 1 if f.get("is_substitute") is True: substitutions += 1 print(f" By task type: {dict(by_task)}") print(f" By rating: {dict(by_rating)}") print(f" Substitution rate: {substitutions/len(feedback)*100:.1f}%") print(f" Substitution pairs: {len(pairs)}") return 0 if __name__ == "__main__": sys.exit(main())