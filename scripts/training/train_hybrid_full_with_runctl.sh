#!/bin/bash # Full hybrid embedding training with runctl # Usage: ./scripts/training/train_hybrid_full_with_runctl.sh [local|aws] [instance-id] [options...] # # Designed per cursor rules: # - Use ../runctl/target/release/runctl (parent directory) # - Use --data-s3 and --output-s3 for cloud training # - Enable checkpointing for long runs # - Show all progress (no tail/head piping) set -euo pipefail SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)" PROJECT_ROOT="$(cd "$SCRIPT_DIR/../../.." && pwd)" RUNCTL_BIN="${RUNCTL_BIN:-$(cd "$PROJECT_ROOT/.." && pwd)/runctl/target/release/runctl}" # Find or build runctl (per cursor rules) if [[ ! -f "$RUNCTL_BIN" ]]; then if [[ -d "$PROJECT_ROOT/../runctl" ]]; then echo "Warning: runctl not found, building..." (cd "$PROJECT_ROOT/../runctl" && cargo build --release) || { echo "Error: Failed to build runctl" exit 1 } else echo "Error: runctl not found at $RUNCTL_BIN" exit 1 fi fi MODE="${1:-local}" shift || true # Default arguments (per cursor rules - primary training data) DECKS_PATH="${DECKS_PATH:-data/processed/decks_all_final.jsonl}" PAIRS_PATH="${PAIRS_PATH:-data/processed/pairs_large.csv}" GRAPH_PATH="${GRAPH_PATH:-data/graphs/incremental_graph.json}" GNN_OUTPUT="${GNN_OUTPUT:-data/embeddings/gnn_graphsage.json}" TEST_SET="${TEST_SET:-experiments/test_set_unified_magic.json}" GNN_EPOCHS="${GNN_EPOCHS:-100}" GNN_LR="${GNN_LR:-0.01}" CHECKPOINT_INTERVAL="${CHECKPOINT_INTERVAL:-}" RESUME_FROM="${RESUME_FROM:-}" INSTRUCTION_MODEL="${INSTRUCTION_MODEL:-intfloat/e5-base-v2}" echo "="*70 echo "HYBRID EMBEDDING FULL TRAINING (runctl)" echo "="*70 echo "Mode: $MODE" echo "Decks: $DECKS_PATH" echo "Pairs (fallback): $PAIRS_PATH" echo "Graph: $GRAPH_PATH" echo "GNN Output: $GNN_OUTPUT" echo "Test Set: $TEST_SET" echo "GNN Epochs: $GNN_EPOCHS" echo "Checkpoint Interval: ${CHECKPOINT_INTERVAL:-disabled}" echo "" # Determine which data source to use USE_DECKS=false USE_PAIRS=false if [[ -f "$DECKS_PATH" ]]; then USE_DECKS=true TRAIN_SCRIPT="src/ml/scripts/train_hybrid_full.py" TRAIN_DATA_ARG="--decks-path" TRAIN_DATA_PATH="$DECKS_PATH" elif [[ -f "$PAIRS_PATH" ]]; then USE_PAIRS=true TRAIN_SCRIPT="src/ml/scripts/train_hybrid_from_pairs.py" TRAIN_DATA_ARG="--pairs-path" TRAIN_DATA_PATH="$PAIRS_PATH" echo "Warning: Using pairs_large.csv (decks_all_final.jsonl not found)" else echo "Error: No training data found!" echo " Expected: $DECKS_PATH or $PAIRS_PATH" echo " Sync from S3: s5cmd sync s3://games-collections/processed/decks_all_final.jsonl data/processed/" exit 1 fi case "$MODE" in local) echo "Running local training (slow, for testing)..." echo "Warning: For full training, use AWS mode (4-8x faster)" echo "" COMMON_ARGS=( "$TRAIN_DATA_ARG" "$TRAIN_DATA_PATH" "--graph-path" "$GRAPH_PATH" "--gnn-output" "$GNN_OUTPUT" "--test-set" "$TEST_SET" "--gnn-epochs" "$GNN_EPOCHS" "--gnn-lr" "$GNN_LR" "--instruction-model" "$INSTRUCTION_MODEL" ) if [[ -n "$CHECKPOINT_INTERVAL" ]]; then COMMON_ARGS+=("--checkpoint-interval" "$CHECKPOINT_INTERVAL") fi if [[ -n "$RESUME_FROM" ]]; then COMMON_ARGS+=("--resume-from" "$RESUME_FROM") fi if [[ "$USE_DECKS" == "true" ]]; then uv run python -m ml.scripts.train_hybrid_full "${COMMON_ARGS[@]}" "$@" else uv run python -m ml.scripts.train_hybrid_from_pairs "${COMMON_ARGS[@]}" "$@" fi ;; aws|runpod) INSTANCE_ID="${1:-}" if [[ -z "$INSTANCE_ID" ]]; then echo "No instance ID provided, creating new instance with EBS volume..." INSTANCE_TYPE="${INSTANCE_TYPE:-g4dn.xlarge}" DATA_VOLUME_SIZE="${DATA_VOLUME_SIZE:-500}" echo " Instance type: $INSTANCE_TYPE" echo " EBS volume size: ${DATA_VOLUME_SIZE}GB" CREATE_OUTPUT=$("$RUNCTL_BIN" aws create \ --spot \ --data-volume-size "$DATA_VOLUME_SIZE" \ --iam-instance-profile EC2-SSM-InstanceProfile \ --key-name tarek \ "$INSTANCE_TYPE" 2>&1) echo "$CREATE_OUTPUT" INSTANCE_ID=$(echo "$CREATE_OUTPUT" | grep -oE 'i-[a-z0-9]+' | head -1 || echo "") if [[ -z "$INSTANCE_ID" ]]; then echo "Error: Failed to create instance or extract instance ID" exit 1 fi echo " Created instance: $INSTANCE_ID" echo " Waiting for instance to be ready..." sleep 30 fi shift || true echo "Running on AWS (fast GPU training)..." echo " Instance: $INSTANCE_ID" echo "" # S3 paths (per cursor rules) S3_DATA_BASE="s3://games-collections/processed/" S3_OUTPUT_BASE="s3://games-collections/embeddings/" S3_GRAPHS_BASE="s3://games-collections/graphs/" # Build runctl command (per cursor rules) # Use --data-s3 and --output-s3 for cloud training # Per cursor rules: Show all progress (no tail/head piping) # Determine S3 paths based on data source if [[ "$USE_DECKS" == "true" ]]; then DATA_S3="$S3_DATA_BASE$(basename "$TRAIN_DATA_PATH")" else DATA_S3="$S3_DATA_BASE$(basename "$TRAIN_DATA_PATH")" fi # Build Python script arguments # Paths are relative to the base S3 path (s3://games-collections/) PYTHON_ARGS=( "$TRAIN_DATA_ARG" "processed/$(basename "$TRAIN_DATA_PATH")" "--graph-path" "graphs/incremental_graph.json" "--gnn-output" "embeddings/gnn_graphsage.json" "--test-set" "experiments/$(basename "$TEST_SET")" "--gnn-epochs" "$GNN_EPOCHS" "--gnn-lr" "$GNN_LR" "--instruction-model" "$INSTRUCTION_MODEL" "--progress-dir" "training_progress" ) # Add checkpoint interval if set if [[ -n "$CHECKPOINT_INTERVAL" ]]; then PYTHON_ARGS+=("--checkpoint-interval" "$CHECKPOINT_INTERVAL") fi if [[ -n "$CHECKPOINT_INTERVAL" ]]; then PYTHON_ARGS+=("--checkpoint-interval" "$CHECKPOINT_INTERVAL") fi if [[ -n "$RESUME_FROM" ]]; then PYTHON_ARGS+=("--resume-from" "$RESUME_FROM") fi # Check if code exists on EBS volume (skip sync if found) # This avoids re-uploading 68MB of code every time CODE_ON_EBS=false if command -v "$PROJECT_ROOT/scripts/check_ebs_code.sh" > /dev/null 2>&1; then if "$PROJECT_ROOT/scripts/check_ebs_code.sh" "$INSTANCE_ID" > /dev/null 2>&1; then CODE_ON_EBS=true echo "âœ“ Code found on EBS volume - skipping code sync" echo " Using code from /mnt/data/code" fi fi # runctl will sync S3 paths automatically # Per cursor rules: Use --data-s3 and --output-s3 for cloud training # Note: runctl only supports one --data-s3 and one --output-s3, so we use base paths # The script will access files relative to these base paths # SSM code sync: runctl automatically uses SSM if instance has IAM profile # Skip code sync if code already exists on EBS (much faster!) if [[ "$CODE_ON_EBS" == "true" ]]; then # Use --no-sync-code if runctl supports it, otherwise we'll need to work around # For now, runctl doesn't have --no-sync-code, so we'll let it sync but it should be fast # if code is already there (runctl might detect it) echo " Note: Code sync will still run but should be faster with EBS" fi "$RUNCTL_BIN" aws train "$INSTANCE_ID" \ "$TRAIN_SCRIPT" \ --data-s3 "s3://games-collections/" \ --output-s3 "s3://games-collections/" \ -- \ "${PYTHON_ARGS[@]}" \ "$@" ;; *) echo "Error: Unknown mode: $MODE (use 'local' or 'aws')" exit 1 ;; esac echo "" echo "="*70 echo "TRAINING COMPLETE" echo "="*70
