#!/usr/bin/env bash # Execute all next steps: Evaluation → Fine-tuning → Comparison # # This script orchestrates: # 1. Comprehensive downstream evaluation with task-aware instructions # 2. Instruction embedding fine-tuning (on GPU via runctl) # 3. Comparison of results # # Usage: # ./scripts/execute_all_next_steps.sh [--game magic] [--skip-eval] [--skip-finetune] [--local-finetune] set -euo pipefail SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)" PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)" cd "$PROJECT_ROOT" # Default arguments GAME="magic" SKIP_EVAL=false SKIP_FINETUNE=false LOCAL_FINETUNE=false QUICK_EVAL=false # Parse arguments while [[ $# -gt 0 ]]; do case $1 in --game) GAME="$2" shift 2 ;; --skip-eval) SKIP_EVAL=true shift ;; --skip-finetune) SKIP_FINETUNE=true shift ;; --local-finetune) LOCAL_FINETUNE=true shift ;; --quick) QUICK_EVAL=true shift ;; *) echo "Unknown option: $1" exit 1 ;; esac done echo "═══════════════════════════════════════════════════════════════════════" echo "EXECUTING ALL NEXT STEPS" echo "═══════════════════════════════════════════════════════════════════════" echo "" echo "Game: $GAME" echo "Skip evaluation: $SKIP_EVAL" echo "Skip fine-tuning: $SKIP_FINETUNE" echo "Local fine-tuning: $LOCAL_FINETUNE" echo "Quick evaluation: $QUICK_EVAL" echo "" # Step 1: Comprehensive Evaluation if [ "$SKIP_EVAL" != "true" ]; then echo "═══════════════════════════════════════════════════════════════════════" echo "STEP 1: COMPREHENSIVE DOWNSTREAM EVALUATION" echo "═══════════════════════════════════════════════════════════════════════" echo "" EVAL_ARGS=("--game" "$GAME" "--compare") if [ "$QUICK_EVAL" == "true" ]; then EVAL_ARGS+=("--quick") fi if bash scripts/run_comprehensive_downstream_eval.sh "${EVAL_ARGS[@]}"; then echo "" echo " Evaluation complete!" echo "" else echo "" echo "Warning: Evaluation had issues (continuing anyway)" echo "" fi else echo "⏭️ Skipping evaluation (--skip-eval)" echo "" fi # Step 2: Prepare Fine-tuning Data if [ "$SKIP_FINETUNE" != "true" ]; then echo "═══════════════════════════════════════════════════════════════════════" echo "STEP 2: PREPARING FINE-TUNING DATA" echo "═══════════════════════════════════════════════════════════════════════" echo "" TRAINING_DATA="experiments/instruction_finetuning_data.json" if [ ! -f "$TRAINING_DATA" ]; then echo "Preparing multi-task training data..." if uv run python scripts/prepare_instruction_finetuning_data.py --output "$TRAINING_DATA"; then echo " Training data prepared: $TRAINING_DATA" # Show data distribution echo "" echo "Training data distribution:" uv run python -c " import json from pathlib import Path data_path = Path('$TRAINING_DATA') if data_path.exists(): with open(data_path) as f: data = json.load(f) for task, pairs in data.items(): print(f' {task}: {len(pairs)} pairs') total = sum(len(pairs) for pairs in data.values()) print(f' Total: {total} pairs') " else echo "Error: Failed to prepare training data" exit 1 fi else echo " Training data already exists: $TRAINING_DATA" fi echo "" fi # Step 3: Fine-tune Instruction Embeddings if [ "$SKIP_FINETUNE" != "true" ]; then echo "═══════════════════════════════════════════════════════════════════════" echo "STEP 3: FINE-TUNING INSTRUCTION EMBEDDINGS" echo "═══════════════════════════════════════════════════════════════════════" echo "" if [ "$LOCAL_FINETUNE" == "true" ]; then echo "Running fine-tuning locally (may be slow without GPU)..." echo "" bash scripts/finetune_instruction_embeddings_all_tasks.sh else echo "Fine-tuning on AWS GPU via runctl..." echo "" echo "This will:" echo " 1. Create AWS g5.xlarge instance (GPU)" echo " 2. Fine-tune instruction embeddings for all tasks" echo " 3. Auto-stop instance when complete" echo "" read -p "Continue with AWS fine-tuning? (y/N) " -n 1 -r echo if [[ $REPLY =~ ^[Yy]$ ]]; then RUNCTL_SCRIPT="scripts/training/train_instruction_finetune_runctl.sh" if [ -f "$RUNCTL_SCRIPT" ]; then bash "$RUNCTL_SCRIPT" else echo "Warning: runctl script not found: $RUNCTL_SCRIPT" echo " Falling back to local fine-tuning..." bash scripts/finetune_instruction_embeddings_all_tasks.sh fi else echo "⏭️ Skipping fine-tuning (user cancelled)" fi fi echo "" fi # Step 4: Evaluate Fine-tuned Models if [ "$SKIP_FINETUNE" != "true" ] && [ -d "data/embeddings/instruction_finetuned" ]; then echo "═══════════════════════════════════════════════════════════════════════" echo "STEP 4: EVALUATING FINE-TUNED MODELS" echo "═══════════════════════════════════════════════════════════════════════" echo "" for task_dir in data/embeddings/instruction_finetuned/*/; do if [ -d "$task_dir" ]; then task=$(basename "$task_dir") echo "Evaluating fine-tuned model for task: $task" uv run python scripts/evaluate_instruction_embeddings.py \ --base-model "intfloat/e5-base-v2" \ --fine-tuned "$task_dir" \ --test-set "experiments/test_set_unified_${GAME}.json" \ --task-type "$task" \ --output "experiments/instruction_eval_${task}_finetuned.json" || { echo " Warning: Evaluation failed for task: $task" } fi done echo "" fi # Step 5: Compare Results echo "═══════════════════════════════════════════════════════════════════════" echo "STEP 5: COMPARING RESULTS" echo "═══════════════════════════════════════════════════════════════════════" echo "" # Find latest evaluation results LATEST_EVAL=$(ls -td experiments/downstream_eval_*/results.json 2>/dev/null | head -1 || echo "") if [ -n "$LATEST_EVAL" ]; then echo "Latest evaluation: $LATEST_EVAL" echo "" # Show summary uv run python -c " import json from pathlib import Path eval_path = Path('$LATEST_EVAL') if eval_path.exists(): with open(eval_path) as f: results = json.load(f) print('Evaluation Summary:') print('=' * 70) tasks = results.get('tasks', {}) for task_name, task_results in tasks.items(): print(f'\n{task_name.upper()}:') if 'error' in task_results: print(f' Error: Error: {task_results[\"error\"]}') else: for key, value in task_results.items(): if isinstance(value, (int, float)) and not isinstance(value, bool): if 'p_at' in key or 'avg' in key or 'rank' in key: print(f' {key}: {value:.4f}') elif 'total' in key or 'found' in key or 'completed' in key: print(f' {key}: {value}') critique = results.get('critique', {}) if critique: print(f'\nOverall Assessment: {critique.get(\"overall_assessment\", \"N/A\")}') " else echo "Warning: No evaluation results found" fi echo "" echo "═══════════════════════════════════════════════════════════════════════" echo " ALL STEPS COMPLETE" echo "═══════════════════════════════════════════════════════════════════════" echo "" echo "Next steps:" echo " 1. Review evaluation results in: experiments/downstream_eval_*/" echo " 2. Compare fine-tuned vs zero-shot performance" echo " 3. Deploy best model to production" echo ""