#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pyyaml>=6.0", # ] # /// """ End-to-end test of annotation workflow. Tests: 1. Create LLM annotation with metadata 2. Create hand annotation batch with IAA tracking 3. Convert annotations to substitution pairs 4. Add annotations to graph 5. Verify metadata throughout pipeline """ from __future__ import annotations import json import tempfile from datetime import datetime from pathlib import Path from typing import Any try: import yaml HAS_YAML = True except ImportError: HAS_YAML = False print("Install pyyaml: pip install pyyaml") import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) def test_llm_annotation_creation(): """Test creating LLM annotation with metadata.""" print("Test 1: LLM Annotation Creation with Metadata") print("-" * 60) try: from ml.annotation.llm_annotator import CardSimilarityAnnotation ann = CardSimilarityAnnotation( card1="Lightning Bolt", card2="Chain Lightning", similarity_score=0.9, similarity_type="functional", reasoning="Both 1-mana red burn spells", is_substitute=True, context_dependent=False, example_decks=[], model_name="anthropic/claude-4.5-sonnet", model_params={"provider": "openrouter", "temperature": 0.7}, prompt_hash="test_prompt_hash", annotator_id="test_judge_1", timestamp=datetime.now().isoformat(), ) # Verify metadata assert ann.model_name == "anthropic/claude-4.5-sonnet" assert ann.model_params is not None assert ann.annotator_id == "test_judge_1" assert ann.timestamp is not None # Test serialization data = ann.model_dump() assert "model_name" in data assert "model_params" in data # Write to JSONL with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f: f.write(json.dumps(data) + "\n") test_file = Path(f.name) # Load back from ml.utils.annotation_utils import load_similarity_annotations loaded = load_similarity_annotations(test_file) assert len(loaded) == 1 assert loaded[0]["model_name"] == "anthropic/claude-4.5-sonnet" test_file.unlink() print(" LLM annotation with metadata created and loaded successfully") return True except Exception as e: print(f" Error: Error: {e}") import traceback traceback.print_exc() return False def test_hand_annotation_iaa(): """Test hand annotation batch with IAA tracking.""" print("\nTest 2: Hand Annotation IAA Tracking") print("-" * 60) if not HAS_YAML: print(" Warning: PyYAML not available, skipping") return False # Create test batch structure test_batch = { "metadata": { "game": "magic", "batch_id": "test_batch", "num_queries": 1, }, "instructions": { "iaa_tracking": { "annotator_id": "Set this to your annotator ID", "annotation_date": "Date of annotation (ISO format)", "confidence": "Your confidence (1-5 scale, optional)", }, }, "tasks": [ { "query": "Lightning Bolt", "candidates": [ { "card": "Chain Lightning", "relevance": 4, "similarity_type": "functional", "is_substitute": True, "annotator_id": "test_annotator_1", "annotation_date": datetime.now().isoformat(), } ] } ] } with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f: yaml.dump(test_batch, f) test_file = Path(f.name) try: # Load and verify with open(test_file) as f: loaded = yaml.safe_load(f) assert "instructions" in loaded assert "iaa_tracking" in loaded["instructions"] assert "annotator_id" in loaded["instructions"]["iaa_tracking"] # Check candidate has annotator_id candidate = loaded["tasks"][0]["candidates"][0] assert "annotator_id" in candidate test_file.unlink() print(" Hand annotation with IAA tracking created and loaded successfully") return True except Exception as e: print(f" Error: Error: {e}") import traceback traceback.print_exc() return False def test_annotation_to_substitution_pairs(): """Test converting annotations to substitution pairs with metadata.""" print("\nTest 3: Annotation to Substitution Pairs") print("-" * 60) try: from ml.utils.annotation_utils import ( extract_substitution_pairs_from_annotations, load_similarity_annotations, ) # Create test annotation test_ann = { "card1": "Lightning Bolt", "card2": "Chain Lightning", "similarity_score": 0.9, "similarity_type": "functional", "is_substitute": True, "model_name": "anthropic/claude-4.5-sonnet", "annotator_id": "test_judge_1", } with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f: f.write(json.dumps(test_ann) + "\n") ann_file = Path(f.name) # Load and extract pairs annotations = load_similarity_annotations(ann_file) pairs = extract_substitution_pairs_from_annotations(annotations, min_similarity=0.8) assert len(pairs) == 1 assert ("Lightning Bolt", "Chain Lightning") in pairs or ("Chain Lightning", "Lightning Bolt") in pairs ann_file.unlink() print(" Annotations converted to substitution pairs successfully") return True except Exception as e: print(f" Error: Error: {e}") import traceback traceback.print_exc() return False def test_graph_metadata_integration(): """Test adding annotation metadata to graph.""" print("\nTest 4: Graph Metadata Integration") print("-" * 60) try: from ml.data.incremental_graph import Edge, IncrementalCardGraph # Create test edge with annotation metadata edge = Edge( card1="Lightning Bolt", card2="Chain Lightning", weight=5.0, metadata={ "annotation": { "similarity_type": "functional", "is_substitute": True, "model_name": "anthropic/claude-4.5-sonnet", "annotator_id": "test_judge_1", } } ) # Verify metadata assert edge.metadata is not None assert "annotation" in edge.metadata assert edge.metadata["annotation"]["similarity_type"] == "functional" # Test serialization edge_dict = edge.to_dict() assert "metadata" in edge_dict assert "annotation" in edge_dict["metadata"] print(" Graph edge with annotation metadata created and serialized") return True except Exception as e: print(f" Error: Error: {e}") import traceback traceback.print_exc() return False def main() -> int: """Run all tests.""" print("=" * 70) print("ANNOTATION WORKFLOW END-TO-END TESTS") print("=" * 70) print() results = { "llm_annotation": test_llm_annotation_creation(), "hand_annotation_iaa": test_hand_annotation_iaa(), "substitution_pairs": test_annotation_to_substitution_pairs(), "graph_integration": test_graph_metadata_integration(), } print("\n" + "=" * 70) print("TEST RESULTS") print("=" * 70) all_passed = all(results.values()) for test_name, passed in results.items(): status = " PASS" if passed else "Error: FAIL" print(f" {test_name}: {status}") print() if all_passed: print(" All tests passed!") return 0 else: print("Warning: Some tests failed") return 1 if __name__ == "__main__": import sys sys.exit(main())