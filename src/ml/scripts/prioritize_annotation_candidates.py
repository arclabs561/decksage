#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pyyaml>=6.0", # ] # /// """ Active Learning: Prioritize annotation candidates. Uses multiple signals to identify which card pairs would benefit most from annotation: 1. High embedding similarity but low co-occurrence (uncertainty) 2. Pairs in test sets but not annotated 3. Pairs with single annotation (need IAA) 4. Pairs with conflicting annotations 5. Pairs important for downstream tasks """ from __future__ import annotations import argparse import json from collections import Counter, defaultdict from pathlib import Path from typing import Any try: import yaml HAS_YAML = True except ImportError: HAS_YAML = False import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) from ml.utils.annotation_utils import load_similarity_annotations, load_hand_annotations from ml.utils.data_loading import load_test_set def compute_annotation_priority_scores( pairs_csv: Path | None = None, embeddings_path: Path | None = None, test_set_path: Path | None = None, existing_annotations: list[Path] | None = None, game: str | None = None, ) -> list[dict[str, Any]]: """Compute priority scores for annotation candidates.""" # Load existing annotations annotated_pairs = set() pair_annotation_counts = Counter() conflicting_pairs = [] if existing_annotations: for ann_path in existing_annotations: if not ann_path.exists(): continue if ann_path.suffix == ".yaml": annotations = load_hand_annotations(ann_path) else: annotations = load_similarity_annotations(ann_path) for ann in annotations: card1 = ann.get("card1") card2 = ann.get("card2") if card1 and card2: pair = tuple(sorted([card1, card2])) annotated_pairs.add(pair) pair_annotation_counts[pair] += 1 # Check for conflicts (different similarity types for same pair) similarity_type = ann.get("similarity_type") if pair in pair_annotation_counts and pair_annotation_counts[pair] > 1: # Could check for conflicts here pass # Load test set pairs test_set_pairs = set() if test_set_path and test_set_path.exists(): test_set = load_test_set(test_set_path) for query, candidates in test_set.items(): for candidate in candidates: pair = tuple(sorted([query, candidate])) test_set_pairs.add(pair) # Load embedding similarities (if available) embedding_similarities = {} if embeddings_path and embeddings_path.exists(): try: from gensim.models import KeyedVectors wv = KeyedVectors.load(str(embeddings_path)) # Sample pairs for priority scoring vocab = list(wv.key_to_index.keys())[:1000] # Limit for performance for i, card1 in enumerate(vocab): if game and card1 not in vocab: # Filter by game if needed continue for card2 in vocab[i+1:]: try: sim = wv.similarity(card1, card2) pair = tuple(sorted([card1, card2])) embedding_similarities[pair] = sim except KeyError: continue except Exception as e: print(f"Warning: Could not load embeddings: {e}") # Compute priority scores candidate_scores = [] # Priority 1: Test set pairs without annotations for pair in test_set_pairs: if pair not in annotated_pairs: candidate_scores.append({ "card1": pair[0], "card2": pair[1], "priority_score": 100.0, # Highest priority "priority_reason": "In test set but not annotated", "signals": { "in_test_set": True, "has_annotation": False, } }) # Priority 2: High embedding similarity but no annotation for pair, sim in embedding_similarities.items(): if pair not in annotated_pairs and sim > 0.7: candidate_scores.append({ "card1": pair[0], "card2": pair[1], "priority_score": 80.0 + (sim * 20), # 80-100 range "priority_reason": f"High embedding similarity ({sim:.2f}) but not annotated", "signals": { "embedding_similarity": sim, "has_annotation": False, } }) # Priority 3: Pairs with single annotation (need IAA) for pair, count in pair_annotation_counts.items(): if count == 1: candidate_scores.append({ "card1": pair[0], "card2": pair[1], "priority_score": 60.0, "priority_reason": "Single annotation (needs IAA)", "signals": { "annotation_count": count, "needs_iaa": True, } }) # Priority 4: High embedding similarity pairs (general coverage) for pair, sim in embedding_similarities.items(): if pair not in annotated_pairs and 0.5 < sim <= 0.7: candidate_scores.append({ "card1": pair[0], "card2": pair[1], "priority_score": 40.0 + (sim * 40), # 40-68 range "priority_reason": f"Moderate embedding similarity ({sim:.2f})", "signals": { "embedding_similarity": sim, "has_annotation": False, } }) # Sort by priority score candidate_scores.sort(key=lambda x: x["priority_score"], reverse=True) return candidate_scores def main() -> int: parser = argparse.ArgumentParser(description="Prioritize annotation candidates") parser.add_argument("--pairs-csv", type=str, help="Pairs CSV file") parser.add_argument("--embeddings", type=str, help="Embeddings file (.wv)") parser.add_argument("--test-set", type=str, help="Test set JSON file") parser.add_argument("--annotations-dir", type=str, default="annotations", help="Existing annotations directory") parser.add_argument("--game", type=str, choices=["MTG", "PKM", "YGO"], help="Filter by game") parser.add_argument("--output", type=str, required=True, help="Output JSON file") parser.add_argument("--top-k", type=int, default=100, help="Top K candidates to return") args = parser.parse_args() # Find existing annotations annotations_dir = Path(args.annotations_dir) existing_annotations = [] if annotations_dir.exists(): existing_annotations = ( list(annotations_dir.glob("*.yaml")) + list(annotations_dir.glob("*.jsonl")) ) print("=" * 70) print("ANNOTATION CANDIDATE PRIORITIZATION") print("=" * 70) print() print(f"Existing annotations: {len(existing_annotations)} files") print() candidates = compute_annotation_priority_scores( pairs_csv=Path(args.pairs_csv) if args.pairs_csv else None, embeddings_path=Path(args.embeddings) if args.embeddings else None, test_set_path=Path(args.test_set) if args.test_set else None, existing_annotations=existing_annotations, game=args.game, ) # Take top K top_candidates = candidates[:args.top_k] print(f" Priority Scores:") print(f" Total candidates: {len(candidates)}") print(f" Top {args.top_k} candidates:") print() for i, cand in enumerate(top_candidates[:20], 1): # Show top 20 print(f" {i:3d}. {cand['card1']} <-> {cand['card2']}") print(f" Score: {cand['priority_score']:.1f} - {cand['priority_reason']}") print() # Save results with open(args.output, "w") as f: json.dump({ "total_candidates": len(candidates), "top_k": args.top_k, "candidates": top_candidates, }, f, indent=2) print(f"âœ“ Saved {len(top_candidates)} candidates to {args.output}") print() print("=" * 70) print(" Prioritization complete!") print("=" * 70) return 0 if __name__ == "__main__": import sys sys.exit(main())