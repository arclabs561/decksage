#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pyyaml>=6.0", # ] # /// """ Score annotation quality using multiple signals. Quality signals: 1. Consistency: Multiple annotators agree 2. Completeness: Has all required fields 3. Metadata: Tracks model/params/timestamp 4. Downstream support: Has fields for downstream tasks 5. Confidence: Annotator confidence scores """ from __future__ import annotations import argparse import json from collections import Counter, defaultdict from pathlib import Path from typing import Any try: import yaml HAS_YAML = True except ImportError: HAS_YAML = False import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) from ml.utils.annotation_utils import load_similarity_annotations, load_hand_annotations def score_annotation_quality(annotation_path: Path) -> dict[str, Any]: """Score quality of annotations.""" # Load annotations if annotation_path.suffix == ".yaml": annotations = load_hand_annotations(annotation_path) is_hand = True else: annotations = load_similarity_annotations(annotation_path) is_hand = False if not annotations: return {"error": "No annotations found"} # Quality signals quality_scores = [] required_fields_llm = ["card1", "card2", "similarity_score", "similarity_type"] required_fields_hand = ["card1", "card2", "relevance"] downstream_fields = ["role_match", "archetype_context", "format_context", "substitution_quality"] metadata_fields_llm = ["model_name", "model_params", "annotator_id", "timestamp"] metadata_fields_hand = ["annotator_id", "annotation_date"] for ann in annotations: score = { "card1": ann.get("card1"), "card2": ann.get("card2"), "completeness": 0.0, "metadata": 0.0, "downstream_support": 0.0, "confidence": 0.0, "total_score": 0.0, } # Completeness: Required fields present if is_hand: required = required_fields_hand present = sum(1 for f in required if ann.get(f) is not None) score["completeness"] = present / len(required) * 100 else: required = required_fields_llm present = sum(1 for f in required if ann.get(f) is not None) score["completeness"] = present / len(required) * 100 # Metadata: Tracking fields present if is_hand: metadata = metadata_fields_hand else: metadata = metadata_fields_llm present_meta = sum(1 for f in metadata if ann.get(f) is not None) score["metadata"] = present_meta / len(metadata) * 100 if metadata else 0.0 # Downstream support: Fields for downstream tasks present_downstream = sum(1 for f in downstream_fields if ann.get(f) is not None) score["downstream_support"] = present_downstream / len(downstream_fields) * 100 # Confidence: Annotator confidence (if available) confidence = ann.get("confidence") if confidence is not None: # Convert 1-5 scale to 0-100 score["confidence"] = (confidence / 5.0) * 100 else: score["confidence"] = 50.0 # Default medium confidence # Total score: Weighted average score["total_score"] = ( score["completeness"] * 0.3 + score["metadata"] * 0.2 + score["downstream_support"] * 0.3 + score["confidence"] * 0.2 ) quality_scores.append(score) # Aggregate statistics avg_completeness = sum(s["completeness"] for s in quality_scores) / len(quality_scores) avg_metadata = sum(s["metadata"] for s in quality_scores) / len(quality_scores) avg_downstream = sum(s["downstream_support"] for s in quality_scores) / len(quality_scores) avg_confidence = sum(s["confidence"] for s in quality_scores) / len(quality_scores) avg_total = sum(s["total_score"] for s in quality_scores) / len(quality_scores) # Quality distribution high_quality = sum(1 for s in quality_scores if s["total_score"] >= 80) medium_quality = sum(1 for s in quality_scores if 60 <= s["total_score"] < 80) low_quality = sum(1 for s in quality_scores if s["total_score"] < 60) return { "total_annotations": len(annotations), "average_scores": { "completeness": avg_completeness, "metadata": avg_metadata, "downstream_support": avg_downstream, "confidence": avg_confidence, "total": avg_total, }, "quality_distribution": { "high": high_quality, "medium": medium_quality, "low": low_quality, }, "individual_scores": quality_scores, } def main() -> int: parser = argparse.ArgumentParser(description="Score annotation quality") parser.add_argument("--input", type=str, required=True, help="Annotation file") parser.add_argument("--output", type=str, help="Output JSON report") args = parser.parse_args() annotation_path = Path(args.input) if not annotation_path.exists(): print(f"Error: Annotation file not found: {annotation_path}") return 1 print("=" * 70) print("ANNOTATION QUALITY SCORING") print("=" * 70) print() results = score_annotation_quality(annotation_path) if "error" in results: print(f"Error: Error: {results['error']}") return 1 print(f" Quality Scores:") print(f" Total annotations: {results['total_annotations']}") print() print(f" Average Scores:") print(f" Completeness: {results['average_scores']['completeness']:.1f}%") print(f" Metadata: {results['average_scores']['metadata']:.1f}%") print(f" Downstream Support: {results['average_scores']['downstream_support']:.1f}%") print(f" Confidence: {results['average_scores']['confidence']:.1f}%") print(f" Total: {results['average_scores']['total']:.1f}%") print() print(f" Quality Distribution:") print(f" High (≥80%): {results['quality_distribution']['high']}") print(f" Medium (60-80%): {results['quality_distribution']['medium']}") print(f" Low (<60%): {results['quality_distribution']['low']}") print() # Show low-quality examples low_quality = [s for s in results["individual_scores"] if s["total_score"] < 60] if low_quality: print(f"Warning: Low Quality Examples (top 5):") for s in sorted(low_quality, key=lambda x: x["total_score"])[:5]: print(f" {s['card1']} <-> {s['card2']}: {s['total_score']:.1f}%") print() # Save report if args.output: with open(args.output, "w") as f: json.dump(results, f, indent=2) print(f"✓ Saved report to {args.output}") print("=" * 70) print(" Quality scoring complete!") print("=" * 70) return 0 if __name__ == "__main__": import sys sys.exit(main())
