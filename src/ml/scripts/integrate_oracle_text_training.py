#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # ] # /// """ Integrate Oracle Text into Training Pipeline. Adds semantic similarity edges based on oracle text similarity. Uses text embeddings to find semantically similar cards. """ from __future__ import annotations import argparse import json import logging from collections import defaultdict from pathlib import Path from typing import Any try: from ..utils.logging_config import setup_script_logging, log_exception logger = setup_script_logging() except ImportError: logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def log_exception(logger, message, exc, **kwargs): logger.error(f"{message}: {exc}", exc_info=True) try: import pandas as pd import numpy as np HAS_DEPS = True pd_isna = pd.isna pd_notna = pd.notna except ImportError as e: HAS_DEPS = False print(f"Missing dependencies: {e}") def pd_isna(x): return x is None or (isinstance(x, float) and (x != x)) def pd_notna(x): return not pd_isna(x) logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def load_card_attributes(attrs_csv: Path) -> dict[str, dict[str, Any]]: """Load card attributes including oracle text.""" if not attrs_csv.exists(): logger.warning(f"Card attributes file not found: {attrs_csv}") return {} df = pd.read_csv(attrs_csv) attrs = {} for _, row in df.iterrows(): name = row.get("name", "") if name: attrs[name] = { "oracle_text": row.get("oracle_text", ""), "keywords": row.get("keywords", ""), "type": row.get("type", ""), } logger.info(f"Loaded attributes for {len(attrs)} cards") return attrs def compute_text_similarity(text1: str, text2: str) -> float: """ Simple text similarity using word overlap. For production, use proper text embeddings (sentence-transformers). """ # Handle NaN/None/float values if pd_isna(text1) or pd_isna(text2): return 0.0 # Convert to string if needed text1 = str(text1) if text1 else "" text2 = str(text2) if text2 else "" if not text1 or not text2: return 0.0 words1 = set(text1.lower().split()) words2 = set(text2.lower().split()) if not words1 or not words2: return 0.0 intersection = len(words1 & words2) union = len(words1 | words2) return intersection / union if union > 0 else 0.0 def create_text_similarity_edges( card_attrs: dict[str, dict[str, Any]], threshold: float = 0.3, max_edges_per_card: int = 10, ) -> list[tuple[str, str, float]]: """ Create edges based on oracle text similarity. Args: card_attrs: Card attributes dict threshold: Minimum similarity threshold max_edges_per_card: Maximum edges per card (top-k) Returns: List of (card1, card2, similarity) tuples """ edges = [] card_names = list(card_attrs.keys()) logger.info(f"Computing text similarity for {len(card_names)} cards...") for i, card1 in enumerate(card_names): if i % 100 == 0: logger.info(f" Processing {i}/{len(card_names)}...") text1 = card_attrs[card1].get("oracle_text", "") if pd_isna(text1) or not text1: continue text1 = str(text1) similarities = [] for card2 in card_names[i+1:]: text2 = card_attrs[card2].get("oracle_text", "") if pd_isna(text2) or not text2: continue text2 = str(text2) sim = compute_text_similarity(text1, text2) if sim >= threshold: similarities.append((card2, sim)) # Take top-k similarities.sort(key=lambda x: x[1], reverse=True) for card2, sim in similarities[:max_edges_per_card]: edges.append((card1, card2, sim)) logger.info(f"Created {len(edges)} text similarity edges") return edges def merge_text_edges_into_pairs( pairs_csv: Path, text_edges: list[tuple[str, str, float]], output_csv: Path, text_weight: float = 2.0, ) -> None: """ Merge text similarity edges into pairs CSV. Adds new rows for text similarity edges, or enhances existing edges. """ if not HAS_DEPS: raise ImportError("pandas required") logger.info(f"Loading pairs from {pairs_csv}...") pairs_df = pd.read_csv(pairs_csv) logger.info(f" Loaded {len(pairs_df)} existing pairs") # Create text edges DataFrame text_df = pd.DataFrame(text_edges, columns=["NAME_1", "NAME_2", "TEXT_SIMILARITY"]) # Merge with existing pairs # For existing pairs, add text similarity column # For new pairs, add as new rows with COUNT = 0 but TEXT_SIMILARITY > 0 # Normalize card names (sorted) text_df["PAIR_KEY"] = text_df.apply( lambda row: tuple(sorted([row["NAME_1"], row["NAME_2"]])), axis=1 ) # Add text similarity to existing pairs pairs_df["PAIR_KEY"] = pairs_df.apply( lambda row: tuple(sorted([row["NAME_1"], row["NAME_2"]])), axis=1 ) # Merge merged = pairs_df.merge( text_df[["PAIR_KEY", "TEXT_SIMILARITY"]], on="PAIR_KEY", how="left" ) # Add new text-only pairs existing_keys = set(merged["PAIR_KEY"]) new_pairs = [] for _, row in text_df.iterrows(): if row["PAIR_KEY"] not in existing_keys: new_pairs.append({ "NAME_1": row["NAME_1"], "NAME_2": row["NAME_2"], "COUNT_SET": 0, "COUNT_MULTISET": 0, "TEXT_SIMILARITY": row["TEXT_SIMILARITY"], "PAIR_KEY": row["PAIR_KEY"], }) if new_pairs: new_df = pd.DataFrame(new_pairs) merged = pd.concat([merged, new_df], ignore_index=True) logger.info(f" Added {len(new_pairs)} new text similarity pairs") # Drop pair key column merged = merged.drop(columns=["PAIR_KEY"]) # Save merged.to_csv(output_csv, index=False) logger.info(f" Saved enhanced pairs to {output_csv}") logger.info(f" Total pairs: {len(merged)}") logger.info(f" Pairs with text similarity: {merged['TEXT_SIMILARITY'].notna().sum()}") def main() -> int: """Integrate oracle text into training.""" parser = argparse.ArgumentParser(description="Integrate oracle text similarity") parser.add_argument("--card-attrs", type=Path, required=True, help="Card attributes CSV") parser.add_argument("--pairs", type=Path, help="Existing pairs CSV to enhance") parser.add_argument("--output", type=Path, required=True, help="Output CSV or edges file") parser.add_argument("--threshold", type=float, default=0.3, help="Similarity threshold") parser.add_argument("--max-edges", type=int, default=10, help="Max edges per card") parser.add_argument("--text-weight", type=float, default=2.0, help="Weight for text edges") args = parser.parse_args() if not HAS_DEPS: print("Error: Missing dependencies") return 1 try: # Load card attributes card_attrs = load_card_attributes(args.card_attrs) if not card_attrs: logger.error("No card attributes loaded") return 1 # Create text similarity edges text_edges = create_text_similarity_edges( card_attrs, threshold=args.threshold, max_edges_per_card=args.max_edges, ) # Merge into pairs if provided if args.pairs and args.pairs.exists(): merge_text_edges_into_pairs( args.pairs, text_edges, args.output, text_weight=args.text_weight, ) else: # Just save edges edges_df = pd.DataFrame(text_edges, columns=["card1", "card2", "similarity"]) edges_df.to_csv(args.output, index=False) logger.info(f" Saved {len(text_edges)} text similarity edges to {args.output}") return 0 except Exception as e: log_exception(logger, "Integration failed", e, include_context=True) return 1 if __name__ == "__main__": exit(main())
