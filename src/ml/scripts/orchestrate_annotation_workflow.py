#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pyyaml>=6.0", # ] # /// """ Orchestrate automated annotation workflow. Workflow: 1. Prioritize candidates (active learning) 2. Generate annotation batches 3. Run LLM annotations 4. Validate metadata 5. Compute IAA (if multiple annotators) 6. Score quality 7. Merge into test sets 8. Update graph """ from __future__ import annotations import argparse import json import subprocess from pathlib import Path from typing import Any import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) def run_workflow_step(step_name: str, command: list[str], dry_run: bool = False) -> dict[str, Any]: """Run a workflow step.""" print(f"\n{'=' * 70}") print(f"Step: {step_name}") print(f"{'=' * 70}") print(f"Command: {' '.join(command)}") print() if dry_run: print(" [DRY RUN - Would execute]") return {"status": "dry_run", "step": step_name} try: result = subprocess.run( command, capture_output=True, text=True, check=False, ) if result.returncode == 0: print(" Success") return { "status": "success", "step": step_name, "output": result.stdout, } else: print(f" Warning: Warning (exit code {result.returncode})") print(f" {result.stderr}") return { "status": "warning", "step": step_name, "error": result.stderr, } except Exception as e: print(f" Error: Error: {e}") return { "status": "error", "step": step_name, "error": str(e), } def orchestrate_annotation_workflow( game: str, target_annotations: int = 100, dry_run: bool = False, skip_llm: bool = False, ) -> dict[str, Any]: """Orchestrate full annotation workflow.""" workflow_results = { "steps": [], "status": "in_progress", } # Step 1: Prioritize candidates priority_output = f"experiments/annotation_priorities_{game}.json" step1 = run_workflow_step( "1. Prioritize Annotation Candidates", [ "uv", "run", "python", "src/ml/scripts/prioritize_annotation_candidates.py", "--game", game, "--output", priority_output, "--top-k", str(target_annotations), ], dry_run=dry_run, ) workflow_results["steps"].append(step1) # Step 2: Generate hand annotation batch batch_output = f"annotations/hand_batch_{game}_auto.yaml" step2 = run_workflow_step( "2. Generate Hand Annotation Batch", [ "just", "annotate-generate", game, str(target_annotations), "0", # current_queries ], dry_run=dry_run, ) workflow_results["steps"].append(step2) # Step 3: Run LLM annotations (optional) if not skip_llm: llm_output = f"annotations/llm_annotations_{game}_auto.jsonl" step3 = run_workflow_step( "3. Generate LLM Annotations", [ "just", "annotate-llm", "similarity=100", "strategy=diverse", ], dry_run=dry_run, ) workflow_results["steps"].append(step3) # Step 4: Validate metadata if not dry_run: # Find latest annotation files annotation_files = list(Path("annotations").glob(f"*{game}*.yaml")) + \ list(Path("annotations").glob(f"*{game}*.jsonl")) if annotation_files: latest_file = max(annotation_files, key=lambda p: p.stat().st_mtime) step4 = run_workflow_step( "4. Validate Annotation Metadata", [ "just", "annotate-validate", str(latest_file), "auto", ], dry_run=dry_run, ) workflow_results["steps"].append(step4) # Step 5: Score quality if not dry_run and annotation_files: quality_output = f"experiments/annotation_quality_{game}.json" step5 = run_workflow_step( "5. Score Annotation Quality", [ "uv", "run", "python", "src/ml/scripts/score_annotation_quality.py", "--input", str(latest_file), "--output", quality_output, ], dry_run=dry_run, ) workflow_results["steps"].append(step5) # Determine overall status failed_steps = [s for s in workflow_results["steps"] if s.get("status") == "error"] if failed_steps: workflow_results["status"] = "failed" elif all(s.get("status") in ("success", "dry_run") for s in workflow_results["steps"]): workflow_results["status"] = "success" else: workflow_results["status"] = "partial" return workflow_results def main() -> int: parser = argparse.ArgumentParser(description="Orchestrate annotation workflow") parser.add_argument("--game", type=str, required=True, choices=["magic", "pokemon", "yugioh"], help="Game to annotate") parser.add_argument("--target", type=int, default=100, help="Target number of annotations") parser.add_argument("--dry-run", action="store_true", help="Dry run (don't execute)") parser.add_argument("--skip-llm", action="store_true", help="Skip LLM annotation step") parser.add_argument("--output", type=str, help="Output JSON report") args = parser.parse_args() print("=" * 70) print("ANNOTATION WORKFLOW ORCHESTRATION") print("=" * 70) print() print(f"Game: {args.game}") print(f"Target annotations: {args.target}") print(f"Dry run: {args.dry_run}") print() results = orchestrate_annotation_workflow( game=args.game, target_annotations=args.target, dry_run=args.dry_run, skip_llm=args.skip_llm, ) print("\n" + "=" * 70) print("WORKFLOW SUMMARY") print("=" * 70) print() print(f"Status: {results['status']}") print(f"Steps completed: {len(results['steps'])}") print() for step in results["steps"]: status_icon = { "success": "", "warning": "Warning:", "error": "Error:", "dry_run": "üîç", }.get(step.get("status"), "‚ùì") print(f" {status_icon} {step.get('step', 'Unknown')}") # Save report if args.output: with open(args.output, "w") as f: json.dump(results, f, indent=2) print(f"\n‚úì Saved report to {args.output}") print() print("=" * 70) print(" Workflow orchestration complete!") print("=" * 70) return 0 if results["status"] in ("success", "partial") else 1 if __name__ == "__main__": import sys sys.exit(main())