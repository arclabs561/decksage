#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "sentence-transformers>=2.2.0", # "torch>=2.0.0", # "pandas>=2.0.0", # "numpy<2.0.0", # ] # /// """ Fine-tune instruction-tuned embeddings (E5/BGE) on card similarity tasks. Fine-tunes models like E5-base-v2, E5-Mistral, or BGE-M3 on our specific tasks: 1. Card substitution (functional replacements) 2. Deck completion (suggest cards to add) 3. Budget alternatives 4. Format-specific substitutes 5. Archetype-specific recommendations 6. Contextual discovery (synergies/alternatives/upgrades) Uses contrastive learning with positive/negative pairs from: - Substitution pairs - Test sets (queries + labels) - Graph edges (positive pairs) - Annotations (similarity labels) Training data format: - Positive pairs: (query_card, relevant_card, task_type) - Negative pairs: (query_card, irrelevant_card, task_type) """ from __future__ import annotations import argparse import json import logging from pathlib import Path from typing import Any try: from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation from torch.utils.data import DataLoader import torch HAS_DEPS = True except ImportError as e: HAS_DEPS = False print(f"Missing dependencies: {e}") import sys _script_file = Path(__file__).resolve() _src_dir = _script_file.parent.parent.parent if str(_src_dir) not in sys.path: sys.path.insert(0, str(_src_dir)) from ml.similarity.instruction_tuned_embeddings import InstructionTunedCardEmbedder from ml.utils.paths import PATHS from ml.utils.path_resolution import resolve_path try: from ml.utils.logging_config import setup_script_logging, log_exception except ImportError: try: from ..utils.logging_config import setup_script_logging, log_exception except ImportError: # Fallback: basic logging setup import logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) def setup_script_logging(): return logger def log_exception(logger, msg, e, include_context=False): logger.error(f"{msg}: {e}", exc_info=True) logger = setup_script_logging() def load_training_pairs( substitution_pairs_path: Path | None = None, test_set_path: Path | None = None, graph_path: Path | None = None, annotations_path: Path | None = None, multi_task_data_path: Path | None = None, task_type: str = "substitution", num_negatives_per_positive: int = 5, ) -> tuple[list[InputExample], list[InputExample]]: """ Load training pairs for fine-tuning. Returns: (positive_examples, negative_examples) """ positive_examples = [] negative_examples = [] # Load from multi-task data file (if provided) if multi_task_data_path and multi_task_data_path.exists(): logger.info(f"Loading multi-task training data from {multi_task_data_path}...") with open(multi_task_data_path) as f: multi_task_data = json.load(f) # Get task-specific pairs task_pairs = multi_task_data.get(task_type, []) logger.info(f" Found {len(task_pairs)} pairs for task '{task_type}'") instruction = InstructionTunedCardEmbedder.DEFAULT_INSTRUCTIONS.get( task_type, f"query: Find {task_type} for" ) for query, target in task_pairs: query_text = f"{instruction} {query}" target_text = f"{instruction} {target}" positive_examples.append(InputExample(texts=[query_text, target_text], label=1.0)) logger.info(f" Added {len(positive_examples)} positive examples from multi-task data") # Load substitution pairs substitution_pairs_resolved = resolve_path(substitution_pairs_path) if substitution_pairs_path else None if substitution_pairs_resolved: # Handle S3 paths if isinstance(substitution_pairs_resolved, str) and substitution_pairs_resolved.startswith("s3://"): import tempfile try: import boto3 s3 = boto3.client('s3') temp_pairs = Path(tempfile.mkdtemp()) / "substitution_pairs.json" logger.info(f"Downloading substitution pairs from S3 to {temp_pairs}...") bucket, key = substitution_pairs_resolved.replace("s3://", "").split("/", 1) s3.download_file(bucket, key, str(temp_pairs)) substitution_pairs_resolved = temp_pairs except Exception as e: logger.error(f"Failed to download from S3: {e}") substitution_pairs_resolved = None elif isinstance(substitution_pairs_resolved, Path) and substitution_pairs_resolved.exists(): pass # Use as-is else: substitution_pairs_resolved = None if substitution_pairs_resolved and isinstance(substitution_pairs_resolved, Path) and substitution_pairs_resolved.exists(): logger.info(f"Loading substitution pairs from {substitution_pairs_resolved}...") with open(substitution_pairs_resolved) as f: data = json.load(f) pairs = [] if isinstance(data, list): pairs = [(str(p[0]), str(p[1])) for p in data if len(p) >= 2] elif isinstance(data, dict) and "pairs" in data: pairs = [(str(p[0]), str(p[1])) for p in data["pairs"] if len(p) >= 2] for query, target in pairs: # Format with instruction instruction = InstructionTunedCardEmbedder.DEFAULT_INSTRUCTIONS.get( task_type, f"query: Find {task_type} for" ) query_text = f"{instruction} {query}" target_text = f"{instruction} {target}" positive_examples.append(InputExample(texts=[query_text, target_text], label=1.0)) logger.info(f" Loaded {len(positive_examples)} positive pairs from substitution file") # Load from test set test_set_resolved = resolve_path(test_set_path) if test_set_path else None if test_set_resolved: # Handle S3 paths if isinstance(test_set_resolved, str) and test_set_resolved.startswith("s3://"): import tempfile try: import boto3 s3 = boto3.client('s3') temp_test = Path(tempfile.mkdtemp()) / "test_set.json" logger.info(f"Downloading test set from S3 to {temp_test}...") bucket, key = test_set_resolved.replace("s3://", "").split("/", 1) s3.download_file(bucket, key, str(temp_test)) test_set_resolved = temp_test except Exception as e: logger.error(f"Failed to download from S3: {e}") test_set_resolved = None elif isinstance(test_set_resolved, Path) and test_set_resolved.exists(): pass # Use as-is else: test_set_resolved = None if test_set_resolved and isinstance(test_set_resolved, Path) and test_set_resolved.exists(): logger.info(f"Loading training pairs from test set: {test_set_resolved}...") try: with open(test_set_resolved) as f: test_data = json.load(f) queries = test_data.get("queries", {}) test_positives = 0 test_negatives = 0 for query_card, labels in queries.items(): # Positive: highly_relevant and relevant positives = labels.get("highly_relevant", []) + labels.get("relevant", []) # Negative: irrelevant negatives = labels.get("irrelevant", []) instruction = InstructionTunedCardEmbedder.DEFAULT_INSTRUCTIONS.get( task_type, f"query: Find {task_type} for" ) query_text = f"{instruction} {query_card}" for pos_card in positives: pos_text = f"{instruction} {pos_card}" positive_examples.append(InputExample(texts=[query_text, pos_text], label=1.0)) test_positives += 1 for neg_card in negatives[:num_negatives_per_positive * len(positives)]: neg_text = f"{instruction} {neg_card}" negative_examples.append(InputExample(texts=[query_text, neg_text], label=0.0)) test_negatives += 1 logger.info(f" Added {test_positives} positive, {test_negatives} negative from test set") except Exception as e: logger.warning(f"Failed to load test set: {e}") # Load from graph edges (positive pairs) graph_resolved = resolve_path(graph_path) if graph_path else None if graph_resolved: # Handle S3 paths if isinstance(graph_resolved, str) and graph_resolved.startswith("s3://"): import tempfile try: import boto3 s3 = boto3.client('s3') use_sqlite = graph_resolved.endswith(".db") temp_graph = Path(tempfile.mkdtemp()) / ("graph.db" if use_sqlite else "graph.json") logger.info(f"Downloading graph from S3 to {temp_graph}...") bucket, key = graph_resolved.replace("s3://", "").split("/", 1) s3.download_file(bucket, key, str(temp_graph)) graph_resolved = temp_graph except Exception as e: logger.error(f"Failed to download from S3: {e}") graph_resolved = None elif isinstance(graph_resolved, Path) and graph_resolved.exists(): pass # Use as-is else: graph_resolved = None if graph_resolved and isinstance(graph_resolved, Path) and graph_resolved.exists(): logger.info(f"Loading positive pairs from graph: {graph_resolved}...") try: from ml.data.incremental_graph import IncrementalCardGraph except ImportError: from ..data.incremental_graph import IncrementalCardGraph use_sqlite = graph_resolved.suffix == ".db" graph = IncrementalCardGraph(graph_resolved, use_sqlite=use_sqlite) # Sample edges as positive pairs (lower min_weight to get more examples) edges = graph.query_edges(min_weight=1) # Lower weight to get more examples logger.info(f" Found {len(edges)} edges in graph") if edges: instruction = InstructionTunedCardEmbedder.DEFAULT_INSTRUCTIONS.get( task_type, f"query: Find {task_type} for" ) # Sample up to 10K edges import random sample_size = min(10000, len(edges)) sampled_edges = random.sample(edges, sample_size) if len(edges) > sample_size else edges graph_positives = 0 for edge in sampled_edges: query_text = f"{instruction} {edge.card1}" target_text = f"{instruction} {edge.card2}" positive_examples.append(InputExample(texts=[query_text, target_text], label=1.0)) graph_positives += 1 logger.info(f" Added {graph_positives} positive pairs from graph") else: logger.warning(f" No edges found in graph (min_weight=1)") # Load from annotations if annotations_path and annotations_path.exists(): logger.info(f"Loading training pairs from annotations: {annotations_path}...") try: from ml.utils.annotation_utils import load_similarity_annotations, extract_substitution_pairs_from_annotations except ImportError: from ..utils.annotation_utils import load_similarity_annotations, extract_substitution_pairs_from_annotations annotations = load_similarity_annotations(annotations_path) pairs = extract_substitution_pairs_from_annotations(annotations, min_similarity=0.7) instruction = InstructionTunedCardEmbedder.DEFAULT_INSTRUCTIONS.get( task_type, f"query: Find {task_type} for" ) for query, target in pairs: query_text = f"{instruction} {query}" target_text = f"{instruction} {target}" positive_examples.append(InputExample(texts=[query_text, target_text], label=1.0)) logger.info(f" Added {len(pairs)} positive pairs from annotations") logger.info(f"Total: {len(positive_examples)} positive, {len(negative_examples)} negative examples") return positive_examples, negative_examples def fine_tune_instruction_embeddings( base_model_name: str = "intfloat/e5-base-v2", output_path: Path | None = None, substitution_pairs_path: Path | None = None, test_set_path: Path | None = None, graph_path: Path | None = None, annotations_path: Path | None = None, multi_task_data_path: Path | None = None, task_type: str = "substitution", epochs: int = 3, batch_size: int = 16, learning_rate: float = 2e-5, warmup_steps: int = 100, num_negatives_per_positive: int = 5, ) -> SentenceTransformer: """ Fine-tune instruction-tuned embeddings on card similarity tasks. Args: base_model_name: Base model to fine-tune (e.g., "intfloat/e5-base-v2") output_path: Path to save fine-tuned model substitution_pairs_path: Path to substitution pairs JSON test_set_path: Path to test set JSON graph_path: Path to incremental graph (for positive pairs) annotations_path: Path to annotations JSONL task_type: Task type ("substitution", "completion", etc.) epochs: Number of training epochs batch_size: Batch size learning_rate: Learning rate warmup_steps: Warmup steps num_negatives_per_positive: Number of negatives per positive Returns: Fine-tuned SentenceTransformer model """ if not HAS_DEPS: raise ImportError("sentence-transformers, torch required") logger.info("="*70) logger.info("FINE-TUNING INSTRUCTION EMBEDDINGS") logger.info("="*70) logger.info(f"Base model: {base_model_name}") logger.info(f"Task type: {task_type}") logger.info(f"Epochs: {epochs}, Batch size: {batch_size}, LR: {learning_rate}") logger.info("") # Load base model logger.info(f"Loading base model: {base_model_name}...") model = SentenceTransformer(base_model_name) logger.info("✓ Model loaded") # Load training data logger.info("Loading training data...") positive_examples, negative_examples = load_training_pairs( substitution_pairs_path=substitution_pairs_path, test_set_path=test_set_path, graph_path=graph_path, annotations_path=annotations_path, multi_task_data_path=multi_task_data_path, task_type=task_type, num_negatives_per_positive=num_negatives_per_positive, ) if not positive_examples: raise ValueError("No positive training examples found") # Combine examples all_examples = positive_examples + negative_examples logger.info(f"Total training examples: {len(all_examples)}") # Create data loader train_dataloader = DataLoader(all_examples, shuffle=True, batch_size=batch_size) # Define loss (contrastive loss for similarity learning) train_loss = losses.CosineSimilarityLoss(model) # Fine-tune logger.info("Starting fine-tuning...") model.fit( train_objectives=[(train_dataloader, train_loss)], epochs=epochs, warmup_steps=warmup_steps, optimizer_params={'lr': learning_rate}, output_path=str(output_path) if output_path else None, show_progress_bar=True, ) logger.info("✓ Fine-tuning complete") # Save model if output_path: output_path.parent.mkdir(parents=True, exist_ok=True) model.save(str(output_path)) logger.info(f"✓ Saved fine-tuned model to {output_path}") return model def main() -> int: """Fine-tune instruction embeddings.""" parser = argparse.ArgumentParser(description="Fine-tune instruction-tuned embeddings on card similarity tasks") parser.add_argument("--base-model", type=str, default="intfloat/e5-base-v2", help="Base model to fine-tune") parser.add_argument("--output", type=Path, required=True, help="Output directory for fine-tuned model") parser.add_argument("--substitution-pairs", type=Path, help="Substitution pairs JSON") parser.add_argument("--test-set", type=Path, help="Test set JSON (queries + labels)") parser.add_argument("--graph", type=Path, help="Incremental graph (for positive pairs)") parser.add_argument("--annotations", type=Path, help="Annotations JSONL") parser.add_argument("--multi-task-data", type=Path, help="Multi-task training data JSON (from prepare_instruction_finetuning_data.py)") parser.add_argument("--task-type", type=str, default="substitution", choices=["substitution", "completion", "budget", "format", "archetype", "similar", "synergy", "upgrade", "downgrade"], help="Task type") parser.add_argument("--epochs", type=int, default=3, help="Training epochs") parser.add_argument("--batch-size", type=int, default=16, help="Batch size") parser.add_argument("--learning-rate", type=float, default=2e-5, help="Learning rate") parser.add_argument("--warmup-steps", type=int, default=100, help="Warmup steps") parser.add_argument("--num-negatives", type=int, default=5, help="Number of negatives per positive") args = parser.parse_args() if not HAS_DEPS: logger.error("Missing dependencies: sentence-transformers, torch") return 1 try: model = fine_tune_instruction_embeddings( base_model_name=args.base_model, output_path=args.output, substitution_pairs_path=args.substitution_pairs, test_set_path=args.test_set, graph_path=args.graph, annotations_path=args.annotations, multi_task_data_path=args.multi_task_data, task_type=args.task_type, epochs=args.epochs, batch_size=args.batch_size, learning_rate=args.learning_rate, warmup_steps=args.warmup_steps, num_negatives_per_positive=args.num_negatives, ) logger.info("\n Fine-tuning complete!") return 0 except Exception as e: log_exception(logger, "Fine-tuning failed", e, include_context=True) return 1 if __name__ == "__main__": exit(main())