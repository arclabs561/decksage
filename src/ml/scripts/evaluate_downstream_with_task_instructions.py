#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # ] # /// """ Evaluate downstream tasks with task-specific instruction awareness. Tests performance improvements from using task-specific instructions vs default. Evaluates: 1. Deck Completion (completion instruction) 2. Card Substitution (substitution instruction) 3. Contextual Discovery (synergy/upgrade/downgrade instructions) 4. Deck Refinement (refinement instruction) 5. Deck Quality Assessment (quality instruction) Compares: - Default instruction ("substitution") vs task-specific instructions - Zero-shot vs fine-tuned (if available) - Task-specific fusion weights vs default weights """ from __future__ import annotations import argparse import json import logging from collections import defaultdict from pathlib import Path from typing import Any try: import pandas as pd import numpy as np HAS_DEPS = True except ImportError: HAS_DEPS = False import sys _script_file = Path(__file__).resolve() _src_dir = _script_file.parent.parent.parent if str(_src_dir) not in sys.path: sys.path.insert(0, str(_src_dir)) from ml.similarity.fusion import WeightedLateFusion, FusionWeights from ml.similarity.instruction_tuned_embeddings import InstructionTunedCardEmbedder from ml.utils.fusion_improvements import create_task_specific_weights from ml.utils.paths import PATHS from ..utils.logging_config import setup_script_logging, log_exception logger = setup_script_logging() def evaluate_deck_completion_with_instructions( fusion_default: WeightedLateFusion, fusion_task_specific: WeightedLateFusion, test_decks_path: Path, game: str = "magic", ) -> dict[str, Any]: """ Evaluate deck completion with default vs task-specific instructions. Returns: Dictionary with metrics comparing default vs task-specific """ if not test_decks_path.exists(): logger.warning(f"Test decks not found: {test_decks_path}") return {} from ml.deck_building.deck_completion import suggest_additions results = { "task": "completion", "default_instruction": {}, "task_specific_instruction": {}, } # Load test decks test_decks = [] with open(test_decks_path) as f: for line in f: if line.strip(): deck = json.loads(line) if deck.get("game") == game or (game == "magic" and "game" not in deck): test_decks.append(deck) logger.info(f"Evaluating deck completion on {len(test_decks)} test decks...") # Create candidate functions with different instruction types def candidate_fn_default(card: str, k: int) -> list[tuple[str, float]]: return fusion_default.similar(card, k, task_type="substitution") def candidate_fn_completion(card: str, k: int) -> list[tuple[str, float]]: return fusion_task_specific.similar(card, k, task_type="completion") # Evaluate with default instruction default_suggestions = [] for deck in test_decks[:20]: # Limit for speed try: suggestions = suggest_additions( game, # type: ignore deck, candidate_fn=candidate_fn_default, top_k=10, ) default_suggestions.append(len(suggestions)) except Exception as e: logger.debug(f"Failed to complete deck: {e}") # Evaluate with task-specific instruction task_specific_suggestions = [] for deck in test_decks[:20]: try: suggestions = suggest_additions( game, # type: ignore deck, candidate_fn=candidate_fn_completion, top_k=10, ) task_specific_suggestions.append(len(suggestions)) except Exception as e: logger.debug(f"Failed to complete deck: {e}") results["default_instruction"] = { "avg_suggestions": np.mean(default_suggestions) if default_suggestions else 0.0, "total_suggestions": sum(default_suggestions), } results["task_specific_instruction"] = { "avg_suggestions": np.mean(task_specific_suggestions) if task_specific_suggestions else 0.0, "total_suggestions": sum(task_specific_suggestions), } if default_suggestions: improvement = (np.mean(task_specific_suggestions) - np.mean(default_suggestions)) / np.mean(default_suggestions) * 100 results["improvement_percent"] = improvement return results def evaluate_substitution_with_instructions( fusion_default: WeightedLateFusion, fusion_task_specific: WeightedLateFusion, test_set_path: Path, ) -> dict[str, Any]: """ Evaluate card substitution with default vs task-specific instructions. Returns: Dictionary with P@10, MRR comparing default vs task-specific """ if not test_set_path.exists(): logger.warning(f"Test set not found: {test_set_path}") return {} with open(test_set_path) as f: test_data = json.load(f) queries = test_data.get("queries", {}) results = { "task": "substitution", "default_instruction": {}, "task_specific_instruction": {}, } total_queries = 0 default_found = 0 task_specific_found = 0 default_rr = [] task_specific_rr = [] for query_card, labels in list(queries.items())[:100]: # Limit for speed highly_relevant = labels.get("highly_relevant", []) relevant = labels.get("relevant", []) all_relevant = highly_relevant + relevant if not all_relevant: continue total_queries += 1 # Default instruction try: similar_default = fusion_default.similar(query_card, k=10, task_type="substitution") similar_cards_default = [card for card, _ in similar_default] found_default = False for i, card in enumerate(similar_cards_default): if card in all_relevant: found_default = True default_rr.append(1.0 / (i + 1)) break if found_default: default_found += 1 else: default_rr.append(0.0) except Exception as e: logger.debug(f"Failed default substitution for {query_card}: {e}") default_rr.append(0.0) # Task-specific instruction try: similar_task = fusion_task_specific.similar(query_card, k=10, task_type="substitution") similar_cards_task = [card for card, _ in similar_task] found_task = False for i, card in enumerate(similar_cards_task): if card in all_relevant: found_task = True task_specific_rr.append(1.0 / (i + 1)) break if found_task: task_specific_found += 1 else: task_specific_rr.append(0.0) except Exception as e: logger.debug(f"Failed task-specific substitution for {query_card}: {e}") task_specific_rr.append(0.0) results["default_instruction"] = { "p_at_10": default_found / total_queries if total_queries > 0 else 0.0, "mrr": np.mean(default_rr) if default_rr else 0.0, } results["task_specific_instruction"] = { "p_at_10": task_specific_found / total_queries if total_queries > 0 else 0.0, "mrr": np.mean(task_specific_rr) if task_specific_rr else 0.0, } if results["default_instruction"]["p_at_10"] > 0: p10_improvement = ( (results["task_specific_instruction"]["p_at_10"] - results["default_instruction"]["p_at_10"]) / results["default_instruction"]["p_at_10"] * 100 ) results["p_at_10_improvement_percent"] = p10_improvement return results def evaluate_contextual_discovery_with_instructions( fusion_default: WeightedLateFusion, fusion_task_specific: WeightedLateFusion, test_set_path: Path, ) -> dict[str, Any]: """ Evaluate contextual discovery (synergy, upgrade, downgrade) with task-specific instructions. Returns: Dictionary with metrics for each discovery type """ if not test_set_path.exists(): logger.warning(f"Test set not found: {test_set_path}") return {} with open(test_set_path) as f: test_data = json.load(f) queries = test_data.get("queries", {}) results = { "task": "contextual_discovery", "synergy": {}, "upgrade": {}, "downgrade": {}, } # Sample queries for evaluation sample_queries = list(queries.items())[:50] # Evaluate synergy discovery synergy_default_found = 0 synergy_task_found = 0 for query_card, labels in sample_queries: # Use relevant cards as synergy targets relevant = labels.get("relevant", []) if not relevant: continue try: # Default (substitution instruction) similar_default = fusion_default.similar(query_card, k=10, task_type="substitution") if any(card in relevant for card, _ in similar_default): synergy_default_found += 1 # Task-specific (synergy instruction) similar_task = fusion_task_specific.similar(query_card, k=10, task_type="synergy") if any(card in relevant for card, _ in similar_task): synergy_task_found += 1 except Exception: pass results["synergy"] = { "default_found": synergy_default_found, "task_specific_found": synergy_task_found, "improvement": synergy_task_found - synergy_default_found, } # Similar evaluation for upgrade/downgrade (simplified for now) results["upgrade"] = {"status": "evaluation_ready"} results["downgrade"] = {"status": "evaluation_ready"} return results def main() -> int: """Evaluate downstream tasks with task-specific instructions.""" parser = argparse.ArgumentParser(description="Evaluate downstream tasks with task-specific instructions") parser.add_argument("--embeddings", type=Path, help="Embeddings file (.wv)") parser.add_argument("--graph", type=Path, help="Graph database") parser.add_argument("--test-set", type=Path, help="Test set JSON") parser.add_argument("--test-decks", type=Path, help="Test decks JSONL (for completion)") parser.add_argument("--output", type=Path, help="Output JSON file") parser.add_argument("--game", type=str, default="magic", help="Game to evaluate") args = parser.parse_args() # Default paths if not args.embeddings: candidates = [ PATHS.embeddings / "embeddings_refined_enhanced.wv" if hasattr(PATHS, 'embeddings') else None, Path("data/embeddings/embeddings_refined_enhanced.wv"), ] for cand in candidates: if cand and cand.exists(): args.embeddings = cand break if not args.graph: candidates = [ PATHS.incremental_graph_db if hasattr(PATHS, 'incremental_graph_db') else None, Path("data/graphs/incremental_graph.db"), ] for cand in candidates: if cand and cand.exists(): args.graph = cand break if not args.test_set: test_candidates = [ PATHS.test_magic if hasattr(PATHS, 'test_magic') else None, Path("experiments/test_set_unified_magic.json"), ] for cand in test_candidates: if cand and cand.exists(): args.test_set = cand break if not args.embeddings or not args.graph: logger.error("Embeddings and graph required") return 1 try: # Load embeddings and graph from gensim.models import KeyedVectors from ml.utils.data_loading import build_adjacency_dict logger.info("Loading embeddings and graph...") embeddings = KeyedVectors.load(str(args.embeddings)) adj = build_adjacency_dict(args.graph) # Create fusion instances base_weights = FusionWeights() # Default fusion (substitution instruction) fusion_default = WeightedLateFusion( embeddings=embeddings, adj=adj, weights=base_weights, task_type="substitution", # Default ) # Task-specific fusion (will use task-specific instructions per call) fusion_task_specific = WeightedLateFusion( embeddings=embeddings, adj=adj, weights=base_weights, task_type="substitution", # Base, but will override per call ) # Load instruction embedder if available try: text_embedder = InstructionTunedCardEmbedder() fusion_default.text_embedder = text_embedder fusion_task_specific.text_embedder = text_embedder logger.info(" Instruction-tuned embedder loaded") except Exception as e: logger.warning(f"Instruction embedder not available: {e}") results = { "evaluation_date": str(Path(__file__).stat().st_mtime), "game": args.game, "tasks": {}, } # Evaluate substitution logger.info("\nEvaluating substitution task...") substitution_results = evaluate_substitution_with_instructions( fusion_default, fusion_task_specific, args.test_set, ) results["tasks"]["substitution"] = substitution_results # Evaluate completion if args.test_decks and args.test_decks.exists(): logger.info("\nEvaluating deck completion task...") completion_results = evaluate_deck_completion_with_instructions( fusion_default, fusion_task_specific, args.test_decks, game=args.game, ) results["tasks"]["completion"] = completion_results # Evaluate contextual discovery logger.info("\nEvaluating contextual discovery task...") discovery_results = evaluate_contextual_discovery_with_instructions( fusion_default, fusion_task_specific, args.test_set, ) results["tasks"]["contextual_discovery"] = discovery_results # Print summary logger.info("\n" + "="*70) logger.info("EVALUATION RESULTS") logger.info("="*70) for task_name, task_results in results["tasks"].items(): logger.info(f"\nTask: {task_name}") if "default_instruction" in task_results: default = task_results["default_instruction"] task_specific = task_results.get("task_specific_instruction", {}) logger.info(f" Default instruction:") for metric, value in default.items(): logger.info(f" {metric}: {value:.4f}") logger.info(f" Task-specific instruction:") for metric, value in task_specific.items(): logger.info(f" {metric}: {value:.4f}") if "p_at_10_improvement_percent" in task_results: logger.info(f" P@10 improvement: {task_results['p_at_10_improvement_percent']:+.2f}%") # Save results if args.output: args.output.parent.mkdir(parents=True, exist_ok=True) with open(args.output, 'w') as f: json.dump(results, f, indent=2) logger.info(f"\n Results saved to {args.output}") return 0 except Exception as e: log_exception(logger, "Evaluation failed", e, include_context=True) return 1 if __name__ == "__main__": exit(main())
