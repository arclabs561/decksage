#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pyyaml>=6.0", # ] # /// """ Diagnose annotation system completeness: 1. IAA tracking 2. S3 sync status 3. Metadata tracking (model/params/prompts) 4. Graph integration 5. Downstream usage """ from __future__ import annotations import json import subprocess from pathlib import Path from typing import Any try: import yaml HAS_YAML = True except ImportError: HAS_YAML = False import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) def check_iaa_tracking() -> dict[str, Any]: """Check if IAA is being tracked in annotations.""" results = { "has_iaa_module": False, "iaa_in_hand_annotations": False, "iaa_in_llm_annotations": False, "iaa_in_test_sets": False, "issues": [], } # Check if IAA module exists try: from ml.evaluation.inter_annotator_agreement import InterAnnotatorAgreement results["has_iaa_module"] = True except ImportError: results["issues"].append("IAA module not available") return results # Check hand annotations hand_annotations = list(Path("annotations").glob("*.yaml")) for ann_file in hand_annotations[:3]: # Check first 3 try: with open(ann_file) as f: data = yaml.safe_load(f) tasks = data.get("tasks", []) for task in tasks: if "iaa" in str(task).lower(): results["iaa_in_hand_annotations"] = True break except Exception: pass # Check test sets test_sets = [ Path("experiments/test_set_unified_magic.json"), Path("experiments/test_set_unified_pokemon.json"), ] for test_set in test_sets: if test_set.exists(): try: with open(test_set) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data, dict) else data for query_data in list(queries.values())[:5]: # Check first 5 if isinstance(query_data, dict) and "iaa" in query_data: results["iaa_in_test_sets"] = True break except Exception: pass return results def check_s3_sync() -> dict[str, Any]: """Check if annotations are synced to S3.""" results = { "local_annotations": 0, "s3_annotations": 0, "sync_script_exists": False, "issues": [], } # Count local annotations if Path("annotations").exists(): results["local_annotations"] = len(list(Path("annotations").glob("*.yaml"))) + \ len(list(Path("annotations").glob("*.jsonl"))) # Check sync script sync_script = Path("scripts/data_processing/sync_to_s3.sh") if sync_script.exists(): results["sync_script_exists"] = True # Check if annotations are included with open(sync_script) as f: content = f.read() if "annotations" in content.lower(): results["annotations_in_sync_script"] = True # Try to check S3 (may fail if no credentials) try: result = subprocess.run( ["aws", "s3", "ls", "s3://games-collections/annotations/"], capture_output=True, text=True, timeout=5, ) if result.returncode == 0: results["s3_annotations"] = len([l for l in result.stdout.split("\n") if l.strip()]) except Exception: results["issues"].append("Cannot access S3 (credentials or network issue)") return results def check_metadata_tracking() -> dict[str, Any]: """Check if LLM annotations track model/params/prompts.""" results = { "has_model_tracking": False, "has_param_tracking": False, "has_prompt_tracking": False, "sample_annotations": [], "issues": [], } # Check LLM annotation files llm_annotations = list(Path("annotations").glob("*similarity*.jsonl")) + \ list(Path("experiments/annotations_llm").glob("*.jsonl") if Path("experiments/annotations_llm").exists() else []) for ann_file in llm_annotations[:3]: # Check first 3 try: with open(ann_file) as f: for i, line in enumerate(f): if i >= 2: # Check first 2 annotations break if line.strip(): ann = json.loads(line) results["sample_annotations"].append(ann) # Check for metadata fields (new fields we added) if "model_name" in ann or "model" in ann: results["has_model_tracking"] = True if "model_params" in ann or "temperature" in ann or "params" in ann: results["has_param_tracking"] = True if "prompt_hash" in ann or "prompt" in ann or "system_prompt" in ann: results["has_prompt_tracking"] = True if "annotator_id" in ann or "timestamp" in ann: results["has_model_tracking"] = True # Metadata tracking exists except Exception as e: results["issues"].append(f"Error reading {ann_file}: {e}") return results def check_graph_integration() -> dict[str, Any]: """Check if annotations are used in graph metadata for GNN.""" results = { "substitution_in_edgelist": False, "annotations_in_graph_metadata": False, "gnn_uses_annotations": False, "issues": [], } # Check if substitution pairs are added to graph # This is done in train_multitask_refined_enhanced.py train_script = Path("src/ml/scripts/train_multitask_refined_enhanced.py") if train_script.exists(): with open(train_script) as f: content = f.read() if "substitution_pairs" in content and "edge" in content.lower(): results["substitution_in_edgelist"] = True # Check GNN code gnn_script = Path("src/ml/similarity/gnn_embeddings.py") if gnn_script.exists(): with open(gnn_script) as f: content = f.read() if "annotation" in content.lower() or "substitution" in content.lower(): results["gnn_uses_annotations"] = True # Check graph metadata structure graph_file = Path("data/graphs/incremental_graph.json") if graph_file.exists(): try: with open(graph_file) as f: data = json.load(f) # Check if edges have metadata field if "edges" in data: edges = data["edges"] if edges and isinstance(edges, list): first_edge = edges[0] if edges else {} if isinstance(first_edge, dict) and "metadata" in first_edge: results["annotations_in_graph_metadata"] = True except Exception: pass return results def check_downstream_usage() -> dict[str, Any]: """Check if annotations are actually used in downstream tasks.""" results = { "used_in_training": False, "used_in_evaluation": False, "used_in_downstream_tasks": False, "issues": [], } # Check training scripts train_scripts = [ Path("src/ml/scripts/train_multitask_refined_enhanced.py"), Path("src/ml/scripts/train_hybrid_full.py"), ] for script in train_scripts: if script.exists(): with open(script) as f: content = f.read() if "substitution_pairs" in content or "similarity_annotations" in content: results["used_in_training"] = True break # Check evaluation scripts eval_scripts = [ Path("src/ml/scripts/evaluate_downstream_complete.py"), Path("src/ml/scripts/evaluate_multitask.py"), ] for script in eval_scripts: if script.exists(): with open(script) as f: content = f.read() if "substitution" in content.lower() or "annotation" in content.lower(): results["used_in_evaluation"] = True break # Check downstream task scripts downstream_scripts = [ Path("src/ml/scripts/evaluate_downstream_complete.py"), Path("src/ml/deck_building/deck_completion.py"), ] for script in downstream_scripts: if script.exists(): with open(script) as f: content = f.read() if "substitution" in content.lower() or "annotation" in content.lower(): results["used_in_downstream_tasks"] = True break return results def main() -> int: """Run all diagnostic checks.""" print("=" * 70) print("ANNOTATION SYSTEM DIAGNOSTICS") print("=" * 70) print() # 1. IAA Tracking print("1. IAA (Inter-Annotator Agreement) Tracking") print("-" * 70) iaa_results = check_iaa_tracking() print(f" IAA module available: {iaa_results['has_iaa_module']}") print(f" IAA in hand annotations: {iaa_results['iaa_in_hand_annotations']}") print(f" IAA in test sets: {iaa_results['iaa_in_test_sets']}") if iaa_results['issues']: print(f" Warning: Issues: {', '.join(iaa_results['issues'])}") print() # 2. S3 Sync print("2. S3 Sync Status") print("-" * 70) s3_results = check_s3_sync() print(f" Local annotations: {s3_results['local_annotations']}") print(f" S3 annotations: {s3_results['s3_annotations']}") print(f" Sync script exists: {s3_results['sync_script_exists']}") if s3_results.get('annotations_in_sync_script'): print(f" Annotations included in sync script") if s3_results['issues']: print(f" Warning: Issues: {', '.join(s3_results['issues'])}") print() # 3. Metadata Tracking print("3. LLM Annotation Metadata Tracking") print("-" * 70) metadata_results = check_metadata_tracking() print(f" Model tracking: {metadata_results['has_model_tracking']}") print(f" Parameter tracking: {metadata_results['has_param_tracking']}") print(f" Prompt tracking: {metadata_results['has_prompt_tracking']}") if metadata_results['issues']: print(f" Warning: Issues: {', '.join(metadata_results['issues'])}") print() # 4. Graph Integration print("4. Graph Integration (GNN)") print("-" * 70) graph_results = check_graph_integration() print(f" Substitution pairs in edgelist: {graph_results['substitution_in_edgelist']}") print(f" Annotations in graph metadata: {graph_results['annotations_in_graph_metadata']}") print(f" GNN uses annotations: {graph_results['gnn_uses_annotations']}") if graph_results['issues']: print(f" Warning: Issues: {', '.join(graph_results['issues'])}") print() # 5. Downstream Usage print("5. Downstream Task Usage") print("-" * 70) downstream_results = check_downstream_usage() print(f" Used in training: {downstream_results['used_in_training']}") print(f" Used in evaluation: {downstream_results['used_in_evaluation']}") print(f" Used in downstream tasks: {downstream_results['used_in_downstream_tasks']}") if downstream_results['issues']: print(f" Warning: Issues: {', '.join(downstream_results['issues'])}") print() # Summary print("=" * 70) print("SUMMARY") print("=" * 70) all_good = ( iaa_results['has_iaa_module'] and s3_results['sync_script_exists'] and graph_results['substitution_in_edgelist'] and downstream_results['used_in_training'] ) if all_good: print(" Core functionality working") else: print("Warning: Some issues detected - see details above") # Save report report = { "iaa": iaa_results, "s3_sync": s3_results, "metadata": metadata_results, "graph_integration": graph_results, "downstream_usage": downstream_results, } report_path = Path("experiments/annotation_diagnostics.json") with open(report_path, "w") as f: json.dump(report, f, indent=2) print(f"\nðŸ“„ Full report saved to: {report_path}") return 0 if all_good else 1 if __name__ == "__main__": import sys sys.exit(main())
