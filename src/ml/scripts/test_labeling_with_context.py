#!/usr/bin/env python3 """ Test labeling with enhanced context loading. Shows the improvements in action with real card data. """ import sys from pathlib import Path from ml.utils.logging_config import setup_script_logging sys.path.insert(0, str(Path(__file__).parent.parent.parent)) from ml.scripts.generate_labels_enhanced import load_card_context, make_enhanced_label_agent from ml.utils.pydantic_ai_helpers import get_default_model import logging s: %(message)s') logger = setup_script_logging() def test_labeling_with_context(): """Test labeling with full context.""" print("\n" + "="*60) print("TESTING LABELING WITH ENHANCED CONTEXT") print("="*60) # Test cards test_query = "Lightning Bolt" card_attrs_path = Path(__file__).parent.parent.parent.parent / "data" / "processed" / "card_attributes_enriched.csv" if not card_attrs_path.exists(): print(f"Warning: Card attributes not found, creating minimal test...") # Show what would be loaded print(f" Would load context from: {card_attrs_path}") return print(f"\nðŸ“‹ Loading context for: {test_query}") print(f" From: {card_attrs_path.name}") context = load_card_context(test_query, card_attrs_path) if context: print(f"\n Context loaded successfully!") print(f"\n Context Fields:") for key, value in context.items(): if value: # Only show non-empty fields display_value = str(value)[:100] + "..." if len(str(value)) > 100 else str(value) print(f" {key:20} = {display_value}") print(f"\n This context would be used in labeling prompts") print(f" - Oracle text for understanding card function") print(f" - Type, CMC, colors for similarity matching") print(f" - Power/toughness for creature comparisons") print(f" - Keywords for ability-based similarity") else: print(f"Warning: No context found for {test_query}") # Test model selection print(f"\nðŸ¤– Model Selection:") model = get_default_model("annotator") print(f" Default annotator model: {model}") print(f" (Would use this for labeling with full context)") # Show what the prompt would include if context: print(f"\nðŸ“ Example Prompt Context (what LLM would see):") print(f" Card: {test_query}") if context.get("type"): print(f" Type: {context['type']}") if context.get("mana_cost"): print(f" Mana Cost: {context['mana_cost']}") if context.get("cmc"): print(f" CMC: {context['cmc']}") if context.get("oracle_text"): oracle_preview = context['oracle_text'][:150] + "..." if len(context['oracle_text']) > 150 else context['oracle_text'] print(f" Oracle Text: {oracle_preview}") if __name__ == "__main__": test_labeling_with_context()