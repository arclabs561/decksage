#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # /// """ AWS S3 Data Operations Download/upload embeddings, signals, and data from/to S3. Uses PEP 723 inline dependencies (none needed - uses subprocess for AWS CLI). """ import json import subprocess import sys from pathlib import Path from typing import Optional def run_aws_command(cmd: list[str], description: str) -> tuple[bool, str]: """Run AWS CLI command.""" print(f"\n{'='*70}") print(f"{description}") print(f"Command: aws {' '.join(cmd)}") print(f"{'='*70}\n") try: result = subprocess.run( ["aws"] + cmd, capture_output=True, text=True, check=False, ) if result.returncode == 0: return True, result.stdout else: return False, result.stderr or result.stdout except Exception as e: return False, str(e) def check_s3_path(s3_path: str) -> bool: """Check if S3 path exists.""" # Extract bucket and prefix if not s3_path.startswith("s3://"): return False parts = s3_path[5:].split("/", 1) bucket = parts[0] prefix = parts[1] if len(parts) > 1 else "" cmd = ["s3", "ls", s3_path] success, output = run_aws_command(cmd, f"Checking {s3_path}") return success def download_from_s3(s3_path: str, local_path: Path, description: str = "") -> bool: """Download file/directory from S3.""" local_path.parent.mkdir(parents=True, exist_ok=True) cmd = ["s3", "cp", s3_path, str(local_path)] if local_path.is_dir() or str(local_path).endswith("/"): cmd.append("--recursive") success, output = run_aws_command(cmd, f"Downloading {description or s3_path}") if success: print(f" Downloaded to {local_path}") return True else: print(f" Error: Failed: {output[:200]}") return False def upload_to_s3(local_path: Path, s3_path: str, description: str = "") -> bool: """Upload file/directory to S3.""" if not local_path.exists(): print(f" Error: Local path does not exist: {local_path}") return False cmd = ["s3", "cp", str(local_path), s3_path] if local_path.is_dir(): cmd.append("--recursive") success, output = run_aws_command(cmd, f"Uploading {description or local_path}") if success: print(f" Uploaded to {s3_path}") return True else: print(f" Error: Failed: {output[:200]}") return False def list_s3_prefix(s3_prefix: str) -> list[str]: """List objects in S3 prefix.""" cmd = ["s3", "ls", s3_prefix] success, output = run_aws_command(cmd, f"Listing {s3_prefix}") if not success: return [] objects = [] for line in output.strip().split("\n"): if line.strip(): # Parse: "2024-01-01 12:00:00 123456 file.txt" parts = line.strip().split() if len(parts) >= 4: obj_name = " ".join(parts[3:]) objects.append(obj_name) return objects def main() -> int: """Main operations.""" import argparse parser = argparse.ArgumentParser(description="AWS S3 data operations") parser.add_argument( "--bucket", type=str, help="S3 bucket name (e.g., decksage-data)", ) parser.add_argument( "--operation", choices=["download", "upload", "list", "check"], required=True, help="Operation to perform", ) parser.add_argument( "--resource", choices=["embeddings", "signals", "pairs", "test-sets", "all"], help="Resource type to operate on", ) parser.add_argument( "--s3-path", type=str, help="Full S3 path (s3://bucket/path)", ) parser.add_argument( "--local-path", type=str, help="Local path", ) args = parser.parse_args() # Default bucket if not provided bucket = args.bucket or "decksage-data" base_prefix = f"s3://{bucket}/" print("=" * 70) print("AWS S3 Data Operations") print("=" * 70) print() if args.operation == "check": # Check what's available in S3 print("Checking S3 resources...") print() resources = { "embeddings": f"{base_prefix}embeddings/", "signals": f"{base_prefix}signals/", "pairs": f"{base_prefix}processed/", "test-sets": f"{base_prefix}test-sets/", } for name, s3_path in resources.items(): print(f" {name}: {s3_path}") objects = list_s3_prefix(s3_path) if objects: print(f" Found {len(objects)} objects") for obj in objects[:5]: print(f" - {obj}") if len(objects) > 5: print(f" ... and {len(objects) - 5} more") else: print(f" Error: Not found or empty") print() elif args.operation == "list": # List objects s3_path = args.s3_path or f"{base_prefix}{args.resource}/" print(f"Listing: {s3_path}") objects = list_s3_prefix(s3_path) for obj in objects: print(f" {obj}") elif args.operation == "download": # Download resources if args.resource == "embeddings": s3_path = args.s3_path or f"{base_prefix}embeddings/" local_path = Path(args.local_path or "data/embeddings/") download_from_s3(s3_path, local_path, "embeddings") elif args.resource == "signals": s3_path = args.s3_path or f"{base_prefix}signals/" local_path = Path(args.local_path or "experiments/signals/") download_from_s3(s3_path, local_path, "signals") elif args.resource == "pairs": s3_path = args.s3_path or f"{base_prefix}processed/pairs_large.csv" local_path = Path(args.local_path or "data/processed/pairs_large.csv") download_from_s3(s3_path, local_path, "pairs CSV") elif args.resource == "all": # Download all resources = { "embeddings": (f"{base_prefix}embeddings/", Path("data/embeddings/")), "signals": (f"{base_prefix}signals/", Path("experiments/signals/")), "pairs": (f"{base_prefix}processed/pairs_large.csv", Path("data/processed/pairs_large.csv")), } for name, (s3, local) in resources.items(): download_from_s3(s3, local, name) elif args.operation == "upload": # Upload resources if args.resource == "embeddings": local_path = Path(args.local_path or "data/embeddings/") s3_path = args.s3_path or f"{base_prefix}embeddings/" upload_to_s3(local_path, s3_path, "embeddings") elif args.resource == "signals": local_path = Path(args.local_path or "experiments/signals/") s3_path = args.s3_path or f"{base_prefix}signals/" upload_to_s3(local_path, s3_path, "signals") elif args.resource == "pairs": local_path = Path(args.local_path or "data/processed/pairs_large.csv") s3_path = args.s3_path or f"{base_prefix}processed/pairs_large.csv" upload_to_s3(local_path, s3_path, "pairs CSV") print() print("=" * 70) return 0 if __name__ == "__main__": sys.exit(main())
