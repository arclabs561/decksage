#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # "gensim>=4.3.0", # ] # /// """ Test script to validate training refinements and measure improvements. Tests: 1. Log normalization vs no normalization 2. Different negative sampling counts 3. Different batch sizes 4. Hard negative mining vs standard training 5. Parameter combinations Outputs comparison metrics (MRR, P@10, R@10) for each configuration. """ from __future__ import annotations import argparse import json import logging import sys import tempfile from pathlib import Path from typing import Any from ml.utils.logging_config import setup_script_logging # Add project root to path _script_file = Path(__file__).resolve() _src_dir = _script_file.parent.parent.parent if str(_src_dir) not in sys.path: sys.path.insert(0, str(_src_dir)) try: import pandas as pd import numpy as np from gensim.models import Word2Vec, KeyedVectors HAS_DEPS = True except ImportError as e: HAS_DEPS = False print(f"Missing dependencies: {e}") s - %(levelname)s - %(message)s') logger = setup_script_logging() def create_minimal_test_data(num_pairs: int = 1000) -> tuple[Path, list[tuple[str, str]]]: """Create minimal test data for quick validation.""" import random # Create synthetic card pairs cards = [f"Card_{i}" for i in range(100)] pairs = [] test_pairs = [] for _ in range(num_pairs): card1, card2 = random.sample(cards, 2) pairs.append((card1, card2)) # Create some test pairs if len(test_pairs) < 50: test_pairs.append((card1, card2)) # Write to temp CSV temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) temp_path = Path(temp_file.name) df = pd.DataFrame(pairs, columns=['CARD1', 'CARD2']) df['COUNT_MULTISET'] = np.random.randint(1, 10, len(df)) df.to_csv(temp_path, index=False) return temp_path, test_pairs def train_baseline( pairs_path: Path, output_path: Path, dim: int = 64, epochs: int = 5, **kwargs: Any, ) -> KeyedVectors: """Train baseline model with default parameters.""" logger.info(f"Training baseline model...") # Load pairs df = pd.read_csv(pairs_path) # Create simple walks (just pairs as 2-node walks) walks = [] for _, row in df.iterrows(): walks.append([str(row['CARD1']), str(row['CARD2'])]) # Train model = Word2Vec( sentences=walks, vector_size=dim, window=5, min_count=1, workers=2, epochs=epochs, sg=1, **kwargs, ) wv = model.wv wv.save(str(output_path)) logger.info(f" Saved baseline to {output_path}") return wv def train_refined( pairs_path: Path, output_path: Path, dim: int = 64, epochs: int = 5, negative: int = 5, batch_words: int = 10000, sample: float = 1e-3, alpha: float = 0.025, min_alpha: float = 0.0001, **kwargs: Any, ) -> KeyedVectors: """Train refined model with research-based parameters.""" logger.info(f"Training refined model (negative={negative}, batch_words={batch_words})...") # Load pairs df = pd.read_csv(pairs_path) # Create simple walks walks = [] for _, row in df.iterrows(): walks.append([str(row['CARD1']), str(row['CARD2'])]) # Train with refined parameters model = Word2Vec( sentences=walks, vector_size=dim, window=5, min_count=1, workers=2, epochs=epochs, sg=1, negative=negative, sample=sample, alpha=alpha, min_alpha=min_alpha, batch_words=batch_words, **kwargs, ) wv = model.wv wv.save(str(output_path)) logger.info(f" Saved refined to {output_path}") return wv def evaluate_ranking_quality( wv: KeyedVectors, test_pairs: list[tuple[str, str]], k: int = 10, ) -> dict[str, float]: """Evaluate ranking quality (MRR, P@K, R@K).""" mrr_sum = 0.0 precision_sum = 0.0 recall_sum = 0.0 num_evaluated = 0 for query, relevant in test_pairs: if query not in wv or relevant not in wv: continue try: # Get most similar similar_items = wv.most_similar(query, topn=k * 2) similar_items_dict = {item: score for item, score in similar_items} # MRR rank = None for i, (item, _) in enumerate(similar_items[:k], 1): if item == relevant: rank = i break if rank is not None: mrr_sum += 1.0 / rank precision_sum += 1.0 else: # Check extended list for recall for i, (item, _) in enumerate(similar_items, 1): if item == relevant: recall_sum += 1.0 / min(i, k) break num_evaluated += 1 except Exception as e: logger.warning(f"Error evaluating ({query}, {relevant}): {e}") continue if num_evaluated == 0: return {'mrr': 0.0, 'p@k': 0.0, 'r@k': 0.0, 'num_evaluated': 0} return { 'mrr': mrr_sum / num_evaluated, 'p@k': precision_sum / num_evaluated, 'r@k': recall_sum / num_evaluated, 'num_evaluated': num_evaluated, } def test_parameter_sweep( pairs_path: Path, test_pairs: list[tuple[str, str]], output_dir: Path, quick: bool = False, ) -> dict[str, Any]: """Test different parameter combinations. Based on research findings: - Negative sampling: 5-20 optimal for large vocabularies - Batch size: Larger batches (10K+) improve MRR through diverse negatives - Subsampling: 1e-3 to 1e-5 typical for frequent word downsampling """ results = {} # Test configurations (research-based) if quick: # Quick test with fewer configs configs = [ {'name': 'baseline', 'negative': 5, 'batch_words': 1000, 'sample': 0.0, 'alpha': 0.025, 'min_alpha': 0.0001}, {'name': 'refined', 'negative': 5, 'batch_words': 10000, 'sample': 1e-3, 'alpha': 0.025, 'min_alpha': 0.0001}, ] else: # Full parameter sweep configs = [ # Baseline (old defaults) {'name': 'baseline', 'negative': 5, 'batch_words': 1000, 'sample': 0.0, 'alpha': 0.025, 'min_alpha': 0.0001}, # Research-based refinements {'name': 'refined_default', 'negative': 5, 'batch_words': 10000, 'sample': 1e-3, 'alpha': 0.025, 'min_alpha': 0.0001}, {'name': 'refined_high_negative', 'negative': 10, 'batch_words': 10000, 'sample': 1e-3, 'alpha': 0.025, 'min_alpha': 0.0001}, {'name': 'refined_large_batch', 'negative': 5, 'batch_words': 20000, 'sample': 1e-3, 'alpha': 0.025, 'min_alpha': 0.0001}, {'name': 'refined_combined', 'negative': 10, 'batch_words': 20000, 'sample': 1e-3, 'alpha': 0.025, 'min_alpha': 0.0001}, # Learning rate variations {'name': 'refined_high_lr', 'negative': 5, 'batch_words': 10000, 'sample': 1e-3, 'alpha': 0.05, 'min_alpha': 0.0001}, {'name': 'refined_low_lr', 'negative': 5, 'batch_words': 10000, 'sample': 1e-3, 'alpha': 0.01, 'min_alpha': 0.0001}, ] for config in configs: logger.info(f"\n{'='*70}") logger.info(f"Testing: {config['name']}") logger.info(f"{'='*70}") output_path = output_dir / f"{config['name']}.wv" try: wv = train_refined( pairs_path=pairs_path, output_path=output_path, dim=64, epochs=5, negative=config['negative'], batch_words=config['batch_words'], sample=config['sample'], alpha=config.get('alpha', 0.025), min_alpha=config.get('min_alpha', 0.0001), ) metrics = evaluate_ranking_quality(wv, test_pairs, k=10) results[config['name']] = { 'config': config, 'metrics': metrics, } logger.info(f" MRR: {metrics['mrr']:.4f}") logger.info(f" P@10: {metrics['p@k']:.4f}") logger.info(f" R@10: {metrics['r@k']:.4f}") except Exception as e: logger.error(f" Failed: {e}") results[config['name']] = {'error': str(e)} return results def main() -> int: parser = argparse.ArgumentParser(description="Test training refinements") parser.add_argument( "--pairs", type=Path, help="Path to pairs CSV (default: create minimal test data)", ) parser.add_argument( "--output-dir", type=Path, default=Path("data/test_training"), help="Output directory for test models", ) parser.add_argument( "--quick", action="store_true", help="Quick test with minimal data", ) args = parser.parse_args() if not HAS_DEPS: logger.error("Missing dependencies") return 1 # Create output directory args.output_dir.mkdir(parents=True, exist_ok=True) # Load test set if provided (try canonical loader first) test_pairs = None if args.test_set and args.test_set.exists(): try: # Try using canonical loader try: from ml.utils.data_loading import load_test_set test_data = load_test_set(args.test_set) # Extract test pairs from test set test_pairs = [] for query, labels in test_data.items(): if isinstance(labels, dict): # Take first highly_relevant, then first relevant for relevant in labels.get('highly_relevant', [])[:1]: test_pairs.append((query, relevant)) if not labels.get('highly_relevant'): for relevant in labels.get('relevant', [])[:1]: test_pairs.append((query, relevant)) logger.info(f"Loaded {len(test_pairs)} test pairs from {args.test_set} (via canonical loader)") except ImportError: # Fallback to direct JSON loading import json with open(args.test_set) as f: test_data = json.load(f) test_pairs = [] if isinstance(test_data, dict): for query, labels in test_data.items(): if isinstance(labels, dict): for relevant in labels.get('highly_relevant', [])[:1]: test_pairs.append((query, relevant)) if not labels.get('highly_relevant'): for relevant in labels.get('relevant', [])[:1]: test_pairs.append((query, relevant)) elif isinstance(labels, list): for relevant in labels[:1]: test_pairs.append((query, relevant)) logger.info(f"Loaded {len(test_pairs)} test pairs from {args.test_set} (via direct JSON)") except Exception as e: logger.warning(f"Failed to load test set: {e}, using pairs data") test_pairs = None # Create or load training data if args.pairs: pairs_path = args.pairs # Create test pairs from data if not loaded from test set if test_pairs is None: df = pd.read_csv(pairs_path) test_pairs = [ (str(row['CARD1']), str(row['CARD2'])) for _, row in df.head(50).iterrows() ] else: logger.info("Creating minimal test data...") num_pairs = 500 if args.quick else 2000 pairs_path, generated_test_pairs = create_minimal_test_data(num_pairs) if test_pairs is None: test_pairs = generated_test_pairs logger.info(f" Created {num_pairs} pairs, {len(test_pairs)} test pairs") # Run parameter sweep logger.info("\n" + "="*70) logger.info("PARAMETER SWEEP TEST") logger.info("="*70) results = test_parameter_sweep(pairs_path, test_pairs, args.output_dir, quick=args.quick) # Save results results_path = args.output_dir / "test_results.json" with open(results_path, 'w') as f: json.dump(results, f, indent=2) logger.info(f"\n{'='*70}") logger.info("RESULTS SUMMARY") logger.info(f"{'='*70}") for name, result in results.items(): if 'error' in result: logger.info(f"{name}: ERROR - {result['error']}") else: metrics = result['metrics'] logger.info(f"{name}:") logger.info(f" MRR: {metrics['mrr']:.4f}") logger.info(f" P@10: {metrics['p@k']:.4f}") logger.info(f" R@10: {metrics['r@k']:.4f}") logger.info(f"\n Results saved to: {results_path}") # Cleanup temp file if created if not args.pairs and pairs_path.exists(): pairs_path.unlink() return 0 if __name__ == "__main__": sys.exit(main())