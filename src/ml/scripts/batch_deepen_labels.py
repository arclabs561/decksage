#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pydantic-ai>=0.0.12", # ] # /// """ Batch deepen labels for existing queries (add more labels per query). Useful for: - Increasing label count for queries with <15 labels - Improving coverage for downstream tasks (substitution needs both levels) - Iteratively refining gold data quality """ from __future__ import annotations import argparse import json import logging from pathlib import Path from typing import Any try: from pydantic_ai import Agent HAS_PYDANTIC_AI = True except ImportError: HAS_PYDANTIC_AI = False logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) # Import path handling import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) # Import from iterative refine (reuse deepen function) try: from ml.scripts.iterative_refine_gold_data import deepen_labels_for_query HAS_DEEPEN = True except ImportError: HAS_DEEPEN = False logger.error("Deepen function not available") deepen_labels_for_query = None def batch_deepen_labels( test_set_path: Path, output_path: Path, num_judges: int = 3, target_labels: int = 20, min_current_labels: int = 0, # Only deepen queries with at least this many batch_size: int = 10, ) -> dict[str, Any]: """ Batch deepen labels for queries that need more. Args: test_set_path: Input test set output_path: Output test set num_judges: Number of judges per query target_labels: Target minimum labels per query min_current_labels: Only deepen queries with at least this many labels batch_size: Checkpoint interval """ if not HAS_PYDANTIC_AI: logger.error("pydantic-ai required") return {} if not HAS_DEEPEN or not deepen_labels_for_query: logger.error("Deepen function not available") return {} # Load test set with open(test_set_path) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data, dict) else data # Identify queries needing more labels queries_to_deepen = [] for query_name, query_data in queries.items(): if not isinstance(query_data, dict): continue current_count = sum( len(query_data.get(level, [])) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"] ) if current_count < target_labels and current_count >= min_current_labels: queries_to_deepen.append((query_name, query_data, current_count)) logger.info(f"Found {len(queries_to_deepen)} queries needing more labels:") logger.info(f" Target: {target_labels} labels per query") logger.info(f" Current range: {min(q[2] for q in queries_to_deepen) if queries_to_deepen else 0}-{max(q[2] for q in queries_to_deepen) if queries_to_deepen else 0}") if not queries_to_deepen: logger.info(" All queries have sufficient labels!") return {"queries_deepened": 0, "total_queries": len(queries)} # Deepen labels updated = queries.copy() processed = 0 successful = 0 total_labels_added = 0 for i, (query_name, query_data, current_count) in enumerate(queries_to_deepen, 1): logger.info(f"[{i}/{len(queries_to_deepen)}] Deepening {query_name} ({current_count} â†’ target {target_labels})...") deepened = deepen_labels_for_query( query_name, query_data, num_judges=num_judges, target_labels=target_labels, ) new_count = sum( len(deepened.get(level, [])) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"] ) if new_count > current_count: updated[query_name] = deepened successful += 1 labels_added = new_count - current_count total_labels_added += labels_added logger.info(f" Added {labels_added} labels (now {new_count})") else: logger.warning(f" Warning: No new labels added") processed += 1 # Checkpoint if i % batch_size == 0: logger.info(f" ðŸ’¾ Checkpoint: {i}/{len(queries_to_deepen)} queries processed") checkpoint_path = output_path.parent / f"{output_path.stem}_checkpoint.json" with open(checkpoint_path, "w") as f: json.dump({"version": "deepened_checkpoint", "queries": updated}, f, indent=2) # Save output_path.parent.mkdir(parents=True, exist_ok=True) final_data = { "version": "deepened_labels", "target_labels": target_labels, "queries": updated, } with open(output_path, "w") as f: json.dump(final_data, f, indent=2) logger.info(f"\n Deepened {successful}/{processed} queries") logger.info(f" Added {total_labels_added} total labels") logger.info(f" Saved to {output_path}") return { "queries_deepened": successful, "queries_attempted": processed, "total_labels_added": total_labels_added, "total_queries": len(updated), } def main() -> int: """Batch deepen labels for existing queries.""" parser = argparse.ArgumentParser(description="Batch deepen labels for existing queries") parser.add_argument( "--input", type=str, required=True, help="Input test set JSON", ) parser.add_argument( "--output", type=str, required=True, help="Output test set JSON", ) parser.add_argument( "--num-judges", type=int, default=3, help="Number of judges per query", ) parser.add_argument( "--target-labels", type=int, default=20, help="Target minimum labels per query", ) parser.add_argument( "--min-current-labels", type=int, default=0, help="Only deepen queries with at least this many labels", ) parser.add_argument( "--batch-size", type=int, default=10, help="Checkpoint interval", ) args = parser.parse_args() if not HAS_PYDANTIC_AI: logger.error("pydantic-ai required: pip install pydantic-ai") return 1 result = batch_deepen_labels( Path(args.input), Path(args.output), num_judges=args.num_judges, target_labels=args.target_labels, min_current_labels=args.min_current_labels, batch_size=args.batch_size, ) if result: print("\n=== Deepening Summary ===") print(f"Queries deepened: {result['queries_deepened']}/{result['queries_attempted']}") print(f"Total labels added: {result['total_labels_added']}") print(f"Total queries: {result['total_queries']}") return 0 return 1 if __name__ == "__main__": import sys sys.exit(main())
