#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pyyaml>=6.0", # ] # /// """ Generate comprehensive annotation system report. Includes: 1. Coverage statistics 2. Quality metrics 3. IAA scores 4. Metadata completeness 5. Downstream task support 6. Recommendations """ from __future__ import annotations import argparse import json from collections import Counter, defaultdict from pathlib import Path from typing import Any try: import yaml HAS_YAML = True except ImportError: HAS_YAML = False import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) from ml.utils.annotation_utils import load_similarity_annotations, load_hand_annotations def generate_comprehensive_report( annotations_dir: Path = Path("annotations"), output_path: Path | None = None, ) -> dict[str, Any]: """Generate comprehensive annotation system report.""" # Find all annotation files annotation_files = ( list(annotations_dir.glob("*.yaml")) + list(annotations_dir.glob("*.jsonl")) ) # Aggregate statistics all_annotations = [] all_cards = set() similarity_types = Counter() games = Counter() metadata_tracking = { "with_model_name": 0, "with_model_params": 0, "with_annotator_id": 0, "with_timestamp": 0, } downstream_fields = { "role_match": 0, "archetype_context": 0, "format_context": 0, "substitution_quality": 0, } quality_scores = [] for ann_file in annotation_files: try: if ann_file.suffix == ".yaml": annotations = load_hand_annotations(ann_file) else: annotations = load_similarity_annotations(ann_file) for ann in annotations: all_annotations.append(ann) all_cards.add(ann.get("card1", "")) all_cards.add(ann.get("card2", "")) similarity_types[ann.get("similarity_type", "unknown")] += 1 # Metadata tracking if ann.get("model_name"): metadata_tracking["with_model_name"] += 1 if ann.get("model_params"): metadata_tracking["with_model_params"] += 1 if ann.get("annotator_id"): metadata_tracking["with_annotator_id"] += 1 if ann.get("timestamp"): metadata_tracking["with_timestamp"] += 1 # Downstream fields for field in downstream_fields: if ann.get(field) is not None: downstream_fields[field] += 1 # Quality score (simplified) quality = 0.0 if ann.get("similarity_score") is not None or ann.get("relevance") is not None: quality += 25 if ann.get("similarity_type"): quality += 25 if ann.get("annotator_id"): quality += 25 if any(ann.get(f) for f in downstream_fields): quality += 25 quality_scores.append(quality) except Exception as e: print(f"Warning: Could not process {ann_file}: {e}") # Pair statistics pair_counts = Counter() for ann in all_annotations: card1 = ann.get("card1") card2 = ann.get("card2") if card1 and card2: pair = tuple(sorted([card1, card2])) pair_counts[pair] += 1 # Generate recommendations recommendations = [] if len(all_annotations) < 100: recommendations.append("Consider expanding annotation coverage (currently <100 annotations)") single_annotation_pairs = sum(1 for count in pair_counts.values() if count == 1) if single_annotation_pairs > len(pair_counts) * 0.5: recommendations.append(f"Many pairs have single annotation ({single_annotation_pairs}). Consider IAA validation.") avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0 if avg_quality < 75: recommendations.append(f"Average quality score is {avg_quality:.1f}%. Consider improving metadata completeness.") metadata_pct = sum(metadata_tracking.values()) / (len(all_annotations) * 4) * 100 if all_annotations else 0 if metadata_pct < 50: recommendations.append("Metadata tracking is incomplete. Ensure all annotations include model/params/timestamp.") downstream_pct = sum(downstream_fields.values()) / (len(all_annotations) * 4) * 100 if all_annotations else 0 if downstream_pct < 30: recommendations.append("Downstream field coverage is low. Consider retrofitting existing annotations.") return { "summary": { "total_annotations": len(all_annotations), "unique_cards": len(all_cards), "unique_pairs": len(pair_counts), "annotation_files": len(annotation_files), }, "coverage": { "similarity_type_distribution": dict(similarity_types), "single_annotation_pairs": single_annotation_pairs, "multi_annotation_pairs": len(pair_counts) - single_annotation_pairs, }, "metadata_tracking": { **metadata_tracking, "completeness_pct": metadata_pct, }, "downstream_support": { **downstream_fields, "coverage_pct": downstream_pct, }, "quality": { "average_score": avg_quality, "high_quality": sum(1 for q in quality_scores if q >= 75), "medium_quality": sum(1 for q in quality_scores if 50 <= q < 75), "low_quality": sum(1 for q in quality_scores if q < 50), }, "recommendations": recommendations, } def main() -> int: parser = argparse.ArgumentParser(description="Generate annotation system report") parser.add_argument("--annotations-dir", type=str, default="annotations", help="Annotations directory") parser.add_argument("--output", type=str, required=True, help="Output JSON report") args = parser.parse_args() annotations_dir = Path(args.annotations_dir) if not annotations_dir.exists(): print(f"Error: Annotations directory not found: {annotations_dir}") return 1 print("=" * 70) print("ANNOTATION SYSTEM REPORT") print("=" * 70) print() report = generate_comprehensive_report(annotations_dir, Path(args.output)) print(f" Summary:") print(f" Total annotations: {report['summary']['total_annotations']}") print(f" Unique cards: {report['summary']['unique_cards']}") print(f" Unique pairs: {report['summary']['unique_pairs']}") print(f" Annotation files: {report['summary']['annotation_files']}") print() print(f"üìà Coverage:") print(f" Single annotation pairs: {report['coverage']['single_annotation_pairs']}") print(f" Multi annotation pairs: {report['coverage']['multi_annotation_pairs']}") print() print(f"üîç Metadata Tracking:") print(f" Completeness: {report['metadata_tracking']['completeness_pct']:.1f}%") print() print(f" Downstream Support:") print(f" Coverage: {report['downstream_support']['coverage_pct']:.1f}%") print() print(f" Quality:") print(f" Average score: {report['quality']['average_score']:.1f}%") print(f" High quality: {report['quality']['high_quality']}") print(f" Medium quality: {report['quality']['medium_quality']}") print(f" Low quality: {report['quality']['low_quality']}") print() if report['recommendations']: print(f" Recommendations:") for rec in report['recommendations']: print(f" - {rec}") print() # Save report with open(args.output, "w") as f: json.dump(report, f, indent=2) print(f"‚úì Saved report to {args.output}") print() print("=" * 70) print(" Report generation complete!") print("=" * 70) return 0 if __name__ == "__main__": import sys sys.exit(main())
