#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # ] # /// """ Extract card attributes from pairs CSV and enrich with available data. Since we don't have a dedicated card attributes file, we can: 1. Extract unique card names from pairs 2. Use Scryfall API or other sources to get attributes 3. Or use LLM to infer basic attributes from card names For now, creates a minimal attributes file that can be enriched later. """ from __future__ import annotations import argparse import json import logging from pathlib import Path from typing import Any from ..utils.logging_config import setup_script_logging try: import pandas as pd HAS_DEPS = True except ImportError as e: HAS_DEPS = False print(f"Missing dependencies: {e}") logger = setup_script_logging() def extract_unique_cards(pairs_csv: Path) -> set[str]: """Extract all unique card names from pairs CSV.""" logger.info(f"Loading pairs from {pairs_csv}...") df = pd.read_csv(pairs_csv) all_cards = set(df["NAME_1"]) | set(df["NAME_2"]) logger.info(f"Found {len(all_cards):,} unique cards") return all_cards def create_minimal_attributes(cards: set[str], output_csv: Path) -> None: """Create minimal attributes CSV with card names. This creates a template that can be enriched later with: - Scryfall API calls - LLM inference - Manual annotation """ logger.info("Creating minimal attributes CSV...") # Create DataFrame with card names data = [] for card in sorted(cards): data.append({ "name": card, "type": "", # To be filled "colors": "", # To be filled "mana_cost": "", # To be filled "cmc": 0.0, # To be filled "rarity": "", # To be filled }) df = pd.DataFrame(data) df.to_csv(output_csv, index=False) logger.info(f"Created {output_csv} with {len(data):,} cards") logger.info("Note: Attributes are empty and need to be enriched") def enrich_with_llm(cards: set[str], output_csv: Path) -> None: """Use LLM to infer basic card attributes from names. This is a fallback when we don't have structured card data. """ try: from pydantic_ai import Agent from pydantic import BaseModel, Field import os from pathlib import Path # Load .env env_file = Path(__file__).parent.parent.parent.parent / ".env" if env_file.exists(): try: from dotenv import load_dotenv load_dotenv(env_file) except ImportError: pass class CardAttributes(BaseModel): """Card attributes inferred from name.""" type: str = Field(description="Card type: Creature, Instant, Sorcery, Enchantment, Artifact, Planeswalker, Land") colors: str = Field(description="Color identity: W, U, B, R, G, or combination like WU, or empty for colorless") mana_cost: str = Field(description="Mana cost like {1}{R} or {2}{U}{U}") cmc: float = Field(description="Converted mana cost (number)") rarity: str = Field(description="Rarity: common, uncommon, rare, mythic") model_name = os.getenv("ANNOTATOR_MODEL") or os.getenv("OPENROUTER_MODEL") or "anthropic/claude-3.5-sonnet" provider = os.getenv("LLM_PROVIDER", "openrouter") agent = Agent( f"{provider}:{model_name}", output_type=CardAttributes, system_prompt="You are an expert on TCG cards. Infer card attributes from card names.", ) logger.info("Enriching card attributes with LLM (this may take a while)...") logger.info(f"Processing {len(cards):,} cards in batches...") # Process in batches batch_size = 20 all_attrs = [] processed = 0 cards_list = sorted(list(cards)) for i in range(0, len(cards_list), batch_size): batch = cards_list[i:i+batch_size] logger.info(f"Processing batch {i//batch_size + 1}/{(len(cards_list)-1)//batch_size + 1} ({len(batch)} cards)...") for card in batch: try: prompt = f"Infer attributes for TCG card: {card}\n\nProvide type, colors, mana cost, CMC, and rarity." result = agent.run_sync(prompt) if hasattr(result, 'data') and result.data: attrs = result.data all_attrs.append({ "name": card, "type": attrs.type if hasattr(attrs, 'type') else "", "colors": attrs.colors if hasattr(attrs, 'colors') else "", "mana_cost": attrs.mana_cost if hasattr(attrs, 'mana_cost') else "", "cmc": attrs.cmc if hasattr(attrs, 'cmc') else 0.0, "rarity": attrs.rarity if hasattr(attrs, 'rarity') else "", }) processed += 1 except Exception as e: logger.warning(f"Failed to enrich {card}: {e}") all_attrs.append({ "name": card, "type": "", "colors": "", "mana_cost": "", "cmc": 0.0, "rarity": "", }) # Save df = pd.DataFrame(all_attrs) df.to_csv(output_csv, index=False) logger.info(f" Enriched {processed}/{len(cards)} cards") logger.info(f"Saved to {output_csv}") except ImportError: logger.warning("pydantic-ai not available, creating minimal attributes only") create_minimal_attributes(cards, output_csv) def main() -> int: """Extract card attributes.""" parser = argparse.ArgumentParser(description="Extract card attributes from pairs CSV") parser.add_argument("--input", type=str, required=True, help="Pairs CSV") parser.add_argument("--output", type=str, default="data/processed/card_attributes.csv", help="Output CSV") parser.add_argument("--enrich", action="store_true", help="Enrich with LLM (slow but useful)") args = parser.parse_args() if not HAS_DEPS: logger.error("Missing dependencies") return 1 input_path = Path(args.input) output_path = Path(args.output) output_path.parent.mkdir(parents=True, exist_ok=True) # Extract unique cards cards = extract_unique_cards(input_path) # Create or enrich attributes if args.enrich: enrich_with_llm(cards, output_path) else: create_minimal_attributes(cards, output_path) logger.info("Use --enrich flag to enrich with LLM (will take time but provides useful data)") return 0 if __name__ == "__main__": import sys sys.exit(main())