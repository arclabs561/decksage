#!/usr/bin/env python3 """ Foundation Refinement Script Unified script for T0 foundation items: - T0.1: Test set validation with confidence intervals - T0.2: Deck quality validation - T0.3: Quality dashboard generation Usage: uv run python -m ml.scripts.refine_foundation --all uv run python -m ml.scripts.refine_foundation --test-set-validation uv run python -m ml.scripts.refine_foundation --deck-quality-validation uv run python -m ml.scripts.refine_foundation --quality-dashboard """ from __future__ import annotations import argparse import json import logging import sys from pathlib import Path from ..evaluation.deck_quality_validation import validate_deck_completion from ..evaluation.quality_dashboard import compute_system_health, generate_dashboard_html from ..evaluation.test_set_validation import ( compute_test_set_stats, validate_test_set_coverage, ) from ..utils.paths import PATHS logger = logging.getLogger(__name__) def run_test_set_validation( game: str = "magic", output_dir: Path | None = None, add_metadata: bool = True, ) -> Path: """Run test set validation and return output path.""" if output_dir is None: output_dir = PATHS.experiments test_set_path = getattr(PATHS, f"test_{game}") output_path = output_dir / f"test_set_validation_{game}.json" logger.info(f"Validating test set: {test_set_path}") # Optionally add format/archetype metadata first if add_metadata: try: from .add_format_metadata_to_test_set import add_metadata_to_test_set metadata_output = output_dir / f"test_set_unified_{game}_with_metadata.json" logger.info("Adding format/archetype metadata to test set...") add_metadata_to_test_set( test_set_path=test_set_path, output_path=metadata_output, game=game, ) # Use metadata-enhanced version for validation test_set_path = metadata_output except Exception as e: logger.warning(f"Failed to add metadata: {e}, continuing with original test set") results = validate_test_set_coverage( test_set_path=test_set_path, min_queries=100, min_labels_per_query=5, ) # Save results output_path.parent.mkdir(parents=True, exist_ok=True) with open(output_path, "w") as f: json.dump(results, f, indent=2) logger.info(f" Test set validation complete: {output_path}") logger.info(f" Status: {'VALID' if results['valid'] else 'ISSUES FOUND'}") logger.info(f" Queries: {results['stats']['total_queries']}") logger.info(f" With labels: {results['stats']['queries_with_labels']}") if results["issues"]: logger.warning("Issues found:") for issue in results["issues"]: logger.warning(f" - {issue}") return output_path def run_deck_quality_validation( game: str = "magic", output_dir: Path | None = None, embeddings_path: Path | None = None, pairs_path: Path | None = None, ) -> Path | None: """Run deck quality validation and return output path.""" if output_dir is None: output_dir = PATHS.experiments output_path = output_dir / f"deck_quality_validation_{game}.json" logger.info(f"Validating deck completion quality for {game}") # Import validation function from ..evaluation.deck_quality_validation import validate_deck_completion # Load similarity function try: from ..evaluation.similarity_helper import ( create_similarity_function, create_similarity_function_from_defaults, create_similarity_function_from_env, ) import os # Try environment variables first, then explicit paths, then defaults try: similarity_fn = create_similarity_function_from_env() logger.info("Loaded similarity function from environment variables") except ValueError: if embeddings_path or pairs_path: similarity_fn = create_similarity_function( embeddings_path=embeddings_path, pairs_path=pairs_path, method="fusion", ) logger.info("Loaded similarity function from provided paths") else: similarity_fn = create_similarity_function_from_defaults(game=game) logger.info(f"Loaded similarity function from defaults for {game}") # Run validation results = validate_deck_completion( game=game, num_test_cases=10, similarity_fn=similarity_fn, tag_set_fn=None, # Optional - will use if available cmc_fn=None, # Optional - will use if available ) # Save results output_path.parent.mkdir(parents=True, exist_ok=True) with open(output_path, "w") as f: json.dump(results, f, indent=2) logger.info(f" Deck quality validation complete: {output_path}") logger.info(f" Success rate: {results['success_rate'] * 100:.1f}%") logger.info(f" Avg quality: {results['avg_quality_score']:.2f}/10.0") logger.info(f" Status: {'VALID' if results['valid'] else 'FAILED'}") except Exception as e: logger.error(f"Deck quality validation failed: {e}", exc_info=True) results = { "valid": False, "error": str(e), "success_rate": 0.0, "results": [], } output_path.parent.mkdir(parents=True, exist_ok=True) with open(output_path, "w") as f: json.dump(results, f, indent=2) logger.warning(f"Warning: Deck quality validation failed: {output_path}") return output_path def run_quality_dashboard( test_set_validation_path: Path | None = None, completion_validation_path: Path | None = None, evaluation_results_path: Path | None = None, output_dir: Path | None = None, ) -> Path: """Generate quality dashboard and return output path.""" if output_dir is None: output_dir = PATHS.experiments output_path = output_dir / "quality_dashboard.html" logger.info("Generating quality dashboard") health = compute_system_health( test_set_validation_path=test_set_validation_path, completion_validation_path=completion_validation_path, evaluation_results_path=evaluation_results_path, ) generate_dashboard_html(health, output_path) logger.info(f" Quality dashboard generated: {output_path}") logger.info(f" Status: {health.overall_status.upper()}") logger.info(f" Metrics: {len(health.metrics)}") return output_path def main() -> int: """Main entry point.""" parser = argparse.ArgumentParser( description="Foundation refinement (T0 items)", formatter_class=argparse.RawDescriptionHelpFormatter, epilog=""" Examples: # Run all foundation items python -m ml.scripts.refine_foundation --all # Run individual items python -m ml.scripts.refine_foundation --test-set-validation python -m ml.scripts.refine_foundation --deck-quality-validation python -m ml.scripts.refine_foundation --quality-dashboard # Specify game python -m ml.scripts.refine_foundation --all --game pokemon """, ) parser.add_argument( "--all", action="store_true", help="Run all foundation items", ) parser.add_argument( "--test-set-validation", action="store_true", help="Validate test set coverage (T0.1)", ) parser.add_argument( "--deck-quality-validation", action="store_true", help="Validate deck completion quality (T0.2)", ) parser.add_argument( "--quality-dashboard", action="store_true", help="Generate quality dashboard (T0.3)", ) parser.add_argument( "--game", type=str, default="magic", choices=["magic", "pokemon", "yugioh"], help="Game to validate", ) parser.add_argument( "--output-dir", type=Path, help="Output directory (default: experiments/)", ) parser.add_argument( "--evaluation-results", type=Path, help="Path to evaluation results JSON (for dashboard)", ) args = parser.parse_args() # Setup logging logging.basicConfig( level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", ) # Determine what to run run_all = args.all run_test_set = args.test_set_validation or run_all run_deck_quality = args.deck_quality_validation or run_all run_dashboard = args.quality_dashboard or run_all if not (run_test_set or run_deck_quality or run_dashboard): parser.print_help() return 1 output_dir = args.output_dir or PATHS.experiments output_dir.mkdir(parents=True, exist_ok=True) # Run test set validation test_set_validation_path = None if run_test_set: test_set_validation_path = run_test_set_validation( game=args.game, output_dir=output_dir, ) # Run deck quality validation completion_validation_path = None if run_deck_quality: # Try to get embeddings/pairs from environment or defaults import os embeddings_path = None pairs_path = None emb_env = os.getenv("EMBEDDINGS_PATH") pairs_env = os.getenv("PAIRS_PATH") if emb_env: embeddings_path = Path(emb_env) if pairs_env: pairs_path = Path(pairs_env) completion_validation_path = run_deck_quality_validation( game=args.game, output_dir=output_dir, embeddings_path=embeddings_path, pairs_path=pairs_path, ) # Generate dashboard if run_dashboard: run_quality_dashboard( test_set_validation_path=test_set_validation_path, completion_validation_path=completion_validation_path, evaluation_results_path=args.evaluation_results, output_dir=output_dir, ) logger.info(" Foundation refinement complete") return 0 if __name__ == "__main__": sys.exit(main())