#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # ] # /// """ Merge multiple test sets and analyze coverage. Combines: - Explicit test sets (LLM-generated) - Implicit test sets (extracted from data) - Synthetic test sets (generated from patterns) Provides analysis of: - Query overlap - Source diversity - Coverage statistics """ from __future__ import annotations import argparse import json from collections import Counter, defaultdict from pathlib import Path from typing import Any try: import pandas as pd HAS_DEPS = True except ImportError: HAS_DEPS = False def load_test_set(path: Path) -> dict[str, Any]: """Load a test set JSON file.""" with open(path) as f: return json.load(f) def merge_test_sets( test_set_paths: list[Path], output_path: Path, game: str = "magic", ) -> dict[str, Any]: """Merge multiple test sets into one comprehensive set.""" print(f"ðŸ“ Merging {len(test_set_paths)} test sets...") merged = { "game": game, "sources": {}, "queries": {}, } source_stats = Counter() query_sources = defaultdict(list) for test_set_path in test_set_paths: test_set = load_test_set(test_set_path) source_name = test_set_path.stem # Track source if "sources" in test_set: merged["sources"][source_name] = test_set["sources"] else: merged["sources"][source_name] = {"queries": len(test_set.get("queries", {}))} # Merge queries queries = test_set.get("queries", test_set) if isinstance(queries, dict): for query, labels in queries.items(): if query not in merged["queries"]: merged["queries"][query] = { "type": labels.get("type", "unknown"), "sources": [], "highly_relevant": [], "relevant": [], "somewhat_relevant": [], } merged["queries"][query]["sources"].append(source_name) source_stats[source_name] += 1 query_sources[query].append(source_name) # Merge labels (deduplicate) for level in ["highly_relevant", "relevant", "somewhat_relevant"]: existing = set(merged["queries"][query][level]) new = set(labels.get(level, [])) merged["queries"][query][level] = list(existing | new) # Save merged set with open(output_path, "w") as f: json.dump(merged, f, indent=2) print(f" Merged into {len(merged['queries'])} unique queries") print(f" Source distribution: {dict(source_stats)}") return merged def analyze_test_set_coverage( test_set: dict[str, Any], pairs_csv: Path | None = None, ) -> dict[str, Any]: """Analyze test set coverage and quality.""" print(f"\n Analyzing test set coverage...") queries = test_set.get("queries", {}) analysis = { "total_queries": len(queries), "query_types": Counter(), "source_distribution": Counter(), "label_statistics": { "avg_highly_relevant": 0.0, "avg_relevant": 0.0, "avg_somewhat_relevant": 0.0, "queries_with_labels": 0, }, "coverage_analysis": {}, } # Analyze query types for query, labels in queries.items(): query_type = labels.get("type", "unknown") analysis["query_types"][query_type] += 1 # Track sources for source in labels.get("sources", []): analysis["source_distribution"][source] += 1 # Label statistics highly_rel = len(labels.get("highly_relevant", [])) relevant = len(labels.get("relevant", [])) somewhat_rel = len(labels.get("somewhat_relevant", [])) if highly_rel + relevant + somewhat_rel > 0: analysis["label_statistics"]["queries_with_labels"] += 1 analysis["label_statistics"]["avg_highly_relevant"] += highly_rel analysis["label_statistics"]["avg_relevant"] += relevant analysis["label_statistics"]["avg_somewhat_relevant"] += somewhat_rel # Average labels if analysis["label_statistics"]["queries_with_labels"] > 0: n = analysis["label_statistics"]["queries_with_labels"] analysis["label_statistics"]["avg_highly_relevant"] /= n analysis["label_statistics"]["avg_relevant"] /= n analysis["label_statistics"]["avg_somewhat_relevant"] /= n # Coverage analysis (if pairs CSV provided) if pairs_csv and pairs_csv.exists() and HAS_DEPS: df = pd.read_csv(pairs_csv, nrows=10000) name1_col = df.columns[0] name2_col = df.columns[1] all_cards = set(df[name1_col].unique()) | set(df[name2_col].unique()) queries_in_data = sum(1 for q in queries.keys() if q in all_cards) analysis["coverage_analysis"] = { "queries_in_data": queries_in_data, "queries_not_in_data": len(queries) - queries_in_data, "coverage_percentage": (queries_in_data / len(queries) * 100) if queries else 0.0, } print(f" Analysis complete:") print(f" Total queries: {analysis['total_queries']}") print(f" Query types: {dict(analysis['query_types'])}") print(f" Avg labels per query: {analysis['label_statistics']['avg_highly_relevant']:.1f} highly, " f"{analysis['label_statistics']['avg_relevant']:.1f} relevant, " f"{analysis['label_statistics']['avg_somewhat_relevant']:.1f} somewhat") if analysis["coverage_analysis"]: cov = analysis["coverage_analysis"] print(f" Coverage: {cov['queries_in_data']}/{analysis['total_queries']} " f"({cov['coverage_percentage']:.1f}%) in data") return analysis def main() -> int: """Merge and analyze test sets.""" parser = argparse.ArgumentParser( description="Merge and analyze multiple test sets" ) parser.add_argument("--test-sets", type=str, nargs="+", required=True, help="Test set JSON files to merge") parser.add_argument("--output", type=str, default="experiments/test_set_merged_comprehensive.json", help="Output merged test set JSON") parser.add_argument("--analysis-output", type=str, help="Output analysis JSON (optional)") parser.add_argument("--pairs-csv", type=str, help="Pairs CSV for coverage analysis") parser.add_argument("--game", type=str, default="magic", choices=["magic", "pokemon", "yugioh"], help="Game name") args = parser.parse_args() if not HAS_DEPS: print("Error: Missing dependencies (pandas)") return 1 test_set_paths = [Path(p) for p in args.test_sets] # Check all files exist for path in test_set_paths: if not path.exists(): print(f"Error: Test set not found: {path}") return 1 output_path = Path(args.output) output_path.parent.mkdir(parents=True, exist_ok=True) pairs_csv = Path(args.pairs_csv) if args.pairs_csv else None print("=" * 70) print("Merge and Analyze Test Sets") print("=" * 70) print(f"\nTest sets to merge: {len(test_set_paths)}") for path in test_set_paths: print(f" - {path}") print(f"Output: {output_path}\n") # Merge test sets merged = merge_test_sets( test_set_paths=test_set_paths, output_path=output_path, game=args.game, ) # Analyze analysis = analyze_test_set_coverage( test_set=merged, pairs_csv=pairs_csv, ) # Save analysis if requested if args.analysis_output: analysis_path = Path(args.analysis_output) analysis_path.parent.mkdir(parents=True, exist_ok=True) with open(analysis_path, "w") as f: json.dump(analysis, f, indent=2, default=str) # default=str for Counter print(f"\n Analysis saved to {analysis_path}") print(f"\n Merged test set created: {output_path}") print(f" Total queries: {len(merged['queries'])}") return 0 if __name__ == "__main__": import sys sys.exit(main())