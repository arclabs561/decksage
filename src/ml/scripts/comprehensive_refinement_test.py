#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # "gensim>=4.3.0", # ] # /// """ Comprehensive test of all training refinements using real test sets. Tests: 1. Baseline vs refined parameters 2. Different parameter combinations 3. Hard negative mining effectiveness 4. Real test set evaluation (test_set_unified_*.json) Outputs detailed comparison metrics and recommendations. """ from __future__ import annotations import argparse import json import logging import sys from pathlib import Path from typing import Any # Add project root to path _script_file = Path(__file__).resolve() _src_dir = _script_file.parent.parent.parent if str(_src_dir) not in sys.path: sys.path.insert(0, str(_src_dir)) try: import pandas as pd import numpy as np from gensim.models import Word2Vec, KeyedVectors HAS_DEPS = True except ImportError as e: HAS_DEPS = False print(f"Missing dependencies: {e}") s - %(levelname)s - %(message)s') logger = setup_script_logging() def load_test_set_canonical(test_set_path: Path) -> list[tuple[str, str]]: """Load test set and extract query-relevant pairs.""" try: from ml.utils.data_loading import load_test_set test_data = load_test_set(test_set_path) except ImportError: # Fallback with open(test_set_path) as f: test_data = json.load(f) test_pairs = [] for query, labels in test_data.items(): if isinstance(labels, dict): # Take highly_relevant first, then relevant for relevant in labels.get('highly_relevant', []): test_pairs.append((query, relevant)) for relevant in labels.get('relevant', []): test_pairs.append((query, relevant)) return test_pairs def evaluate_on_test_set( wv: KeyedVectors, test_pairs: list[tuple[str, str]], k: int = 10, ) -> dict[str, float]: """Evaluate embeddings on test set.""" from ml.utils.training_validation import compute_ranking_metrics return compute_ranking_metrics(wv, test_pairs, k) def train_with_config( pairs_path: Path, output_path: Path, config: dict[str, Any], dim: int = 128, epochs: int = 10, ) -> KeyedVectors: """Train embeddings with specific configuration.""" logger.info(f"Training with config: {config['name']}") # Load pairs df = pd.read_csv(pairs_path) # Create simple walks from pairs walks = [] for _, row in df.iterrows(): walks.append([str(row['CARD1']), str(row['CARD2'])]) # Train model = Word2Vec( sentences=walks, vector_size=dim, window=10, min_count=1, workers=4, epochs=epochs, sg=1, negative=config.get('negative', 5), sample=config.get('sample', 1e-3), alpha=config.get('alpha', 0.025), min_alpha=config.get('min_alpha', 0.0001), batch_words=config.get('batch_words', 10000), ) wv = model.wv wv.save(str(output_path)) logger.info(f" Saved to {output_path}") return wv def main() -> int: parser = argparse.ArgumentParser(description="Comprehensive refinement test") parser.add_argument( "--pairs", type=Path, help="Path to pairs CSV (default: PATHS.pairs_large)", ) parser.add_argument( "--test-set", type=Path, help="Path to test set JSON (default: test_set_unified_magic.json)", ) parser.add_argument( "--output-dir", type=Path, default=Path("data/test_refinements"), help="Output directory for test models", ) parser.add_argument( "--quick", action="store_true", help="Quick test with minimal epochs", ) args = parser.parse_args() if not HAS_DEPS: logger.error("Missing dependencies") return 1 # Use canonical paths try: from ml.utils.paths import PATHS if not args.pairs: args.pairs = PATHS.pairs_large if not args.test_set: args.test_set = PATHS.test_magic except ImportError: if not args.pairs: args.pairs = Path("data/processed/pairs_large.csv") if not args.test_set: args.test_set = Path("experiments/test_set_unified_magic.json") # Check files exist if not args.pairs.exists(): logger.error(f"Pairs file not found: {args.pairs}") return 1 if not args.test_set.exists(): logger.warning(f"Test set not found: {args.test_set}, will use pairs data") test_pairs = None else: logger.info(f"Loading test set: {args.test_set}") test_pairs = load_test_set_canonical(args.test_set) logger.info(f" Loaded {len(test_pairs)} test pairs") # Create output directory args.output_dir.mkdir(parents=True, exist_ok=True) # Test configurations epochs = 5 if args.quick else 10 configs = [ { 'name': 'baseline', 'negative': 5, 'batch_words': 1000, 'sample': 0.0, 'alpha': 0.025, 'min_alpha': 0.0001, }, { 'name': 'refined_default', 'negative': 5, 'batch_words': 10000, 'sample': 1e-3, 'alpha': 0.025, 'min_alpha': 0.0001, }, { 'name': 'refined_optimal', 'negative': 10, 'batch_words': 20000, 'sample': 1e-3, 'alpha': 0.025, 'min_alpha': 0.0001, }, ] results = {} logger.info("\n" + "="*70) logger.info("COMPREHENSIVE REFINEMENT TEST") logger.info("="*70) logger.info(f"Pairs: {args.pairs}") logger.info(f"Test set: {args.test_set}") logger.info(f"Epochs: {epochs}") logger.info("") # Train and evaluate each configuration for config in configs: logger.info(f"\n{'='*70}") logger.info(f"Testing: {config['name']}") logger.info(f"{'='*70}") output_path = args.output_dir / f"{config['name']}.wv" try: # Train wv = train_with_config( pairs_path=args.pairs, output_path=output_path, config=config, dim=128, epochs=epochs, ) # Evaluate if test pairs available if test_pairs: metrics = evaluate_on_test_set(wv, test_pairs, k=10) results[config['name']] = { 'config': config, 'metrics': metrics, } logger.info(f" MRR: {metrics['mrr']:.4f}") logger.info(f" P@10: {metrics['p@k']:.4f}") logger.info(f" R@10: {metrics['r@k']:.4f}") logger.info(f" Evaluated: {metrics['num_evaluated']} pairs") else: results[config['name']] = { 'config': config, 'status': 'trained', } logger.info(" Trained (no test set for evaluation)") except Exception as e: from ..utils.logging_config import log_exception log_exception(logger, f" Failed: {e}", e, include_context=True) results[config['name']] = {'error': str(e)} # Compare results if test_pairs and 'baseline' in results and 'refined_optimal' in results: baseline_metrics = results['baseline'].get('metrics', {}) optimal_metrics = results['refined_optimal'].get('metrics', {}) if baseline_metrics and optimal_metrics: mrr_improvement = optimal_metrics['mrr'] - baseline_metrics['mrr'] mrr_improvement_pct = (mrr_improvement / baseline_metrics['mrr'] * 100) if baseline_metrics['mrr'] > 0 else 0.0 logger.info("\n" + "="*70) logger.info("IMPROVEMENT SUMMARY") logger.info("="*70) logger.info(f"MRR improvement: {mrr_improvement:+.4f} ({mrr_improvement_pct:+.1f}%)") logger.info(f"P@10 improvement: {optimal_metrics['p@k'] - baseline_metrics['p@k']:+.4f}") logger.info(f"R@10 improvement: {optimal_metrics['r@k'] - baseline_metrics['r@k']:+.4f}") # Save results results_path = args.output_dir / "comprehensive_results.json" with open(results_path, 'w') as f: json.dump(results, f, indent=2) logger.info(f"\n Results saved to: {results_path}") return 0 if __name__ == "__main__": sys.exit(main())