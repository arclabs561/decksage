#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pyyaml>=6.0", # ] # /// """ Benchmark annotation system performance. Measures: 1. Annotation throughput (annotations per hour) 2. Metadata completeness over time 3. IAA trends 4. Quality score trends 5. Coverage expansion rate """ from __future__ import annotations import argparse import json from collections import defaultdict from datetime import datetime from pathlib import Path from typing import Any try: import yaml HAS_YAML = True except ImportError: HAS_YAML = False import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) from ml.utils.annotation_utils import load_similarity_annotations, load_hand_annotations def benchmark_annotation_system( annotations_dir: Path = Path("annotations"), ) -> dict[str, Any]: """Benchmark annotation system.""" annotation_files = ( list(annotations_dir.glob("*.yaml")) + list(annotations_dir.glob("*.jsonl")) ) # Track by timestamp annotations_by_time = [] metadata_completeness = [] quality_scores = [] for ann_file in annotation_files: try: # Get file modification time as proxy for annotation time mtime = ann_file.stat().st_mtime ann_time = datetime.fromtimestamp(mtime) if ann_file.suffix == ".yaml": annotations = load_hand_annotations(ann_file) else: annotations = load_similarity_annotations(ann_file) for ann in annotations: annotations_by_time.append({ "timestamp": ann_time.isoformat(), "annotation": ann, }) # Metadata completeness has_meta = sum(1 for f in ["model_name", "annotator_id", "timestamp"] if ann.get(f)) metadata_completeness.append({ "timestamp": ann_time.isoformat(), "completeness": has_meta / 3.0 * 100, }) # Quality score (simplified) quality = 0.0 if ann.get("similarity_score") is not None or ann.get("relevance") is not None: quality += 25 if ann.get("similarity_type"): quality += 25 if ann.get("annotator_id"): quality += 25 if any(ann.get(f) for f in ["role_match", "archetype_context"]): quality += 25 quality_scores.append({ "timestamp": ann_time.isoformat(), "quality": quality, }) except Exception as e: print(f"Warning: Could not process {ann_file}: {e}") # Sort by time annotations_by_time.sort(key=lambda x: x["timestamp"]) metadata_completeness.sort(key=lambda x: x["timestamp"]) quality_scores.sort(key=lambda x: x["timestamp"]) # Compute trends if len(annotations_by_time) > 1: time_span = (datetime.fromisoformat(annotations_by_time[-1]["timestamp"]) - datetime.fromisoformat(annotations_by_time[0]["timestamp"])).total_seconds() / 3600 throughput = len(annotations_by_time) / time_span if time_span > 0 else 0 else: throughput = 0 # Metadata trend if metadata_completeness: early_meta = sum(m["completeness"] for m in metadata_completeness[:len(metadata_completeness)//2]) / (len(metadata_completeness)//2) late_meta = sum(m["completeness"] for m in metadata_completeness[len(metadata_completeness)//2:]) / (len(metadata_completeness) - len(metadata_completeness)//2) metadata_trend = late_meta - early_meta else: metadata_trend = 0 # Quality trend if quality_scores: early_quality = sum(q["quality"] for q in quality_scores[:len(quality_scores)//2]) / (len(quality_scores)//2) late_quality = sum(q["quality"] for q in quality_scores[len(quality_scores)//2:]) / (len(quality_scores) - len(quality_scores)//2) quality_trend = late_quality - early_quality else: quality_trend = 0 return { "total_annotations": len(annotations_by_time), "throughput": { "annotations_per_hour": throughput, "time_span_hours": time_span if len(annotations_by_time) > 1 else 0, }, "metadata_trend": { "early_avg": early_meta if metadata_completeness else 0, "late_avg": late_meta if metadata_completeness else 0, "trend": metadata_trend, }, "quality_trend": { "early_avg": early_quality if quality_scores else 0, "late_avg": late_quality if quality_scores else 0, "trend": quality_trend, }, "timeline": { "annotations": annotations_by_time, "metadata": metadata_completeness, "quality": quality_scores, }, } def main() -> int: parser = argparse.ArgumentParser(description="Benchmark annotation system") parser.add_argument("--annotations-dir", type=str, default="annotations", help="Annotations directory") parser.add_argument("--output", type=str, required=True, help="Output JSON report") args = parser.parse_args() annotations_dir = Path(args.annotations_dir) if not annotations_dir.exists(): print(f"Error: Annotations directory not found: {annotations_dir}") return 1 print("=" * 70) print("ANNOTATION SYSTEM BENCHMARK") print("=" * 70) print() benchmark = benchmark_annotation_system(annotations_dir) print(f" Performance Metrics:") print(f" Total annotations: {benchmark['total_annotations']}") print(f" Throughput: {benchmark['throughput']['annotations_per_hour']:.1f} annotations/hour") print() print(f"ðŸ“ˆ Trends:") print(f" Metadata completeness:") print(f" Early avg: {benchmark['metadata_trend']['early_avg']:.1f}%") print(f" Late avg: {benchmark['metadata_trend']['late_avg']:.1f}%") print(f" Trend: {benchmark['metadata_trend']['trend']:+.1f}%") print() print(f" Quality:") print(f" Early avg: {benchmark['quality_trend']['early_avg']:.1f}%") print(f" Late avg: {benchmark['quality_trend']['late_avg']:.1f}%") print(f" Trend: {benchmark['quality_trend']['trend']:+.1f}%") print() # Save report with open(args.output, "w") as f: json.dump(benchmark, f, indent=2, default=str) print(f"âœ“ Saved benchmark to {args.output}") print() print("=" * 70) print(" Benchmark complete!") print("=" * 70) return 0 if __name__ == "__main__": import sys sys.exit(main())