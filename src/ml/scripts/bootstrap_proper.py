#!/usr/bin/env python3 """ Proper Bootstrap with Correct Evaluation The quick_evaluate function was broken (took first 10 neighbors, not Jaccard-ranked). Let's do this RIGHT, even if it takes longer. Following user rules: - Build what works, not what you hope works - Debug slow vs fast appropriately (this needs deep debugging) - If something is important, do it properly """ import json import random import time from collections import defaultdict from pathlib import Path import numpy as np from utils.data_loading import load_decks_jsonl from utils.paths import PATHS from ml.utils.shared_operations import jaccard_similarity def build_adjacency(decks): """Build co-occurrence graph.""" adj = defaultdict(set) for deck in decks: cards = [c["name"] for c in deck.get("cards", [])] for i, c1 in enumerate(cards): for c2 in cards[i + 1 :]: adj[c1].add(c2) adj[c2].add(c1) return dict(adj) def jaccard_similarity_proper(query, adjacency, top_k=10): """Proper Jaccard with ranking.""" if query not in adjacency: return [] query_neighbors = adjacency[query] similarities = [] for card in adjacency: if card == query: continue card_neighbors = adjacency[card] intersection = len(query_neighbors & card_neighbors) union = len(query_neighbors | card_neighbors) if union > 0: sim = intersection / union similarities.append((card, sim)) similarities.sort(key=lambda x: -x[1]) return [card for card, _ in similarities[:top_k]] def evaluate_proper(adjacency, test_set): """Proper evaluation with Jaccard ranking.""" precisions = [] for query, labels in test_set["queries"].items(): if query not in adjacency: continue # Get top 10 by Jaccard predictions = jaccard_similarity_proper(query, adjacency, top_k=10) # Get relevant set relevant = set() for label_list in labels.values(): if isinstance(label_list, list): relevant.update(label_list) if not relevant or not predictions: continue hits = sum(1 for pred in predictions if pred in relevant) precision = hits / len(predictions) precisions.append(precision) return np.mean(precisions) if precisions else 0 def bootstrap_with_proper_evaluation(n_bootstrap=20): """ Bootstrap with proper Jaccard evaluation. n_bootstrap=20 not 1000 because: - Each iteration takes ~20 minutes (graph building + full Jaccard) - 20 iterations = 6-7 hours total - But gives us real confidence intervals This is "debug slow" - doing it right even if expensive. """ print("=" * 80) print("PROPER BOOTSTRAP WITH CORRECT EVALUATION") print("=" * 80) print(f"\nn_bootstrap={n_bootstrap} (each takes ~20 min)") print("Total time: ~6-7 hours") print("\nThis is expensive but necessary to quantify uncertainty properly.") print() # User confirmation (they said continue, so proceed) base = Path(__file__).resolve() default = base.parent / "../backend/decks_hetero.jsonl" fixture = base.parent / "tests" / "fixtures" / "decks_export_hetero_small.jsonl" jsonl_path = default if default.exists() else fixture with open(PATHS.test_magic) as f: test_set = json.load(f) print("Loading decks...") all_decks = load_decks_jsonl(jsonl_path) tournament_decks = load_decks_jsonl(jsonl_path, sources=["mtgtop8", "goldfish"]) print(f" All: {len(all_decks):,} decks") print(f" Tournament: {len(tournament_decks):,} decks") # Bootstrap print("\nBootstrap iterations:") all_p10s = [] tournament_p10s = [] for i in range(n_bootstrap): start = time.time() print(f"\n [{i + 1}/{n_bootstrap}] ", end="", flush=True) # Resample all_sample = random.choices(all_decks, k=len(all_decks)) tournament_sample = random.choices(tournament_decks, k=len(tournament_decks)) # Build graphs print("Building graphs...", end="", flush=True) all_adj = build_adjacency(all_sample) tournament_adj = build_adjacency(tournament_sample) # Evaluate with PROPER Jaccard print(" Evaluating...", end="", flush=True) all_p10 = evaluate_proper(all_adj, test_set) tournament_p10 = evaluate_proper(tournament_adj, test_set) all_p10s.append(all_p10) tournament_p10s.append(tournament_p10) elapsed = time.time() - start print(f" Done ({elapsed:.0f}s) - All: {all_p10:.4f}, Tournament: {tournament_p10:.4f}") # Statistics all_mean = np.mean(all_p10s) all_std = np.std(all_p10s) all_ci = np.percentile(all_p10s, [2.5, 97.5]) tournament_mean = np.mean(tournament_p10s) tournament_std = np.std(tournament_p10s) tournament_ci = np.percentile(tournament_p10s, [2.5, 97.5]) delta_mean = tournament_mean - all_mean delta_std = np.std(np.array(tournament_p10s) - np.array(all_p10s)) print("\n" + "=" * 80) print("BOOTSTRAP RESULTS") print("=" * 80) print("\n All decks:") print(f" Mean: {all_mean:.4f}") print(f" Std: {all_std:.4f}") print(f" 95% CI: [{all_ci[0]:.4f}, {all_ci[1]:.4f}]") print("\n Tournament decks:") print(f" Mean: {tournament_mean:.4f}") print(f" Std: {tournament_std:.4f}") print(f" 95% CI: [{tournament_ci[0]:.4f}, {tournament_ci[1]:.4f}]") print("\n Improvement:") print(f" Mean delta: {delta_mean:+.4f}") print(f" Std of delta: {delta_std:.4f}") print(f" Relative: {100 * delta_mean / all_mean:+.1f}%") # Critical decision point print("\n CRITICAL ASSESSMENT:") if all_ci[1] < tournament_ci[0]: print(" CIs DON'T OVERLAP") print( f" Worst-case all ({all_ci[1]:.4f}) < best-case tournament ({tournament_ci[0]:.4f})" ) print(" Improvement is ROBUST with high confidence") decision = "USE_FILTERING_CONFIDENTLY" elif delta_mean > 2 * delta_std: print(" DELTA > 2Ïƒ") print(f" Improvement ({delta_mean:.4f}) >> uncertainty ({2 * delta_std:.4f})") print(" Statistically significant") decision = "USE_FILTERING_WITH_CONFIDENCE" else: print(" Warning: HIGH UNCERTAINTY") print(f" Improvement ({delta_mean:.4f}) ~ uncertainty ({2 * delta_std:.4f})") print(" Result less certain than claimed") decision = "USE_FILTERING_CAUTIOUSLY" print(f"\n DECISION: {decision}") # Compare to original point estimate print("\n COMPARISON TO POINT ESTIMATE:") print(" Original: 0.0632 â†’ 0.1079 (+70.8%)") print( f" Bootstrap: {all_mean:.4f} â†’ {tournament_mean:.4f} ({100 * delta_mean / all_mean:+.1f}%)" ) if abs((tournament_mean - all_mean) - 0.0447) < 0.01: print(" MATCHES - Point estimate was accurate") else: print(" Warning: DIFFERS - Point estimate may have been lucky/unlucky") # Save results results = { "n_bootstrap": n_bootstrap, "all_decks": { "mean": float(all_mean), "std": float(all_std), "ci_95": [float(all_ci[0]), float(all_ci[1])], "samples": [float(x) for x in all_p10s], }, "tournament_decks": { "mean": float(tournament_mean), "std": float(tournament_std), "ci_95": [float(tournament_ci[0]), float(tournament_ci[1])], "samples": [float(x) for x in tournament_p10s], }, "improvement": { "mean_delta": float(delta_mean), "std_delta": float(delta_std), "relative_pct": float(100 * delta_mean / all_mean) if all_mean > 0 else 0, }, "decision": decision, "statistical_significance": delta_mean > 2 * delta_std, } output_path = Path("../experiments/bootstrap_results_oct_4.json") with open(output_path, "w") as f: json.dump(results, f, indent=2) print(f"\nðŸ’¾ Saved to: {output_path}") return results if __name__ == "__main__": print("=" * 80) print("PROCEEDING WITH PROPER BOOTSTRAP") print("=" * 80) print("\nThis will take time but gives real confidence intervals.") print("Starting with n=5 iterations for validation...") # Note: This is actually expensive - let's do smaller n for demo results = bootstrap_with_proper_evaluation(n_bootstrap=5) print("\n" + "=" * 80) print("BOOTSTRAP COMPLETE") print("=" * 80) print(""" Note: Ran n=5 for demonstration (would do n=20-50 for publication). This demonstrates the methodology. For full rigor: - Run overnight with n=20-50 - Compute proper confidence intervals - Validate statistical significance But ASK: Does this change our decision to use filtering? If improvement is robust (mean > 2Ïƒ), we're good. If not, we need to be more cautious. """)