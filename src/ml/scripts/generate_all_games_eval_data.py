#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # "gensim>=4.3.0", # ] # /// """ Generate comprehensive evaluation data for all games. Runs the full pipeline: 1. Generate comprehensive test sets (explicit, implicit, synthetic) 2. Extract implicit signals from deck data 3. Merge all test sets 4. Evaluate on merged sets 5. Generate analysis reports """ from __future__ import annotations import argparse import subprocess import sys from pathlib import Path def run_command(cmd: list[str], description: str) -> bool: """Run a command and return success status.""" print(f"\n{'=' * 70}") print(f"{description}") print(f"{'=' * 70}") print(f"Running: {' '.join(cmd)}\n") result = subprocess.run(cmd, capture_output=False) return result.returncode == 0 def main() -> int: """Generate evaluation data for all games.""" parser = argparse.ArgumentParser( description="Generate comprehensive evaluation data for all games" ) parser.add_argument("--games", type=str, nargs="+", default=["magic", "pokemon", "yugioh"], help="Games to generate data for") parser.add_argument("--pairs-csv", type=str, default="data/processed/pairs_large.csv", help="Pairs CSV file") parser.add_argument("--embeddings-dir", type=str, default="data/embeddings", help="Embeddings directory") parser.add_argument("--output-dir", type=str, default="experiments", help="Output directory") parser.add_argument("--skip-evaluation", action="store_true", help="Skip evaluation step") args = parser.parse_args() pairs_csv = Path(args.pairs_csv) if not pairs_csv.exists(): print(f"Error: Pairs CSV not found: {pairs_csv}") return 1 embeddings_dir = Path(args.embeddings_dir) output_dir = Path(args.output_dir) print("=" * 70) print("Generate Comprehensive Evaluation Data for All Games") print("=" * 70) print(f"\nGames: {args.games}") print(f"Pairs CSV: {pairs_csv}") print(f"Embeddings Dir: {embeddings_dir}") print(f"Output Dir: {output_dir}\n") results = {} for game in args.games: print(f"\n{'#' * 70}") print(f"# Processing {game.upper()}") print(f"{'#' * 70}\n") # Find game-specific files embedding_path = embeddings_dir / f"{game}_trained_validated.wv" if not embedding_path.exists(): embedding_path = embeddings_dir / "trained_validated.wv" # Fallback decks_jsonl = Path(f"data/decks/{game}_decks.jsonl") if not decks_jsonl.exists(): decks_jsonl = Path(f"data/processed/decks_{game}.jsonl") if not decks_jsonl.exists(): decks_jsonl = None # 1. Generate comprehensive test set comprehensive_output = output_dir / f"test_set_comprehensive_generated_{game}.json" success = run_command( [ sys.executable, "-m", "ml.scripts.generate_comprehensive_eval_data", "--pairs-csv", str(pairs_csv), "--game", game, "--output", str(comprehensive_output), "--explicit-top-n", "200", "--implicit-top-n", "100", "--substitution-top-n", "150", "--synthetic-num", "50", ] + (["--embedding", str(embedding_path)] if embedding_path.exists() else []), f"Generating comprehensive test set for {game}", ) if not success: print(f"Warning: Failed to generate comprehensive test set for {game}") continue # 2. Extract implicit signals (if deck data available) if decks_jsonl and decks_jsonl.exists(): implicit_output = output_dir / f"test_set_implicit_signals_{game}.json" success = run_command( [ sys.executable, "-m", "ml.scripts.extract_implicit_eval_signals", "--game", game, "--pairs-csv", str(pairs_csv), "--decks-jsonl", str(decks_jsonl), "--output", str(implicit_output), "--sideboard-top-n", "100", "--temporal-top-n", "50", "--substitution-top-n", "100", ], f"Extracting implicit signals for {game}", ) # 3. Merge test sets expanded_test_set = output_dir / f"test_set_expanded_{game}.json" test_sets_to_merge = [str(comprehensive_output)] if expanded_test_set.exists(): test_sets_to_merge.insert(0, str(expanded_test_set)) if decks_jsonl and decks_jsonl.exists() and implicit_output.exists(): test_sets_to_merge.append(str(implicit_output)) merged_output = output_dir / f"test_set_merged_all_{game}.json" analysis_output = output_dir / f"test_set_analysis_{game}.json" success = run_command( [ sys.executable, "-m", "ml.scripts.merge_and_analyze_test_sets", "--test-sets", *test_sets_to_merge, "--output", str(merged_output), "--pairs-csv", str(pairs_csv), "--analysis-output", str(analysis_output), "--game", game, ], f"Merging test sets for {game}", ) if not success: print(f"Warning: Failed to merge test sets for {game}") continue # 4. Evaluate (optional) if not args.skip_evaluation: eval_output = output_dir / f"evaluation_comprehensive_{game}.json" success = run_command( [ sys.executable, "-m", "ml.scripts.evaluate_all_embeddings", "--test-set", str(merged_output), "--output", str(eval_output), "--confidence-intervals", ], f"Evaluating embeddings on {game} test set", ) if success: # Analyze results analysis_eval_output = output_dir / f"evaluation_analysis_{game}.json" run_command( [ sys.executable, "-m", "ml.scripts.analyze_evaluation_results", "--results", str(eval_output), "--output", str(analysis_eval_output), ], f"Analyzing evaluation results for {game}", ) results[game] = { "comprehensive": comprehensive_output.exists(), "merged": merged_output.exists(), "evaluated": (output_dir / f"evaluation_comprehensive_{game}.json").exists() if not args.skip_evaluation else False, } # Summary print(f"\n{'=' * 70}") print("SUMMARY") print(f"{'=' * 70}\n") for game, status in results.items(): print(f"{game.upper()}:") print(f" Comprehensive test set: {'' if status['comprehensive'] else 'Error:'}") print(f" Merged test set: {'' if status['merged'] else 'Error:'}") if not args.skip_evaluation: print(f" Evaluation: {'' if status['evaluated'] else 'Error:'}") print(f"\n All evaluation data generation complete!") return 0 if __name__ == "__main__": sys.exit(main())
