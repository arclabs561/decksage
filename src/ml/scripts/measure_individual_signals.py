#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # "gensim>=4.3.0", # ] # /// """ Measure individual signal performance on test set. Measures P@10 for each similarity signal independently: - Embedding signal alone - Jaccard signal alone - Functional signal alone Then compares to fusion to understand signal contributions. """ from __future__ import annotations import argparse import json from pathlib import Path from typing import Any try: import pandas as pd import numpy as np from gensim.models import KeyedVectors HAS_DEPS = True except ImportError as e: HAS_DEPS = False print(f"Missing dependencies: {e}") def load_test_set(test_set_path: Path) -> dict[str, dict[str, Any]]: """Load test set (uses canonical implementation).""" from ml.utils.data_loading import load_test_set as canonical_load data = canonical_load(path=test_set_path) if "queries" in data: return data["queries"] return data def get_all_relevant(labels: dict[str, list[str]]) -> set[str]: """Get all relevant cards (combining all relevance levels).""" relevant = set() for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"]: relevant.update(labels.get(level, [])) return relevant def weighted_precision_at_k( predictions: list[str], labels: dict[str, list[str]], k: int = 10, ) -> float: """Compute weighted precision at k.""" if not predictions: return 0.0 relevance_weights = { "highly_relevant": 1.0, "relevant": 0.75, "somewhat_relevant": 0.5, "marginally_relevant": 0.25, "irrelevant": 0.0, } top_k = predictions[:k] score = 0.0 for card in top_k: for level, weight in relevance_weights.items(): if card in labels.get(level, []): score += weight break return score / k def evaluate_embedding_signal( wv: KeyedVectors, test_set: dict[str, dict[str, Any]], top_k: int = 10, ) -> dict[str, Any]: """Evaluate embedding signal alone (match evaluate_all_embeddings methodology).""" correct = 0 total = 0 reciprocal_ranks = [] relevance_weights = { "highly_relevant": 1.0, "relevant": 0.75, "somewhat_relevant": 0.5, "marginally_relevant": 0.25, "irrelevant": 0.0, } for query, labels in test_set.items(): if query not in wv: continue all_relevant = get_all_relevant(labels) if not all_relevant: continue try: similar = wv.most_similar(query, topn=top_k) candidates = [card for card, _ in similar] except KeyError: continue # Precision@K (weighted by relevance) - match evaluate_all_embeddings score = 0.0 for card in candidates[:top_k]: for level, weight in relevance_weights.items(): if card in labels.get(level, []): score += weight break precision_at_k = score / top_k if precision_at_k > 0: correct += 1 # MRR found = False for rank, candidate in enumerate(candidates, 1): if candidate in all_relevant: reciprocal_ranks.append(1.0 / rank) found = True break if not found: reciprocal_ranks.append(0.0) total += 1 if total == 0: return {"p@10": 0.0, "mrr": 0.0, "num_queries": 0} precision = correct / total return { "p@10": float(precision), "mrr": float(np.mean(reciprocal_ranks)), "num_queries": total, } def evaluate_jaccard_signal( adj: dict[str, set[str]], test_set: dict[str, dict[str, Any]], top_k: int = 10, ) -> dict[str, Any]: """Evaluate Jaccard signal alone (match evaluate_all_embeddings methodology).""" def jaccard_similarity(set1: set[str], set2: set[str]) -> float: intersection = len(set1 & set2) union = len(set1 | set2) return intersection / union if union > 0 else 0.0 correct = 0 total = 0 reciprocal_ranks = [] relevance_weights = { "highly_relevant": 1.0, "relevant": 0.75, "somewhat_relevant": 0.5, "marginally_relevant": 0.25, "irrelevant": 0.0, } for query, labels in test_set.items(): if query not in adj: continue all_relevant = get_all_relevant(labels) if not all_relevant: continue # Compute Jaccard similarity to all neighbors query_neighbors = adj[query] similarities = [] for candidate in adj.keys(): if candidate == query: continue candidate_neighbors = adj[candidate] sim = jaccard_similarity(query_neighbors, candidate_neighbors) similarities.append((candidate, sim)) # Sort by similarity similarities.sort(key=lambda x: x[1], reverse=True) candidates = [card for card, _ in similarities[:top_k]] # Precision@K (weighted by relevance) - match evaluate_all_embeddings score = 0.0 for card in candidates[:top_k]: for level, weight in relevance_weights.items(): if card in labels.get(level, []): score += weight break precision_at_k = score / top_k if precision_at_k > 0: correct += 1 # MRR found = False for rank, candidate in enumerate(candidates, 1): if candidate in all_relevant: reciprocal_ranks.append(1.0 / rank) found = True break if not found: reciprocal_ranks.append(0.0) total += 1 if total == 0: return {"p@10": 0.0, "mrr": 0.0, "num_queries": 0} precision = correct / total return { "p@10": float(precision), "mrr": float(np.mean(reciprocal_ranks)), "num_queries": total, } def load_graph_for_jaccard(pairs_csv: Path | None = None, graph_db: Path | None = None, game: str | None = None) -> dict[str, set[str]]: """Load graph adjacency for Jaccard similarity (uses shared implementation).""" from ml.utils.shared_operations import load_graph_for_jaccard as shared_load return shared_load(pairs_csv=pairs_csv, graph_db=graph_db, game=game) def main() -> int: """Measure individual signal performance.""" parser = argparse.ArgumentParser(description="Measure individual signal performance") parser.add_argument( "--test-set", type=str, default="experiments/test_set_unified_magic.json", help="Test set path", ) parser.add_argument( "--pairs-csv", type=str, default="data/processed/pairs_large.csv", help="Pairs CSV for Jaccard", ) parser.add_argument( "--embeddings", type=str, default="data/embeddings/node2vec_default.wv", help="Embeddings file", ) parser.add_argument( "--output", type=str, default="experiments/individual_signal_performance.json", help="Output JSON file", ) args = parser.parse_args() if not HAS_DEPS: print("Error: Missing dependencies") return 1 print("=" * 70) print("Measure Individual Signal Performance") print("=" * 70) print() # Load test set test_set_path = Path(args.test_set) if not test_set_path.exists(): print(f"Error: Test set not found: {test_set_path}") return 1 test_set = load_test_set(test_set_path) print(f" Loaded test set: {len(test_set)} queries") print() results = {} # Evaluate embedding signal embed_path = Path(args.embeddings) if embed_path.exists(): print("Evaluating embedding signal...") try: wv = KeyedVectors.load(str(embed_path)) metrics = evaluate_embedding_signal(wv, test_set) results["embedding"] = metrics print(f" P@10: {metrics['p@10']:.4f}, MRR: {metrics['mrr']:.4f}, Queries: {metrics['num_queries']}") except Exception as e: print(f" Error: Error: {e}") results["embedding"] = {"error": str(e)} else: print(f"Warning: Embeddings not found: {embed_path}") results["embedding"] = {"error": "File not found"} print() # Evaluate Jaccard signal pairs_csv = Path(args.pairs_csv) if pairs_csv.exists(): print("Evaluating Jaccard signal...") try: adj = load_graph_for_jaccard(pairs_csv) metrics = evaluate_jaccard_signal(adj, test_set) results["jaccard"] = metrics print(f" P@10: {metrics['p@10']:.4f}, MRR: {metrics['mrr']:.4f}, Queries: {metrics['num_queries']}") except Exception as e: print(f" Error: Error: {e}") results["jaccard"] = {"error": str(e)} else: print(f"Warning: Pairs CSV not found: {pairs_csv}") results["jaccard"] = {"error": "File not found"} print() # Functional signal print("=" * 70) print("Measuring Functional Signal") print("=" * 70) try: from ml.enrichment.card_functional_tagger import FunctionalTagger from ml.enrichment.unified_functional_tagger import UnifiedFunctionalTagger # Try unified tagger first (multi-game support) try: tagger = UnifiedFunctionalTagger() print(" Using UnifiedFunctionalTagger") except Exception: # Fallback to game-specific tagger tagger = FunctionalTagger() print(" Using FunctionalTagger") if test_set_path and test_set_path.exists(): with open(test_set_path) as f: test_data = json.load(f) test_set = test_data.get("queries", test_data) # Measure functional similarity scores = [] for query, labels in list(test_set.items())[:100]: # Sample for speed if isinstance(labels, dict): relevant = labels.get("highly_relevant", []) + labels.get("relevant", []) else: relevant = labels if isinstance(labels, list) else [] if not relevant: continue query_tags = set(tagger.tag_card(query) or []) if not query_tags: continue for candidate in relevant[:5]: # Top 5 candidates candidate_tags = set(tagger.tag_card(candidate) or []) if candidate_tags: # Jaccard similarity of functional tags intersection = len(query_tags & candidate_tags) union = len(query_tags | candidate_tags) if union > 0: jaccard = intersection / union scores.append(jaccard) if scores: avg_score = sum(scores) / len(scores) results["functional"] = { "avg_tag_similarity": avg_score, "num_pairs": len(scores), "p@10_estimate": avg_score, # Rough estimate } print(f" Average functional tag similarity: {avg_score:.4f}") print(f" Evaluated {len(scores)} card pairs") else: results["functional"] = {"error": "No taggable pairs found"} print(" Warning: No taggable pairs found") else: results["functional"] = {"error": "Test set not found"} print(" Warning: Test set not found") except ImportError as e: print(f" Warning: FunctionalTagger not available: {e}") results["functional"] = {"error": f"Import failed: {e}"} except Exception as e: print(f" Error: Error measuring functional signal: {e}") results["functional"] = {"error": str(e)} print() # Summary print("=" * 70) print("Results Summary") print("=" * 70) print() print(f"{'Signal':<20} {'P@10':<10} {'MRR':<10} {'Queries':<10}") print("-" * 50) for signal, metrics in results.items(): if "error" not in metrics: print( f"{signal:<20} {metrics.get('p@10', 0.0):<10.4f} " f"{metrics.get('mrr', 0.0):<10.4f} {metrics.get('num_queries', 0):<10}" ) else: print(f"{signal:<20} {'ERROR':<10}") # Save results output_path = Path(args.output) output_path.parent.mkdir(parents=True, exist_ok=True) with open(output_path, "w") as f: json.dump({ "test_set": str(test_set_path), "results": results, }, f, indent=2) print() print(f" Results saved to {output_path}") return 0 if __name__ == "__main__": import sys sys.exit(main())