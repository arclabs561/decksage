#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pydantic-ai>=0.0.12", # ] # /// """ Iteratively refine gold data (test sets) by adding more judgments. Strategy: 1. Expand queries: Add new queries to increase coverage 2. Deepen labels: Add more labels to existing queries (increase label count) 3. Re-label low quality: Re-label queries with low IAA or insufficient labels 4. Iterate: Repeat until quality targets are met This creates a continuous improvement loop for gold data. Quality Targets: - Label density: 15-25+ labels per query - IAA: â‰¥0.7 (Inter-Annotator Agreement) - Coverage: All card types, formats, archetypes Usage: # Quick test (5 queries, 1 iteration) uv run --script src/ml/scripts/iterative_refine_gold_data.py \ --input experiments/test_set_unified_magic.json \ --output experiments/test_set_refined.json \ --iterations 1 \ --max-queries-per-iter 5 # Production refinement (20 queries per iteration, 3 iterations) uv run --script src/ml/scripts/iterative_refine_gold_data.py \ --input experiments/test_set_unified_magic.json \ --output experiments/test_set_refined.json \ --iterations 3 \ --max-queries-per-iter 20 \ --num-judges 3 \ --min-labels-target 20 See docs/GOLD_DATA_REFINEMENT.md for complete documentation. """ from __future__ import annotations import argparse import json import logging from pathlib import Path from typing import Any try: from pydantic_ai import Agent HAS_PYDANTIC_AI = True except ImportError: HAS_PYDANTIC_AI = False from ..utils.logging_config import setup_script_logging logger = setup_script_logging() # Import path handling import sys script_dir = Path(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) # Import labeling functions try: from ml.scripts.generate_labels_multi_judge import generate_labels_multi_judge HAS_MULTI_JUDGE = True except ImportError: HAS_MULTI_JUDGE = False logger.warning("Multi-judge not available, trying parallel version...") try: from ml.scripts.parallel_multi_judge import generate_labels_parallel as generate_labels_multi_judge HAS_MULTI_JUDGE = True logger.info("Using parallel multi-judge") except ImportError: HAS_MULTI_JUDGE = False generate_labels_multi_judge = None try: from ml.scripts.expand_test_set_with_llm import expand_test_set HAS_EXPAND = True except ImportError: HAS_EXPAND = False expand_test_set = None def detect_game_from_test_set(test_set_path: Path, queries: dict[str, Any] | None = None) -> str: """Detect game from test set path or metadata.""" # Use centralized game detection utility from ml.utils.game_detection import detect_game # Build deck-like dict from queries if available deck_dict = None if queries and isinstance(queries, dict): deck_dict = {"game": queries.get("game")} # Use centralized detection return detect_game( deck=deck_dict, file_path=test_set_path, default="magic", ) def analyze_test_set_quality(test_set_path: Path) -> dict[str, Any]: """Analyze current test set quality.""" with open(test_set_path) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data, dict) else data stats = { "total_queries": len(queries), "queries_with_10plus_labels": 0, "queries_with_20plus_labels": 0, "queries_with_low_iaa": 0, "queries_with_high_iaa": 0, "queries_with_both_levels": 0, # Both highly_relevant AND relevant "avg_labels_per_query": 0.0, "avg_iaa": 0.0, "total_labels": 0, } iaa_scores = [] for query, labels in queries.items(): if not isinstance(labels, dict): continue # Count labels highly_rel = len(labels.get("highly_relevant", [])) relevant = len(labels.get("relevant", [])) somewhat_rel = len(labels.get("somewhat_relevant", [])) marginally_rel = len(labels.get("marginally_relevant", [])) total_labels = highly_rel + relevant + somewhat_rel + marginally_rel stats["total_labels"] += total_labels if total_labels >= 10: stats["queries_with_10plus_labels"] += 1 if total_labels >= 20: stats["queries_with_20plus_labels"] += 1 if highly_rel > 0 and relevant > 0: stats["queries_with_both_levels"] += 1 # Check IAA iaa = labels.get("iaa", {}) agreement = iaa.get("agreement_rate", 1.0) if agreement > 0: iaa_scores.append(agreement) if agreement < 0.6: stats["queries_with_low_iaa"] += 1 elif agreement >= 0.8: stats["queries_with_high_iaa"] += 1 if stats["total_queries"] > 0: stats["avg_labels_per_query"] = stats["total_labels"] / stats["total_queries"] if iaa_scores: stats["avg_iaa"] = sum(iaa_scores) / len(iaa_scores) return stats def deepen_labels_for_query( query: str, existing_labels: dict[str, Any], num_judges: int = 3, target_labels: int = 20, ) -> dict[str, Any]: """ Add more labels to an existing query. Strategy: 1. Generate additional labels with multi-judge 2. Merge with existing labels (avoid duplicates) 3. Maintain relevance level assignments 4. Prefer higher relevance levels when merging """ if not HAS_MULTI_JUDGE or not generate_labels_multi_judge: return existing_labels # Check current label count current_count = sum( len(existing_labels.get(level, [])) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"] ) if current_count >= target_labels: return existing_labels # Already has enough # Generate additional labels use_case = existing_labels.get("use_case") # Detect game from test set if available game = None # Note: This function doesn't have access to test_set_path, so game detection # would need to be passed in or detected from query. For now, use None and let # generate_labels_multi_judge detect it. # Use parallel version if available (faster) try: from ml.scripts.parallel_multi_judge import generate_labels_parallel new_labels = generate_labels_parallel( query, num_judges=num_judges, use_case=use_case, game=game, max_workers=num_judges, timeout=120.0, # 2 minutes per judge ) except (ImportError, TypeError) as e: logger.warning(f"Parallel labeling failed, falling back: {e}") new_labels = generate_labels_multi_judge(query, num_judges=num_judges, use_case=use_case, game=game) except Exception as e: logger.error(f"Label generation failed for {query}: {e}") return existing_labels if not new_labels: return existing_labels # Merge labels (avoid duplicates, prefer higher relevance) existing_sets = { level: set(existing_labels.get(level, [])) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"] } merged = existing_labels.copy() added_count = 0 # Process from highest to lowest relevance (prefer higher levels) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"]: new_cards = new_labels.get(level, []) for card in new_cards: # Skip if already in this level if card in existing_sets[level]: continue # Check if card is in a higher relevance level (don't downgrade) in_higher_level = False higher_levels = { "relevant": ["highly_relevant"], "somewhat_relevant": ["highly_relevant", "relevant"], "marginally_relevant": ["highly_relevant", "relevant", "somewhat_relevant"], } for higher_level in higher_levels.get(level, []): if card in existing_sets[higher_level]: in_higher_level = True break if not in_higher_level: merged.setdefault(level, []).append(card) added_count += 1 # Update IAA (use best of existing and new) existing_iaa = existing_labels.get("iaa", {}) new_iaa = new_labels.get("iaa", {}) if new_iaa.get("agreement_rate", 0) > existing_iaa.get("agreement_rate", 0): merged["iaa"] = new_iaa if added_count > 0: merged["_deepened"] = True merged["_deepened_labels_added"] = added_count return merged def iterative_refine( test_set_path: Path, output_path: Path, num_iterations: int = 3, num_new_queries_per_iter: int = 20, num_judges: int = 3, min_labels_target: int = 15, min_iaa_target: float = 0.7, deepen_existing: bool = True, expand_queries: bool = True, max_queries_per_iter: int | None = None, ) -> dict[str, Any]: """ Iteratively refine test set by expanding and deepening. Args: test_set_path: Input test set output_path: Output refined test set num_iterations: Number of refinement iterations num_new_queries_per_iter: New queries to add per iteration num_judges: Number of judges per query min_labels_target: Target minimum labels per query min_iaa_target: Target minimum IAA deepen_existing: Whether to add more labels to existing queries expand_queries: Whether to add new queries """ if not HAS_PYDANTIC_AI: logger.error("pydantic-ai required") return {} # Load initial test set if test_set_path.exists(): with open(test_set_path) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data, dict) else data logger.info(f"Loaded {len(queries)} existing queries") else: queries = {} logger.info("Starting with empty test set") # Detect game from test set (or use provided) if game: detected_game = game logger.info(f"Using provided game: {detected_game}") else: detected_game = detect_game_from_test_set(test_set_path, data if test_set_path.exists() else None) logger.info(f"Auto-detected game: {detected_game}") # Analyze initial quality logger.info("\n" + "=" * 70) logger.info("INITIAL QUALITY ANALYSIS") logger.info("=" * 70) initial_stats = analyze_test_set_quality(test_set_path) if test_set_path.exists() else { "total_queries": 0, "avg_labels_per_query": 0.0, "avg_iaa": 0.0, } logger.info(f"Total queries: {initial_stats['total_queries']}") logger.info(f"Avg labels per query: {initial_stats['avg_labels_per_query']:.1f}") logger.info(f"Avg IAA: {initial_stats['avg_iaa']:.2f}") logger.info(f"Queries with 10+ labels: {initial_stats.get('queries_with_10plus_labels', 0)}") logger.info(f"Queries with both levels: {initial_stats.get('queries_with_both_levels', 0)}") # Iterative refinement current_queries = queries.copy() for iteration in range(1, num_iterations + 1): logger.info("\n" + "=" * 70) logger.info(f"ITERATION {iteration}/{num_iterations}") logger.info("=" * 70) # Step 1: Expand queries (add new ones) if expand_queries and HAS_EXPAND and expand_test_set: logger.info(f"\nStep 1: Expanding with {num_new_queries_per_iter} new queries...") try: # Create temp file for current state temp_path = output_path.parent / f"{output_path.stem}_temp_iter{iteration}.json" with open(temp_path, "w") as f: json.dump({"queries": current_queries}, f, indent=2) # Expand result = expand_test_set( existing_test_set_path=temp_path, output_path=temp_path, num_new_queries=num_new_queries_per_iter, num_judges=num_judges, batch_size=10, game=detected_game, # Auto-detected from test set ) if result and "queries" in result: # Load expanded with open(temp_path) as f: expanded_data = json.load(f) expanded_queries = expanded_data.get("queries", expanded_data) # Merge (expanded should have all existing + new) current_queries = expanded_queries logger.info(f" Expanded to {len(current_queries)} queries") else: logger.warning(" Warning: Expansion returned no results") # Clean up temp file if temp_path.exists(): temp_path.unlink() except Exception as e: logger.warning(f" Warning: Expansion failed: {e}") # Step 2: Deepen labels for existing queries if deepen_existing: logger.info(f"\nStep 2: Deepening labels (target: {min_labels_target} per query)...") deepened_count = 0 total_added = 0 # Find queries that need deepening queries_to_deepen = [ (name, data) for name, data in current_queries.items() if isinstance(data, dict) and sum(len(data.get(level, [])) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"]) < min_labels_target ] # Limit if max_queries_per_iter is set if max_queries_per_iter and len(queries_to_deepen) > max_queries_per_iter: logger.info(f" Found {len(queries_to_deepen)} queries needing more labels (limiting to {max_queries_per_iter})") queries_to_deepen = queries_to_deepen[:max_queries_per_iter] else: logger.info(f" Found {len(queries_to_deepen)} queries needing more labels") # Process in batches with checkpointing batch_size = 10 for i, (query_name, query_data) in enumerate(queries_to_deepen, 1): current_count = sum( len(query_data.get(level, [])) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"] ) logger.info(f" [{i}/{len(queries_to_deepen)}] Deepening {query_name} ({current_count} â†’ target {min_labels_target})...") try: deepened = deepen_labels_for_query( query_name, query_data, num_judges=num_judges, target_labels=min_labels_target, ) new_count = sum( len(deepened.get(level, [])) for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"] ) if new_count > current_count: current_queries[query_name] = deepened deepened_count += 1 total_added += (new_count - current_count) logger.info(f" Added {new_count - current_count} labels (now {new_count})") else: logger.info(f" Warning: No new labels added") except Exception as e: logger.error(f" Error: Failed to deepen {query_name}: {e}") continue # Checkpoint every batch if i % batch_size == 0: checkpoint_path = output_path.parent / f"{output_path.stem}_checkpoint_iter{iteration}.json" with open(checkpoint_path, "w") as f: json.dump({"queries": current_queries}, f, indent=2) logger.info(f" ðŸ’¾ Checkpoint saved: {i}/{len(queries_to_deepen)} queries processed") logger.info(f" Deepened {deepened_count} queries, added {total_added} total labels") # Step 3: Re-label low quality queries logger.info(f"\nStep 3: Re-labeling low quality queries (IAA < {min_iaa_target})...") relabeled_count = 0 if HAS_MULTI_JUDGE and generate_labels_multi_judge: # Find queries with low IAA queries_to_relabel = [ (name, data) for name, data in current_queries.items() if isinstance(data, dict) and data.get("iaa", {}).get("agreement_rate", 1.0) < min_iaa_target ] # Limit if max_queries_per_iter is set if max_queries_per_iter and len(queries_to_relabel) > max_queries_per_iter: logger.info(f" Found {len(queries_to_relabel)} queries with low IAA (limiting to {max_queries_per_iter})") queries_to_relabel = queries_to_relabel[:max_queries_per_iter] else: logger.info(f" Found {len(queries_to_relabel)} queries with low IAA") for i, (query_name, query_data) in enumerate(queries_to_relabel, 1): # Check IAA iaa = query_data.get("iaa", {}) agreement = iaa.get("agreement_rate", 1.0) logger.info(f" [{i}/{len(queries_to_relabel)}] Re-labeling {query_name} (IAA: {agreement:.2f} â†’ target {min_iaa_target})...") use_case = query_data.get("use_case") try: new_labels = generate_labels_multi_judge( query_name, num_judges=num_judges, use_case=use_case, ) if new_labels and new_labels.get("iaa", {}).get("agreement_rate", 0) > agreement: # Merge: keep existing labels, add new ones merged = query_data.copy() for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"]: existing = set(merged.get(level, [])) new = set(new_labels.get(level, [])) merged[level] = list(existing | new) # Union merged["iaa"] = new_labels.get("iaa", {}) merged["_relabeled"] = True current_queries[query_name] = merged relabeled_count += 1 new_agreement = new_labels.get("iaa", {}).get("agreement_rate", 0) logger.info(f" Improved IAA: {agreement:.2f} â†’ {new_agreement:.2f}") else: logger.info(f" Warning: IAA did not improve") except Exception as e: logger.error(f" Error: Failed to re-label {query_name}: {e}") continue logger.info(f" Re-labeled {relabeled_count} queries") # Analyze quality after iteration logger.info(f"\nStep 4: Quality analysis after iteration {iteration}...") temp_analysis_path = output_path.parent / f"{output_path.stem}_analysis_iter{iteration}.json" with open(temp_analysis_path, "w") as f: json.dump({"queries": current_queries}, f, indent=2) stats = analyze_test_set_quality(temp_analysis_path) logger.info(f" Total queries: {stats['total_queries']}") logger.info(f" Avg labels per query: {stats['avg_labels_per_query']:.1f}") logger.info(f" Avg IAA: {stats['avg_iaa']:.2f}") logger.info(f" Queries with 10+ labels: {stats['queries_with_10plus_labels']}/{stats['total_queries']}") logger.info(f" Queries with both levels: {stats['queries_with_both_levels']}/{stats['total_queries']}") # Checkpoint checkpoint_path = output_path.parent / f"{output_path.stem}_iter{iteration}.json" with open(checkpoint_path, "w") as f: json.dump({ "version": f"iterative_refined_iter{iteration}", "iteration": iteration, "queries": current_queries, }, f, indent=2) logger.info(f" ðŸ’¾ Checkpoint saved to {checkpoint_path}") # Final save logger.info("\n" + "=" * 70) logger.info("FINAL QUALITY ANALYSIS") logger.info("=" * 70) final_data = { "version": "iterative_refined", "iterations": num_iterations, "queries": current_queries, } with open(output_path, "w") as f: json.dump(final_data, f, indent=2) final_stats = analyze_test_set_quality(output_path) logger.info(f"Total queries: {final_stats['total_queries']} (started with {initial_stats['total_queries']})") logger.info(f"Avg labels per query: {final_stats['avg_labels_per_query']:.1f} (started with {initial_stats['avg_labels_per_query']:.1f})") logger.info(f"Avg IAA: {final_stats['avg_iaa']:.2f} (started with {initial_stats['avg_iaa']:.2f})") logger.info(f"Queries with 10+ labels: {final_stats['queries_with_10plus_labels']}/{final_stats['total_queries']}") logger.info(f"Queries with both levels: {final_stats['queries_with_both_levels']}/{final_stats['total_queries']}") logger.info(f"\n Refined test set saved to {output_path}") return { "initial_stats": initial_stats, "final_stats": final_stats, "improvement": { "queries_added": final_stats["total_queries"] - initial_stats["total_queries"], "labels_per_query_delta": final_stats["avg_labels_per_query"] - initial_stats["avg_labels_per_query"], "iaa_delta": final_stats["avg_iaa"] - initial_stats["avg_iaa"], }, } def main() -> int: """Iteratively refine gold data.""" parser = argparse.ArgumentParser( description="Iteratively refine gold data with more judgments" ) parser.add_argument( "--input", type=str, required=True, help="Input test set JSON", ) parser.add_argument( "--output", type=str, required=True, help="Output refined test set JSON", ) parser.add_argument( "--iterations", type=int, default=3, help="Number of refinement iterations", ) parser.add_argument( "--new-queries-per-iter", type=int, default=20, help="New queries to add per iteration", ) parser.add_argument( "--num-judges", type=int, default=3, help="Number of judges per query", ) parser.add_argument( "--min-labels-target", type=int, default=15, help="Target minimum labels per query", ) parser.add_argument( "--min-iaa-target", type=float, default=0.7, help="Target minimum IAA", ) parser.add_argument( "--no-deepen", action="store_true", help="Skip deepening existing queries", ) parser.add_argument( "--no-expand", action="store_true", help="Skip expanding with new queries", ) parser.add_argument( "--max-queries-per-iter", type=int, default=None, help="Maximum queries to process per iteration (for deepening/re-labeling). None = process all", ) parser.add_argument( "--game", type=str, default=None, choices=["magic", "pokemon", "yugioh"], help="Game name (auto-detected from test set if not provided)", ) args = parser.parse_args() if not HAS_PYDANTIC_AI: logger.error("pydantic-ai required: pip install pydantic-ai") return 1 result = iterative_refine( test_set_path=Path(args.input), output_path=Path(args.output), num_iterations=args.iterations, num_new_queries_per_iter=args.new_queries_per_iter, num_judges=args.num_judges, min_labels_target=args.min_labels_target, min_iaa_target=args.min_iaa_target, deepen_existing=not args.no_deepen, expand_queries=not args.no_expand, max_queries_per_iter=args.max_queries_per_iter, game=args.game, ) if result: print("\n=== Refinement Summary ===") print(f"Iterations: {args.iterations}") print(f"Queries added: {result['improvement']['queries_added']}") print(f"Labels per query improvement: {result['improvement']['labels_per_query_delta']:.1f}") print(f"IAA improvement: {result['improvement']['iaa_delta']:.2f}") return 0 return 1 if __name__ == "__main__": import sys sys.exit(main())