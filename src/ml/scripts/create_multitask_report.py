#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [] # /// """ Create comprehensive multi-task training report. Aggregates results from all variants and creates HTML report. """ from __future__ import annotations import argparse import json from pathlib import Path from typing import Any def load_evaluation(path: Path) -> dict[str, Any] | None: """Load evaluation JSON.""" if not path.exists(): return None with open(path) as f: return json.load(f) def create_html_report( comparison: dict[str, Any], evaluations: dict[str, dict[str, Any]], output_path: Path, ) -> None: """Create HTML report.""" html_template = """<!DOCTYPE html> <html> <head> <title>Multi-Task Embedding Comparison</title> <style> body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; background: #f5f5f5; } h1 { color: #333; } .metrics-table { background: white; border-radius: 8px; padding: 20px; margin: 20px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.1); } table { width: 100%; border-collapse: collapse; } th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; } th { background: #f8f9fa; font-weight: 600; } tr:hover { background: #f8f9fa; } .metric-value { font-family: 'Monaco', monospace; } .good {{ color: #28a745; font-weight: 600; }} .bad {{ color: #dc3545; }} .warning {{ color: #ffc107; }} .improvement {{ color: #28a745; }} .degradation {{ color: #dc3545; }} </style> </head> <body> <h1>Multi-Task Embedding Comparison</h1> <p>Generated: {timestamp}</p> <div class="metrics-table"> <h2>Variant Comparison</h2> <table> <thead> <tr> <th>Variant</th> <th>Co-occurrence P@10</th> <th>Functional P@10</th> <th>Overall Score</th> <th>Improvement</th> </tr> </thead> <tbody> {rows} </tbody> </table> </div> <div class="metrics-table"> <h2>Key Findings</h2> <ul> {findings} </ul> </div> </body> </html> """ html = html_template # Build rows rows = [] baseline_co = None baseline_func = None for eval_name, eval_data in evaluations.items(): if eval_data is None: continue tasks = eval_data.get("tasks", {}) co = tasks.get("cooccurrence", {}) func = tasks.get("functional_similarity", {}) multitask = eval_data.get("multitask", {}) co_p10 = co.get("p@10", 0) func_p10 = func.get("p@10", 0) overall = multitask.get("overall_score", 0) # Set baseline if "baseline" in eval_name or "node2vec_default" in eval_name: baseline_co = co_p10 baseline_func = func_p10 # Calculate improvements co_improvement = "" func_improvement = "" if baseline_co and baseline_co > 0: co_change = ((co_p10 - baseline_co) / baseline_co) * 100 co_improvement = f"{co_change:+.1f}%" co_class = "improvement" if co_change > 0 else "degradation" else: co_class = "" if baseline_func and baseline_func > 0: func_change = ((func_p10 - baseline_func) / baseline_func) * 100 func_improvement = f"{func_change:+.1f}%" func_class = "improvement" if func_change > 0 else "degradation" else: func_class = "" co_class_str = f' class="{co_class}"' if co_class else "" func_class_str = f' class="{func_class}"' if func_class else "" row = f""" <tr> <td><strong>{eval_name}</strong></td> <td class="metric-value">{co_p10:.4f} <span{co_class_str}>{co_improvement}</span></td> <td class="metric-value">{func_p10:.4f} <span{func_class_str}>{func_improvement}</span></td> <td class="metric-value">{overall:.4f}</td> <td>{co_improvement} / {func_improvement}</td> </tr>""" rows.append(row) # Build findings findings_list = [] if baseline_co and baseline_func: best_func = max( [(name, data) for name, data in evaluations.items() if data], key=lambda x: x[1].get("tasks", {}).get("functional_similarity", {}).get("p@10", 0), default=(None, None) ) if best_func[0]: best_func_p10 = best_func[1].get("tasks", {}).get("functional_similarity", {}).get("p@10", 0) func_improvement = ((best_func_p10 - baseline_func) / baseline_func) * 100 findings_list.append(f"<li>Best functional similarity: <strong>{best_func[0]}</strong> ({best_func_p10:.4f}, {func_improvement:+.1f}% vs baseline)</li>") findings = "\n".join(findings_list) if findings_list else "<li>No findings available</li>" html = html_template.format( timestamp=comparison.get("timestamp", "unknown"), rows="".join(rows), findings=findings, ) output_path.parent.mkdir(parents=True, exist_ok=True) with open(output_path, "w") as f: f.write(html) def main() -> int: """Create comprehensive report.""" parser = argparse.ArgumentParser(description="Create multi-task comparison report") parser.add_argument("--comparison", type=Path, default=Path("experiments/multitask_comparison.json"), help="Comparison JSON") parser.add_argument("--evaluations-dir", type=Path, default=Path("experiments"), help="Directory with evaluation JSONs") parser.add_argument("--output", type=Path, default=Path("experiments/multitask_report.html"), help="Output HTML") args = parser.parse_args() # Load comparison if args.comparison.exists(): with open(args.comparison) as f: comparison = json.load(f) else: comparison = {} # Load all evaluations evaluations = {} eval_files = [ ("baseline", args.evaluations_dir / "multitask_evaluation.json"), ("multitask_sub2", args.evaluations_dir / "multitask_evaluation_sub2.json"), ("multitask_sub5", args.evaluations_dir / "multitask_evaluation_sub5.json"), ("multitask_sub10", args.evaluations_dir / "multitask_evaluation_sub10.json"), ] for name, path in eval_files: evaluations[name] = load_evaluation(path) # Create report create_html_report(comparison, evaluations, args.output) print(f" Report saved to {args.output}") return 0 if __name__ == "__main__": exit(main())
