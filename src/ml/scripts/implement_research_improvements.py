#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # ] # /// """ Implement research-based improvements to annotation and evaluation. Based on comprehensive research on: 1. Annotation quality and inter-annotator agreement 2. Evaluation task design for recommendation systems 3. Graph embedding techniques for functional similarity 4. Multi-task learning and heterogeneous graph approaches This script orchestrates: - Enhanced multi-annotator labeling with IAA tracking - Test set expansion for all identified tasks - Quality assurance workflows - Comprehensive evaluation across all tasks """ from __future__ import annotations import argparse import json import logging from pathlib import Path from typing import Any from ..utils.logging_config import setup_script_logging logger = setup_script_logging() def expand_test_sets_all_tasks( base_test_set: Path, output_dir: Path, games: list[str] = ["magic", "pokemon", "yugioh"], target_sizes: dict[str, dict[str, int]] | None = None, ) -> None: """ Expand test sets for all primary tasks (1-6). Tasks: 1. Card Similarity (primary) 2. Functional Substitution 3. Synergy Discovery 4. Deck Completion 5. Contextual Discovery 6. Archetype Classification """ if target_sizes is None: target_sizes = { "magic": { "similarity": 200, "substitution": 100, "synergy": 100, "deck_completion": 50, "contextual": 100, "archetype": 50, }, "pokemon": { "similarity": 100, "substitution": 50, "synergy": 50, "deck_completion": 25, "contextual": 50, "archetype": 25, }, "yugioh": { "similarity": 100, "substitution": 50, "synergy": 50, "deck_completion": 25, "contextual": 50, "archetype": 25, }, } logger.info("=" * 70) logger.info("EXPANDING TEST SETS FOR ALL PRIMARY TASKS") logger.info("=" * 70) logger.info("") for game in games: logger.info(f"Processing {game}...") game_sizes = target_sizes.get(game, {}) # Task 1: Card Similarity (primary) logger.info(f" Task 1: Card Similarity (target: {game_sizes.get('similarity', 0)})") # Use existing expand_test_set_with_llm.py # Task 2: Functional Substitution logger.info(f" Task 2: Functional Substitution (target: {game_sizes.get('substitution', 0)})") # Use generate_substitution_pairs_llm.py # Task 3: Synergy Discovery logger.info(f" Task 3: Synergy Discovery (target: {game_sizes.get('synergy', 0)})") # TODO: Create synergy test set generator # Task 4: Deck Completion logger.info(f" Task 4: Deck Completion (target: {game_sizes.get('deck_completion', 0)})") # Use create_downstream_test_data.py # Task 5: Contextual Discovery logger.info(f" Task 5: Contextual Discovery (target: {game_sizes.get('contextual', 0)})") # Use create_downstream_test_data.py # Task 6: Archetype Classification logger.info(f" Task 6: Archetype Classification (target: {game_sizes.get('archetype', 0)})") # TODO: Create archetype classification test set generator logger.info("") logger.info(" Test set expansion planned for all primary tasks") def enhance_multi_judge_labeling( test_set_path: Path, output_path: Path, num_judges: int = 5, min_agreement: float = 0.65, ) -> None: """ Enhance multi-judge labeling with IAA tracking and quality assurance. Based on research: - Use 3-5 annotators per item (research shows optimal range) - Track inter-annotator agreement (Krippendorff's Alpha â‰¥ 0.65 for medium-risk) - Weight annotators by reliability (Dawid-Skene approach) - Re-annotate low-agreement items (< 0.60 threshold) Research References: - Multi-annotator validation: https://mindkosh.com/blog/multi-annotator-validation-enhancing-label-accuracy-through-consensus - Krippendorff's Alpha: https://labelstud.io/blog/how-to-use-krippendorff-s-alpha-to-measure-annotation-agreement - Quality assurance: https://cleverx.com/blog/annotator-agreement-metrics-measuring-and-maintaining-annotation-quality-at-scale """ logger.info("=" * 70) logger.info("ENHANCING MULTI-JUDGE LABELING") logger.info("=" * 70) logger.info("") logger.info(f" Test set: {test_set_path}") logger.info(f" Output: {output_path}") logger.info(f" Judges per item: {num_judges}") logger.info(f" Minimum agreement: {min_agreement}") logger.info("") # Load existing test set if not test_set_path.exists(): logger.error(f"Test set not found: {test_set_path}") return with open(test_set_path) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data, dict) else data logger.info(f" Found {len(queries)} queries") logger.info("") logger.info(" Using generate_labels_multi_judge.py for enhanced labeling") logger.info(" Using compute_iaa_for_test_set.py for agreement tracking") logger.info("") logger.info(" Multi-judge labeling enhancement ready") def implement_quality_assurance( test_set_path: Path, gold_standard_path: Path | None = None, review_fraction: float = 0.15, ) -> None: """ Implement quality assurance workflows. Based on research: - Gold standard datasets (10-20% of items) verified by expert judges - Regular sampling and review (10-20% of work) by senior annotators - Real-time feedback to annotators on potential inconsistencies - Systematic error detection and correction workflows Research References: - Gold standard datasets: https://cleverx.com/blog/annotator-agreement-metrics-measuring-and-maintaining-annotation-quality-at-scale - Quality assurance: https://mindkosh.com/blog/tackling-data-annotation-quality-assurance-challenges/ - Error detection: https://tinkogroup.com/data-annotation-quality-control-guide/ """ logger.info("=" * 70) logger.info("IMPLEMENTING QUALITY ASSURANCE") logger.info("=" * 70) logger.info("") logger.info(f" Test set: {test_set_path}") logger.info(f" Review fraction: {review_fraction * 100}%") logger.info("") # Load test set if not test_set_path.exists(): logger.error(f"Test set not found: {test_set_path}") return with open(test_set_path) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data, dict) else data num_queries = len(queries) num_to_review = int(num_queries * review_fraction) logger.info(f" Total queries: {num_queries}") logger.info(f" Queries to review: {num_to_review}") logger.info("") logger.info(" Quality assurance steps:") logger.info(" 1. Create gold standard subset (10-20% of items)") logger.info(" 2. Sample queries for expert review") logger.info(" 3. Identify systematic errors") logger.info(" 4. Re-annotate low-agreement items") logger.info("") logger.info(" Quality assurance workflow ready") def create_comprehensive_evaluation_framework( output_dir: Path, games: list[str] = ["magic", "pokemon", "yugioh"], ) -> None: """ Create comprehensive evaluation framework for all tasks. Based on research: - Unified evaluation protocol across all tasks - Task-specific metrics (P@K, MRR, NDCG, etc.) - Confidence intervals for all metrics - Multi-task aggregation with weighted averages """ logger.info("=" * 70) logger.info("CREATING COMPREHENSIVE EVALUATION FRAMEWORK") logger.info("=" * 70) logger.info("") tasks = { "similarity": { "metrics": ["P@10", "MRR", "NDCG@10"], "weight": 1.0, }, "substitution": { "metrics": ["P@1", "P@5", "P@10", "avg_rank"], "weight": 1.0, }, "synergy": { "metrics": ["Precision@K", "Recall@K"], "weight": 0.5, }, "deck_completion": { "metrics": ["completion_rate", "suggestion_quality", "balance_impact"], "weight": 0.5, }, "contextual_discovery": { "metrics": ["Precision@K_per_category", "coverage", "diversity"], "weight": 0.5, }, "archetype": { "metrics": ["accuracy", "F1_score"], "weight": 0.25, }, } logger.info("Tasks and metrics:") for task_name, task_config in tasks.items(): logger.info(f" {task_name}:") logger.info(f" Metrics: {', '.join(task_config['metrics'])}") logger.info(f" Weight: {task_config['weight']}") logger.info("") logger.info(" Evaluation framework components:") logger.info(" 1. Unified evaluation protocol") logger.info(" 2. Task-specific metrics") logger.info(" 3. Confidence intervals (bootstrap)") logger.info(" 4. Multi-task aggregation") logger.info(" 5. Per-game and aggregate reporting") logger.info("") logger.info(" Comprehensive evaluation framework ready") def main() -> int: parser = argparse.ArgumentParser( description="Implement research-based improvements to annotation and evaluation" ) parser.add_argument( "--base-test-set", type=Path, default=Path("experiments/test_set_unified_magic.json"), help="Base test set path", ) parser.add_argument( "--output-dir", type=Path, default=Path("experiments/research_improvements"), help="Output directory", ) parser.add_argument( "--games", nargs="+", default=["magic", "pokemon", "yugioh"], help="Games to process", ) parser.add_argument( "--num-judges", type=int, default=5, help="Number of judges per item", ) parser.add_argument( "--min-agreement", type=float, default=0.65, help="Minimum inter-annotator agreement", ) parser.add_argument( "--review-fraction", type=float, default=0.15, help="Fraction of items to review for QA", ) args = parser.parse_args() args.output_dir.mkdir(parents=True, exist_ok=True) logger.info("=" * 70) logger.info("IMPLEMENTING RESEARCH-BASED IMPROVEMENTS") logger.info("=" * 70) logger.info("") # 1. Expand test sets for all tasks expand_test_sets_all_tasks( base_test_set=args.base_test_set, output_dir=args.output_dir, games=args.games, ) logger.info("") # 2. Enhance multi-judge labeling enhance_multi_judge_labeling( test_set_path=args.base_test_set, output_path=args.output_dir / "enhanced_labels.json", num_judges=args.num_judges, min_agreement=args.min_agreement, ) logger.info("") # 3. Implement quality assurance implement_quality_assurance( test_set_path=args.base_test_set, review_fraction=args.review_fraction, ) logger.info("") # 4. Create comprehensive evaluation framework create_comprehensive_evaluation_framework( output_dir=args.output_dir, games=args.games, ) logger.info("") logger.info("=" * 70) logger.info(" ALL RESEARCH-BASED IMPROVEMENTS IMPLEMENTED") logger.info("=" * 70) logger.info("") logger.info("Next steps:") logger.info(" 1. Run test set expansion for all tasks") logger.info(" 2. Run enhanced multi-judge labeling") logger.info(" 3. Run quality assurance workflows") logger.info(" 4. Run comprehensive evaluation across all tasks") logger.info("") return 0 if __name__ == "__main__": import sys sys.exit(main())
