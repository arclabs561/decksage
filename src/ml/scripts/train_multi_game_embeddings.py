#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas>=2.0.0", # "numpy<2.0.0", # "pecanpy>=2.0.0", # "gensim>=4.3.0", # ] # /// """ Train embeddings on all games simultaneously with game-aware representations. Multi-game training benefits: - Larger training corpus (more data) - Cross-game transfer learning (similar patterns across games) - Unified embedding space for cross-game similarity - Better generalization Key design decisions: 1. Game type as node attribute (MTG, YGO, PKM) 2. Game-aware random walks (can stay within game or cross-game) 3. Optional game-specific embeddings vs unified embeddings 4. Evaluation respects game boundaries (don't compare MTG to YGO directly) """ from __future__ import annotations import argparse import json import logging from pathlib import Path from typing import Any from ..utils.logging_config import setup_script_logging try: import pandas as pd import numpy as np from gensim.models import Word2Vec, KeyedVectors from pecanpy.pecanpy import SparseOTF HAS_DEPS = True except ImportError as e: HAS_DEPS = False print(f"Missing dependencies: {e}") logger = setup_script_logging() def infer_game_from_card_name(card_name: str, game_hints: dict[str, list[str]] | None = None) -> str: """Infer game type from card name or context. This is a heuristic - ideally we'd have explicit game metadata. For now, we can use: - Known card lists per game - Source metadata (if available) - Name patterns (less reliable) """ if game_hints: for game, cards in game_hints.items(): if card_name in cards: return game # Fallback: assume MTG if unknown (since it's the largest dataset) return "MTG" def load_multi_game_pairs( pairs_csvs: dict[str, Path], game_hints: dict[str, list[str]] | None = None, ) -> tuple[dict[str, set[str]], dict[tuple[str, str], int], dict[str, str]]: """Load pairs from multiple games and create unified graph. Returns: - adj: adjacency list {card: {neighbors}} - weights: edge weights {(card1, card2): count} - game_map: {card: game_type} """ logger.info("Loading multi-game pairs...") adj: dict[str, set[str]] = {} weights: dict[tuple[str, str], int] = {} game_map: dict[str, str] = {} for game, csv_path in pairs_csvs.items(): if not csv_path.exists(): logger.warning(f"Pairs file not found for {game}: {csv_path}") continue logger.info(f"Loading {game} pairs from {csv_path}...") df = pd.read_csv(csv_path) # Infer column names name_cols = [c for c in df.columns if "NAME" in c.upper() or "name" in c] if len(name_cols) < 2: logger.warning(f"Could not find name columns in {csv_path}") continue count_col = None for c in df.columns: if "COUNT" in c.upper() or "count" in c: count_col = c break card1_col, card2_col = name_cols[0], name_cols[1] for _, row in df.iterrows(): card1 = str(row[card1_col]).strip() card2 = str(row[card2_col]).strip() if not card1 or not card2 or card1 == card2: continue # Normalize pair (alphabetical order) if card1 > card2: card1, card2 = card2, card1 # Add to graph if card1 not in adj: adj[card1] = set() if card2 not in adj: adj[card2] = set() adj[card1].add(card2) adj[card2].add(card1) # Track game game_map[card1] = game game_map[card2] = game # Update weight pair_key = (card1, card2) count = int(row[count_col]) if count_col and pd.notna(row[count_col]) else 1 weights[pair_key] = weights.get(pair_key, 0) + count logger.info(f"Loaded {len(adj):,} cards from {len(pairs_csvs)} games") logger.info(f" Total edges: {len(weights):,}") # Game distribution game_counts = {} for card, game_type in game_map.items(): game_counts[game_type] = game_counts.get(game_type, 0) + 1 logger.info("Game distribution:") for game_type, count in sorted(game_counts.items()): logger.info(f" {game_type}: {count:,} cards") return adj, weights, game_map def create_game_aware_walks( adj: dict[str, set[str]], game_map: dict[str, str], walk_length: int = 80, num_walks: int = 10, cross_game_prob: float = 0.1, ) -> list[list[str]]: """Generate random walks with game awareness. Args: cross_game_prob: Probability of allowing cross-game transitions (0.0 = stay within game, 1.0 = fully mixed) """ logger.info(f"Generating game-aware walks (cross-game prob: {cross_game_prob})...") all_cards = list(adj.keys()) walks = [] for _ in range(num_walks): for start_card in all_cards: if start_card not in adj or not adj[start_card]: continue walk = [start_card] current = start_card current_game = game_map.get(current, "UNKNOWN") for _ in range(walk_length - 1): neighbors = list(adj[current]) if not neighbors: break # Filter neighbors by game (with cross-game probability) if np.random.random() < cross_game_prob: # Allow cross-game: use all neighbors candidates = neighbors else: # Stay within game: filter to same game candidates = [n for n in neighbors if game_map.get(n, "UNKNOWN") == current_game] if not candidates: # Fallback to all neighbors if no same-game neighbors candidates = neighbors # Random walk step current = np.random.choice(candidates) walk.append(current) walks.append(walk) logger.info(f"Generated {len(walks):,} walks") return walks def train_unified_embeddings( walks: list[list[str]], output_path: Path, dim: int = 128, window: int = 10, epochs: int = 10, **kwargs: Any, ) -> KeyedVectors: """Train unified embeddings on multi-game walks.""" logger.info(f"Training unified embeddings (dim={dim}, window={window}, epochs={epochs})...") # Train Word2Vec on walks model = Word2Vec( sentences=walks, vector_size=dim, window=window, min_count=1, workers=4, epochs=epochs, **kwargs, ) # Save model.wv.save(str(output_path)) logger.info(f" Saved embeddings to {output_path}") return model.wv def train_game_specific_embeddings( adj: dict[str, set[str]], game_map: dict[str, str], output_dir: Path, dim: int = 128, **kwargs: Any, ) -> dict[str, KeyedVectors]: """Train separate embeddings for each game.""" logger.info("Training game-specific embeddings...") games = set(game_map.values()) embeddings = {} for game in games: logger.info(f"Training {game} embeddings...") # Filter to game-specific subgraph game_cards = {c for c, g in game_map.items() if g == game} game_adj = {c: {n for n in adj.get(c, set()) if n in game_cards} for c in game_cards} # Generate walks (within game only) walks = create_game_aware_walks( game_adj, {c: game for c in game_cards}, cross_game_prob=0.0, # No cross-game **kwargs, ) # Train output_path = output_dir / f"{game.lower()}_embeddings.wv" wv = train_unified_embeddings(walks, output_path, dim=dim, **kwargs) embeddings[game] = wv return embeddings def main() -> int: """Train multi-game embeddings.""" parser = argparse.ArgumentParser(description="Train embeddings on all games") parser.add_argument("--input", type=str, help="Single multi-game pairs CSV (alternative to --mtg-pairs etc.)") parser.add_argument("--mtg-pairs", type=str, help="MTG pairs CSV") parser.add_argument("--ygo-pairs", type=str, help="Yu-Gi-Oh pairs CSV") parser.add_argument("--pkm-pairs", type=str, help="Pokemon pairs CSV") parser.add_argument("--output", type=str, required=True, help="Output embeddings path") parser.add_argument("--mode", choices=["unified", "game-specific", "both"], default="unified", help="Training mode") parser.add_argument("--cross-game-prob", type=float, default=0.1, help="Cross-game transition probability") parser.add_argument("--dim", type=int, default=128, help="Embedding dimension") parser.add_argument("--walk-length", type=int, default=80, help="Walk length") parser.add_argument("--num-walks", type=int, default=10, help="Number of walks per node") parser.add_argument("--epochs", type=int, default=10, help="Training epochs") args = parser.parse_args() if not HAS_DEPS: logger.error("Missing dependencies") return 1 # Collect pairs files pairs_csvs = {} # Support single multi-game CSV file (from S3 or local) if args.input: input_path = Path(args.input) # Handle S3 paths if str(input_path).startswith("s3://"): import subprocess import tempfile # Download from S3 with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp: local_path = Path(tmp.name) logger.info(f"Downloading {args.input} from S3...") subprocess.run(["aws", "s3", "cp", args.input, str(local_path)], check=True) input_path = local_path if input_path.exists(): # Load and split by game if possible, or treat as unified logger.info(f"Loading multi-game pairs from {input_path}...") # For now, treat as unified MTG (can be enhanced to detect games) pairs_csvs["MULTI"] = input_path else: logger.warning(f"Input file not found: {input_path}") # Also support per-game files if args.mtg_pairs: mtg_path = Path(args.mtg_pairs) if str(mtg_path).startswith("s3://"): import subprocess import tempfile with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp: local_path = Path(tmp.name) subprocess.run(["aws", "s3", "cp", args.mtg_pairs, str(local_path)], check=True) mtg_path = local_path if mtg_path.exists(): pairs_csvs["MTG"] = mtg_path if args.ygo_pairs: ygo_path = Path(args.ygo_pairs) if str(ygo_path).startswith("s3://"): import subprocess import tempfile with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp: local_path = Path(tmp.name) subprocess.run(["aws", "s3", "cp", args.ygo_pairs, str(local_path)], check=True) ygo_path = local_path if ygo_path.exists(): pairs_csvs["YGO"] = ygo_path if args.pkm_pairs: pkm_path = Path(args.pkm_pairs) if str(pkm_path).startswith("s3://"): import subprocess import tempfile with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp: local_path = Path(tmp.name) subprocess.run(["aws", "s3", "cp", args.pkm_pairs, str(local_path)], check=True) pkm_path = local_path if pkm_path.exists(): pairs_csvs["PKM"] = pkm_path if not pairs_csvs: logger.error("No pairs files provided or found") return 1 # Load multi-game graph adj, weights, game_map = load_multi_game_pairs(pairs_csvs) if not adj: logger.error("No graph data loaded") return 1 output_path = Path(args.output) output_path.parent.mkdir(parents=True, exist_ok=True) # Train based on mode if args.mode in ["unified", "both"]: logger.info("=" * 70) logger.info("Training unified embeddings") logger.info("=" * 70) walks = create_game_aware_walks( adj, game_map, walk_length=args.walk_length, num_walks=args.num_walks, cross_game_prob=args.cross_game_prob, ) train_unified_embeddings( walks, output_path, dim=args.dim, epochs=args.epochs, ) if args.mode in ["game-specific", "both"]: logger.info("=" * 70) logger.info("Training game-specific embeddings") logger.info("=" * 70) game_dir = output_path.parent / "game_specific" game_dir.mkdir(exist_ok=True) train_game_specific_embeddings( adj, game_map, game_dir, dim=args.dim, walk_length=args.walk_length, num_walks=args.num_walks, epochs=args.epochs, ) # Save metadata metadata = { "games": list(pairs_csvs.keys()), "total_cards": len(adj), "total_edges": len(weights), "game_distribution": {g: sum(1 for c in game_map.values() if c == g) for g in set(game_map.values())}, "mode": args.mode, "cross_game_prob": args.cross_game_prob, "dim": args.dim, } metadata_path = output_path.parent / f"{output_path.stem}_metadata.json" with open(metadata_path, "w") as f: json.dump(metadata, f, indent=2) logger.info(f" Saved metadata to {metadata_path}") logger.info("=" * 70) logger.info("Multi-game training complete!") logger.info("=" * 70) return 0 if __name__ == "__main__": import sys sys.exit(main())
