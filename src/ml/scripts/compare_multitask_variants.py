#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pandas", # "numpy", # "gensim", # ] # /// """ Compare multiple multi-task embedding variants. Evaluates embeddings trained with different substitution weights and creates comprehensive comparison report. """ from __future__ import annotations import argparse import json from pathlib import Path from typing import Any try: import pandas as pd import numpy as np from gensim.models import KeyedVectors HAS_DEPS = True except ImportError: HAS_DEPS = False def evaluate_embedding( embedding_path: Path, pairs_path: Path, test_set_path: Path, name: str, ) -> dict[str, Any]: """Evaluate a single embedding.""" if not embedding_path.exists(): return {"name": name, "status": "missing"} try: embedding = KeyedVectors.load(str(embedding_path)) except Exception as e: return {"name": name, "status": "error", "error": str(e)} # Import evaluation functions from ml.utils.evaluation import ( compute_precision_at_k, compute_recall_at_k, compute_map, ) results = { "name": name, "status": "success", "vocab_size": len(embedding), } # Co-occurrence task if pairs_path.exists(): pairs_df = pd.read_csv(pairs_path, nrows=50000) cooccurrence_data = {} for _, row in pairs_df.iterrows(): n1, n2 = row.get("NAME_1", ""), row.get("NAME_2", "") if n1 and n2: if n1 not in cooccurrence_data: cooccurrence_data[n1] = set() cooccurrence_data[n1].add(n2) if n2 not in cooccurrence_data: cooccurrence_data[n2] = set() cooccurrence_data[n2].add(n1) valid_queries = [q for q in cooccurrence_data.keys() if q in embedding and len(cooccurrence_data[q]) >= 3][:50] scores = [] for query in valid_queries: cooccurring = list(cooccurrence_data[query])[:20] labels = { "highly_relevant": cooccurring[:10], "relevant": cooccurring[10:20] if len(cooccurring) > 20 else [], } try: similar = embedding.most_similar(query, topn=10) predictions = [card for card, _ in similar] p_at_k = compute_precision_at_k(predictions, labels, k=10) scores.append(p_at_k) except Exception: continue results["cooccurrence"] = { "p@10": float(np.mean(scores)) if scores else 0.0, "num_evaluated": len(scores), } # Functional similarity task if test_set_path.exists(): with open(test_set_path) as f: test_data = json.load(f) functional_test = test_data.get("queries", test_data) scores = [] recalls = [] maps = [] for query, labels in functional_test.items(): if query not in embedding: continue try: similar = embedding.most_similar(query, topn=10) predictions = [card for card, _ in similar] p_at_k = compute_precision_at_k(predictions, labels, k=10) r_at_k = compute_recall_at_k(predictions, labels, k=10) map_at_k = compute_map(predictions, labels, k=10) scores.append(p_at_k) recalls.append(r_at_k) maps.append(map_at_k) except Exception: continue results["functional_similarity"] = { "p@10": float(np.mean(scores)) if scores else 0.0, "r@10": float(np.mean(recalls)) if recalls else 0.0, "map@10": float(np.mean(maps)) if maps else 0.0, "num_evaluated": len(scores), } return results def main() -> int: """Compare multiple multi-task variants.""" parser = argparse.ArgumentParser(description="Compare multi-task embedding variants") parser.add_argument("--pairs", type=Path, required=True, help="Pairs CSV") parser.add_argument("--test-set", type=Path, required=True, help="Test set JSON") parser.add_argument("--output", type=Path, required=True, help="Output comparison JSON") parser.add_argument("--embeddings", nargs="+", help="Embedding files to compare") args = parser.parse_args() if not HAS_DEPS: print("Error: pandas, numpy, gensim required") return 1 # Default embeddings to compare if not args.embeddings: embeddings_to_compare = [ ("baseline", Path("data/embeddings/node2vec_default.wv")), ("multitask_sub2", Path("data/embeddings/multitask_sub2.wv")), ("multitask_sub5", Path("data/embeddings/multitask_sub5.wv")), ("multitask_sub10", Path("data/embeddings/multitask_sub10.wv")), ] else: embeddings_to_compare = [ (Path(e).stem, Path(e)) for e in args.embeddings ] print("Comparing multi-task embedding variants...") print("=" * 70) all_results = [] for name, embedding_path in embeddings_to_compare: print(f"\nEvaluating {name}...") result = evaluate_embedding( embedding_path, args.pairs, args.test_set, name, ) all_results.append(result) if result.get("status") == "success": co = result.get("cooccurrence", {}) func = result.get("functional_similarity", {}) print(f" Co-occurrence P@10: {co.get('p@10', 0):.4f}") print(f" Functional P@10: {func.get('p@10', 0):.4f}") else: print(f" Status: {result.get('status')}") # Create comparison summary comparison = { "timestamp": __import__("datetime").datetime.now().isoformat(), "variants": all_results, "summary": {}, } # Find best for each task successful = [r for r in all_results if r.get("status") == "success"] if successful: best_co = max(successful, key=lambda x: x.get("cooccurrence", {}).get("p@10", 0)) best_func = max(successful, key=lambda x: x.get("functional_similarity", {}).get("p@10", 0)) comparison["summary"] = { "best_cooccurrence": best_co["name"], "best_functional": best_func["name"], "num_variants": len(successful), } # Save args.output.parent.mkdir(parents=True, exist_ok=True) with open(args.output, "w") as f: json.dump(comparison, f, indent=2) print(f"\n Comparison saved to {args.output}") # Print summary table print("\n" + "=" * 70) print("Comparison Summary") print("=" * 70) print(f"{'Variant':<20} {'Co-occ P@10':<15} {'Func P@10':<15} {'Status':<10}") print("-" * 70) for result in all_results: name = result["name"] status = result.get("status", "unknown") co_p10 = result.get("cooccurrence", {}).get("p@10", 0) func_p10 = result.get("functional_similarity", {}).get("p@10", 0) print(f"{name:<20} {co_p10:<15.4f} {func_p10:<15.4f} {status:<10}") return 0 if __name__ == "__main__": exit(main())