#!/usr/bin/env python3 # /// script # requires-python = ">=3.11" # dependencies = [ # "pydantic-ai>=0.0.12", # ] # /// """ Generate labels using multiple LLM judges for IAA tracking. Uses 3-5 LLM judges per query and computes inter-annotator agreement. Research Basis: - Multi-annotator consensus improves annotation quality - Inter-annotator agreement â‰¥ 0.65-0.75 for recommendation systems - Weighted consensus (Dawid-Skene) better than simple majority voting - Accept disagreement as data when it reflects genuine subjectivity References: - Inter-annotator agreement: https://www.innovatiana.com/en/post/inter-annotator-agreement - Multi-annotator validation: https://mindkosh.com/blog/multi-annotator-validation-enhancing-label-accuracy-through-consensus - Krippendorff's Alpha: https://labelstud.io/blog/how-to-use-krippendorff-s-alpha-to-measure-annotation-agreement - Building trustworthy datasets: https://keymakr.com/blog/measuring-inter-annotator-agreement-building-trustworthy-datasets/ - Data annotation best practices: https://www.atltranslate.com/ai/blog/labeling-data-best-practices """ from __future__ import annotations import argparse import json import logging from collections import Counter from pathlib import Path from typing import Any try: from pydantic_ai import Agent HAS_PYDANTIC_AI = True except ImportError: HAS_PYDANTIC_AI = False logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) # Import from existing labeling script import sys from pathlib import Path as P script_dir = P(__file__).parent if str(script_dir) not in sys.path: sys.path.insert(0, str(script_dir)) # Try enhanced version first, fall back to standard try: from generate_labels_enhanced import ( EnhancedCardLabels as CardLabels, make_enhanced_label_agent as make_label_generation_agent, generate_labels_with_context, load_card_context, ) HAS_ENHANCED = True HAS_LABELING_SCRIPTS = True def generate_labels_for_query_with_retry(agent, query, use_case=None, game=None, max_retries=3): """Wrapper for enhanced generation.""" # Always load card context (with game if available) card_context = load_card_context(query, game=game) return generate_labels_with_context(agent, query, use_case, card_context, game=game) except ImportError: HAS_ENHANCED = False try: from generate_labels_for_new_queries_optimized import ( CardLabels, make_label_generation_agent, generate_labels_for_query_with_retry, ) HAS_LABELING_SCRIPTS = True except ImportError as e: HAS_LABELING_SCRIPTS = False logger.warning(f"Could not import labeling scripts: {e}") # Try to use LLM cache if available try: import sys from pathlib import Path as P # Add src to path for ml imports script_dir = P(__file__).parent src_dir = script_dir.parent.parent if str(src_dir) not in sys.path: sys.path.insert(0, str(src_dir)) from ml.utils.llm_cache import LLMCache, load_config HAS_LLM_CACHE = True _llm_cache = LLMCache(load_config(), scope="labeling") except ImportError: HAS_LLM_CACHE = False _llm_cache = None # Try to use cache invalidation strategy try: from ml.utils.cache_invalidation import CacheInvalidationStrategy, get_prompt_version_hash from ml.evaluation.expanded_judge_criteria import get_prompt_version HAS_CACHE_INVALIDATION = True # Get current prompt version _current_prompt_version = get_prompt_version() _cache_strategy = CacheInvalidationStrategy( prompt_version=_current_prompt_version, max_age_days=30, ) except ImportError: HAS_CACHE_INVALIDATION = False _cache_strategy = None _current_prompt_version = None def generate_labels_multi_judge( query: str, num_judges: int = 3, use_case: str | None = None, game: str | None = None, ) -> dict[str, Any]: """ Generate labels using multiple judges and compute agreement. Returns: Dict with labels (majority vote) and IAA metrics """ if not HAS_PYDANTIC_AI: logger.error("pydantic-ai required") return {} if not HAS_LABELING_SCRIPTS: logger.error("Labeling scripts not available") return {} # Generate labels from multiple judges all_judgments = [] for judge_id in range(num_judges): agent = make_label_generation_agent() if not agent: logger.warning(f"Could not create agent for judge {judge_id}") continue # Try cache first (if available) if HAS_LLM_CACHE and _llm_cache: # Use cache invalidation strategy if available if HAS_CACHE_INVALIDATION and _cache_strategy: cache_key = _cache_strategy.get_cache_key( query=query, use_case=use_case, game=game, judge_id=judge_id, ) else: # Fallback to simple cache key cache_key = f"label_{query}_{use_case}_{game}_{judge_id}" cached = _llm_cache.get(cache_key) # Check if cached entry should be invalidated (if strategy available) if cached and HAS_CACHE_INVALIDATION and _cache_strategy: # Handle both dict and non-dict cached values cache_entry = cached if isinstance(cached, dict) else {"data": cached} if _cache_strategy.should_invalidate( cache_entry, current_prompt_version=_current_prompt_version, ): logger.debug(f" Cache entry invalidated for judge {judge_id} (prompt version mismatch)") cached = None if cached: # Extract data from annotated cache entry or use directly if isinstance(cached, dict) and "data" in cached: labels = cached["data"] else: labels = cached logger.debug(f" Using cached labels for judge {judge_id}") else: labels = generate_labels_for_query_with_retry(agent, query, use_case, game=game) # Cache the result with version metadata if labels: if HAS_CACHE_INVALIDATION and _cache_strategy: # Annotate with version metadata annotated = _cache_strategy.annotate_cache_entry( {"data": labels}, prompt_version=_current_prompt_version, ) _llm_cache.set(cache_key, annotated) else: _llm_cache.set(cache_key, labels) else: labels = generate_labels_for_query_with_retry(agent, query, use_case, game=game) if labels and any(labels.values()): all_judgments.append(labels) if not all_judgments: logger.warning(f"No valid judgments for {query}") return { "highly_relevant": [], "relevant": [], "somewhat_relevant": [], "marginally_relevant": [], "irrelevant": [], "iaa": { "num_judges": 0, "agreement_rate": 0.0, }, } # First, validate each judgment for internal contradictions (same card in multiple levels) # This is a quality check - a single judge shouldn't contradict themselves validated_judgments = [] for i, judgment in enumerate(all_judgments): # Check for contradictions within a single judgment card_to_levels = {} # card -> set of levels it appears in for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant", "irrelevant"]: for card in judgment.get(level, []): if card not in card_to_levels: card_to_levels[card] = set() card_to_levels[card].add(level) # Fix contradictions: keep card in highest relevance level only cleaned_judgment = { "highly_relevant": [], "relevant": [], "somewhat_relevant": [], "marginally_relevant": [], "irrelevant": [], } level_priority = { "highly_relevant": 4, "relevant": 3, "somewhat_relevant": 2, "marginally_relevant": 1, "irrelevant": 0, } for card, levels in card_to_levels.items(): if len(levels) > 1: # Contradiction detected - keep highest level best_level = max(levels, key=lambda l: level_priority[l]) logger.warning(f"Judge {i} contradiction: {card} in multiple levels {levels}, keeping {best_level}") cleaned_judgment[best_level].append(card) else: # No contradiction cleaned_judgment[list(levels)[0]].append(card) validated_judgments.append(cleaned_judgment) # Majority vote: card appears in >= 50% of judgments # Note: Judges may disagree (that's expected and measured by IAA) # But each judge should be internally consistent (no contradictions) card_votes: dict[str, dict[str, int]] = {} # card -> {level: count} for judgment in validated_judgments: for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"]: for card in judgment.get(level, []): if card not in card_votes: card_votes[card] = {} card_votes[card][level] = card_votes[card].get(level, 0) + 1 # Assign cards to levels based on majority vote # If judges disagree, use the level with most votes (majority wins) final_labels = { "highly_relevant": [], "relevant": [], "somewhat_relevant": [], "marginally_relevant": [], "irrelevant": [], } threshold = len(validated_judgments) / 2 # Majority for card, votes in card_votes.items(): # Find level with most votes if votes: best_level, vote_count = max(votes.items(), key=lambda x: x[1]) if vote_count >= threshold: final_labels[best_level].append(card) # If no majority, card is excluded (judges disagreed too much) # Validate and filter cross-game contamination (if game is known) # This should be integrated, not post-processing validated_labels = final_labels.copy() # Only use heuristic if game not explicitly provided # In production, game should always be passed as parameter if not game: try: # Check if we can infer game from query name patterns query_lower = query.lower() if any(x in query_lower for x in ["energy", "pokemon", "pikachu", "charizard"]): game = "pokemon" elif any(x in query_lower for x in ["blue-eyes", "dark magician", "exodia", "pot of"]): game = "yugioh" else: game = "magic" # Default except Exception: game = "magic" # Default fallback # Filter cross-game cards if game is known and card database available # NOTE: This happens after majority vote, which is suboptimal but necessary # because we need to collect all judgments first. In future, could filter # during judgment collection, but that would require game to be known upfront. if game: try: from ml.data.card_database import get_card_database card_db = get_card_database() total_filtered = 0 for level in ["highly_relevant", "relevant", "somewhat_relevant", "marginally_relevant"]: cards = validated_labels[level] valid_cards, invalid_cards = card_db.filter_cards_by_game(cards, game) if invalid_cards: total_filtered += len(invalid_cards) logger.warning(f"Filtered {len(invalid_cards)} cross-game cards from {level} for {query}: {invalid_cards[:3]}") validated_labels[level] = valid_cards if total_filtered > 0: logger.info(f"Total cross-game cards filtered for {query}: {total_filtered}") except ImportError: # Card database not available, skip filtering logger.warning("Card database not available, skipping cross-game filtering") except Exception as e: logger.warning(f"Could not filter cross-game cards: {e}") # Compute agreement metrics # Try to use Krippendorff's Alpha if available, fall back to simple agreement try: from ml.evaluation.krippendorff_alpha import compute_iaa_for_labels iaa_metrics = compute_iaa_for_labels(validated_judgments) except ImportError: # Fall back to simple agreement calculation agreement_scores = [] for card, votes in card_votes.items(): total_votes = sum(votes.values()) max_votes = max(votes.values()) if votes else 0 agreement = max_votes / len(validated_judgments) if validated_judgments else 0.0 agreement_scores.append(agreement) avg_agreement = sum(agreement_scores) / len(agreement_scores) if agreement_scores else 0.0 iaa_metrics = { "num_judges": len(validated_judgments), "agreement_rate": avg_agreement, "num_cards": len(card_votes), "krippendorff_alpha": None, # Not available } return { **validated_labels, "iaa": iaa_metrics, } def main() -> int: """Generate labels with multi-judge IAA tracking.""" parser = argparse.ArgumentParser(description="Generate labels with multi-judge IAA") parser.add_argument("--input", type=str, required=True, help="Input test set JSON") parser.add_argument("--output", type=str, required=True, help="Output test set JSON") parser.add_argument("--num-judges", type=int, default=3, help="Number of judges per query") parser.add_argument("--batch-size", type=int, default=5, help="Batch size") args = parser.parse_args() if not HAS_PYDANTIC_AI: logger.error("pydantic-ai required: pip install pydantic-ai") return 1 # Load test set input_path = Path(args.input) if not input_path.exists(): logger.error(f"Input not found: {input_path}") return 1 with open(input_path) as f: data = json.load(f) queries = data.get("queries", data) if isinstance(data, dict) else data # Find queries needing labels queries_needing_labels = [] for query_name, query_data in queries.items(): if isinstance(query_data, dict): has_labels = ( query_data.get("highly_relevant") or query_data.get("relevant") or query_data.get("somewhat_relevant") ) if not has_labels: queries_needing_labels.append((query_name, query_data)) logger.info(f"Found {len(queries_needing_labels)} queries needing labels") if not queries_needing_labels: logger.info(" All queries already have labels!") return 0 # Generate labels with multi-judge updated = queries.copy() if isinstance(queries, dict) else {q: d for q, d in queries.items()} processed = 0 for query_name, query_data in queries_needing_labels: use_case = query_data.get("use_case") logger.info(f"Generating labels for {query_name} ({args.num_judges} judges)...") result = generate_labels_multi_judge(query_name, args.num_judges, use_case) # Merge with existing data updated[query_name] = { **query_data, **{k: v for k, v in result.items() if k != "iaa"}, "iaa": result.get("iaa", {}), } processed += 1 iaa = result.get("iaa", {}) logger.info(f" Generated labels (IAA: {iaa.get('agreement_rate', 0.0):.2f}, {iaa.get('num_judges', 0)} judges)") # Save output_path = Path(args.output) output_path.parent.mkdir(parents=True, exist_ok=True) output_data = { "version": "multi_judge", "queries": updated, } with open(output_path, "w") as f: json.dump(output_data, f, indent=2) logger.info(f" Generated labels for {processed} queries") logger.info(f" Saved to {output_path}") return 0 if __name__ == "__main__": import sys sys.exit(main())
