#!/usr/bin/env python3 """ Test Set Validation and Confidence Intervals Validates test set quality and computes confidence intervals for metrics. Part of T0.1 foundation refinement. """ from __future__ import annotations import json import math from dataclasses import dataclass from pathlib import Path from typing import Any import numpy as np from scipy import stats @dataclass class TestSetStats: """Statistics about a test set.""" total_queries: int queries_with_labels: int avg_labels_per_query: float highly_relevant_count: int relevant_count: int somewhat_relevant_count: int marginally_relevant_count: int irrelevant_count: int coverage_by_game: dict[str, int] coverage_by_format: dict[str, int] coverage_by_archetype: dict[str, int] @dataclass class ConfidenceInterval: """Confidence interval for a metric.""" metric: str value: float lower: float upper: float confidence: float # e.g., 0.95 for 95% CI n: int # Sample size def load_test_set(path: Path) -> dict[str, Any]: """ Load test set from JSON file. Raises: FileNotFoundError: If file doesn't exist json.JSONDecodeError: If file is not valid JSON ValueError: If test set is empty or invalid """ if not path.exists(): raise FileNotFoundError(f"Test set not found: {path}") if path.stat().st_size == 0: raise ValueError(f"Test set file is empty: {path}") with open(path) as f: data = json.load(f) # Validate structure if not isinstance(data, dict): raise ValueError(f"Test set must be a dict, got {type(data)}") # Handle both formats: {"queries": {...}} and direct queries dict queries = data.get("queries", data) if not queries or (isinstance(queries, dict) and len(queries) == 0): raise ValueError(f"Test set is empty: {path}") return queries if isinstance(queries, dict) else data def compute_test_set_stats(test_set: dict[str, Any]) -> TestSetStats: """Compute statistics about test set coverage.""" total = len(test_set) queries_with_labels = 0 label_counts = { "highly_relevant": 0, "relevant": 0, "somewhat_relevant": 0, "marginally_relevant": 0, "irrelevant": 0, } total_labels = 0 coverage_by_game: dict[str, int] = {} coverage_by_format: dict[str, int] = {} coverage_by_archetype: dict[str, int] = {} for query_name, query_data in test_set.items(): if isinstance(query_data, dict): # Count labels has_labels = False for label_type in label_counts.keys(): labels = query_data.get(label_type, []) if isinstance(labels, list): count = len(labels) label_counts[label_type] += count total_labels += count if count > 0: has_labels = True if has_labels: queries_with_labels += 1 # Track coverage game = query_data.get("game") or query_data.get("type", "unknown") coverage_by_game[game] = coverage_by_game.get(game, 0) + 1 fmt = query_data.get("format") if fmt: coverage_by_format[fmt] = coverage_by_format.get(fmt, 0) + 1 archetype = query_data.get("archetype") if archetype: coverage_by_archetype[archetype] = coverage_by_archetype.get(archetype, 0) + 1 avg_labels = total_labels / total if total > 0 else 0.0 return TestSetStats( total_queries=total, queries_with_labels=queries_with_labels, avg_labels_per_query=avg_labels, highly_relevant_count=label_counts["highly_relevant"], relevant_count=label_counts["relevant"], somewhat_relevant_count=label_counts["somewhat_relevant"], marginally_relevant_count=label_counts["marginally_relevant"], irrelevant_count=label_counts["irrelevant"], coverage_by_game=coverage_by_game, coverage_by_format=coverage_by_format, coverage_by_archetype=coverage_by_archetype, ) def bootstrap_confidence_interval( values: list[float], confidence: float = 0.95, n_bootstrap: int = 1000, ) -> ConfidenceInterval: """ Compute confidence interval using bootstrap method. Args: values: List of metric values (e.g., P@10 for each query) confidence: Confidence level (default 0.95 for 95% CI) n_bootstrap: Number of bootstrap samples Returns: ConfidenceInterval with lower and upper bounds """ if not values: return ConfidenceInterval( metric="unknown", value=0.0, lower=0.0, upper=0.0, confidence=confidence, n=0, ) values_array = np.array(values) mean_value = np.mean(values_array) n = len(values) # Bootstrap sampling bootstrap_means = [] for _ in range(n_bootstrap): sample = np.random.choice(values_array, size=n, replace=True) bootstrap_means.append(np.mean(sample)) # Compute percentiles alpha = 1 - confidence lower = np.percentile(bootstrap_means, 100 * alpha / 2) upper = np.percentile(bootstrap_means, 100 * (1 - alpha / 2)) return ConfidenceInterval( metric="bootstrap_mean", value=mean_value, lower=float(lower), upper=float(upper), confidence=confidence, n=n, ) def normal_confidence_interval( values: list[float], confidence: float = 0.95, ) -> ConfidenceInterval: """ Compute confidence interval using normal approximation. Assumes values are normally distributed (reasonable for large n). """ if not values: return ConfidenceInterval( metric="unknown", value=0.0, lower=0.0, upper=0.0, confidence=confidence, n=0, ) values_array = np.array(values) mean_value = np.mean(values_array) std_value = np.std(values_array, ddof=1) # Sample std n = len(values) # t-distribution for small samples, normal for large if n < 30: t_critical = stats.t.ppf((1 + confidence) / 2, df=n - 1) margin = t_critical * std_value / math.sqrt(n) else: z_critical = stats.norm.ppf((1 + confidence) / 2) margin = z_critical * std_value / math.sqrt(n) return ConfidenceInterval( metric="normal_approximation", value=float(mean_value), lower=float(mean_value - margin), upper=float(mean_value + margin), confidence=confidence, n=n, ) def compute_metric_confidence_intervals( per_query_metrics: dict[str, dict[str, float]], confidence: float = 0.95, ) -> dict[str, ConfidenceInterval]: """ Compute confidence intervals for metrics across queries. Args: per_query_metrics: Dict mapping query -> metric_name -> value confidence: Confidence level Returns: Dict mapping metric_name -> ConfidenceInterval """ # Collect all metric names all_metrics = set() for query_metrics in per_query_metrics.values(): all_metrics.update(query_metrics.keys()) intervals = {} for metric_name in all_metrics: values = [ query_metrics[metric_name] for query_metrics in per_query_metrics.values() if metric_name in query_metrics ] if values: # Use bootstrap for robustness ci = bootstrap_confidence_interval(values, confidence=confidence) ci.metric = metric_name intervals[metric_name] = ci return intervals def validate_test_set_coverage( test_set_path: Path, min_queries: int = 100, min_labels_per_query: int = 5, ) -> dict[str, Any]: """ Validate test set meets minimum coverage requirements. Returns dict with validation results and recommendations. """ test_set = load_test_set(test_set_path) stats = compute_test_set_stats(test_set) issues = [] recommendations = [] # Check total queries if stats.total_queries < min_queries: issues.append( f"Insufficient queries: {stats.total_queries} < {min_queries} (minimum)" ) recommendations.append( f"Expand test set to at least {min_queries} queries " f"({min_queries - stats.total_queries} more needed)" ) # Check label coverage if stats.queries_with_labels < stats.total_queries: missing = stats.total_queries - stats.queries_with_labels issues.append(f"{missing} queries lack labels") recommendations.append("Add labels for all queries") # Check average labels per query if stats.avg_labels_per_query < min_labels_per_query: issues.append( f"Low label density: {stats.avg_labels_per_query:.1f} labels/query " f"< {min_labels_per_query} (minimum)" ) recommendations.append( f"Add more labels per query (target: {min_labels_per_query}+)" ) # Check label distribution total_labels = ( stats.highly_relevant_count + stats.relevant_count + stats.somewhat_relevant_count + stats.marginally_relevant_count + stats.irrelevant_count ) if total_labels > 0: highly_relevant_pct = stats.highly_relevant_count / total_labels * 100 if highly_relevant_pct < 20: issues.append( f"Low highly_relevant ratio: {highly_relevant_pct:.1f}% " "(should be 20%+)" ) recommendations.append("Add more highly_relevant labels") # Check format coverage if len(stats.coverage_by_format) < 3: issues.append( f"Limited format coverage: {len(stats.coverage_by_format)} formats " "(should cover 3+ formats)" ) recommendations.append("Add queries across multiple formats") return { "valid": len(issues) == 0, "stats": { "total_queries": stats.total_queries, "queries_with_labels": stats.queries_with_labels, "avg_labels_per_query": stats.avg_labels_per_query, "label_distribution": { "highly_relevant": stats.highly_relevant_count, "relevant": stats.relevant_count, "somewhat_relevant": stats.somewhat_relevant_count, "marginally_relevant": stats.marginally_relevant_count, "irrelevant": stats.irrelevant_count, }, "coverage": { "games": stats.coverage_by_game, "formats": stats.coverage_by_format, "archetypes": stats.coverage_by_archetype, }, }, "issues": issues, "recommendations": recommendations, } def main() -> int: """CLI for test set validation.""" import argparse parser = argparse.ArgumentParser(description="Validate test set coverage") parser.add_argument( "--test-set", type=Path, required=True, help="Path to test set JSON file", ) parser.add_argument( "--min-queries", type=int, default=100, help="Minimum number of queries required", ) parser.add_argument( "--min-labels", type=int, default=5, help="Minimum labels per query", ) parser.add_argument( "--output", type=Path, help="Output JSON file for validation results", ) args = parser.parse_args() if not args.test_set.exists(): print(f"Error: Test set not found: {args.test_set}") return 1 results = validate_test_set_coverage( args.test_set, min_queries=args.min_queries, min_labels_per_query=args.min_labels, ) # Print results print(f"\nTest Set Validation: {args.test_set.name}") print("=" * 60) print(f"Total queries: {results['stats']['total_queries']}") print(f"Queries with labels: {results['stats']['queries_with_labels']}") print( f"Avg labels/query: {results['stats']['avg_labels_per_query']:.1f}" ) print(f"\nStatus: {' VALID' if results['valid'] else 'Error: ISSUES FOUND'}") if results["issues"]: print("\nIssues:") for issue in results["issues"]: print(f" - {issue}") if results["recommendations"]: print("\nRecommendations:") for rec in results["recommendations"]: print(f" - {rec}") # Save results if args.output: with open(args.output, "w") as f: json.dump(results, f, indent=2) print(f"\nResults saved to: {args.output}") return 0 if results["valid"] else 1 if __name__ == "__main__": import sys sys.exit(main())