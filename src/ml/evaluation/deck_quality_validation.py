#!/usr/bin/env python3 """ Deck Quality Validation Validates that deck completion produces high-quality decks. Part of T0.2 foundation refinement. """ from __future__ import annotations import json import logging from dataclasses import dataclass, asdict from pathlib import Path from typing import Any, Callable, Optional from ..deck_building.deck_completion import CompletionConfig, greedy_complete from ..deck_building.deck_quality import ( DeckQualityMetrics, assess_deck_quality, ) from ..utils.data_loading import load_decks_jsonl from ..utils.paths import PATHS logger = logging.getLogger(__name__) @dataclass class CompletionTestCase: """Test case for deck completion.""" name: str incomplete_deck: dict[str, Any] game: str archetype: Optional[str] = None format: Optional[str] = None target_size: Optional[int] = None expected_quality_min: float = 6.0 # Minimum quality score (0-10) @dataclass class CompletionResult: """Result of deck completion test.""" test_case: CompletionTestCase completed_deck: dict[str, Any] quality_metrics: DeckQualityMetrics success: bool error: Optional[str] = None def load_reference_decks( game: str, archetype: Optional[str] = None, format: Optional[str] = None, limit: int = 50, ) -> list[dict[str, Any]]: """Load reference tournament decks for quality comparison.""" decks = load_decks_jsonl( jsonl_path=PATHS.decks_with_metadata, formats=[format] if format else None, ) # Filter by game game_decks = [ d for d in decks if d.get("game") == game or (game == "magic" and d.get("game") is None) ] # Filter by archetype if specified if archetype: game_decks = [ d for d in game_decks if d.get("archetype", "").lower() == archetype.lower() ] return game_decks[:limit] def create_test_cases( game: str = "magic", num_cases: int = 10, ) -> list[CompletionTestCase]: """ Create test cases for deck completion. Uses incomplete tournament decks (missing 10-20 cards). """ decks = load_reference_decks(game, limit=num_cases * 2) if not decks: logger.warning(f"No reference decks found for {game}") return [] test_cases = [] for i, deck in enumerate(decks[:num_cases]): # Create incomplete deck by removing 10-20 cards incomplete = json.loads(json.dumps(deck)) # Deep copy # Remove cards from main partition partition_name = "Main" if game == "magic" else "Main Deck" for partition in incomplete.get("partitions", []): if partition.get("name") == partition_name: cards = partition.get("cards", []) if len(cards) > 20: # Remove 10-20 cards (keep at least 20) remove_count = min(15, len(cards) - 20) partition["cards"] = cards[:-remove_count] break test_cases.append( CompletionTestCase( name=f"{game}_test_{i+1}", incomplete_deck=incomplete, game=game, archetype=deck.get("archetype"), format=deck.get("format"), target_size=None, # Use validator default expected_quality_min=6.0, ) ) return test_cases def run_completion_test( test_case: CompletionTestCase, similarity_fn: Callable[[str, int], list[tuple[str, float]]], tag_set_fn: Optional[Callable[[str], set[str]]] = None, cmc_fn: Optional[Callable[[str], Optional[int]]] = None, price_fn: Optional[Callable[[str], Optional[float]]] = None, ) -> CompletionResult: """ Run a single deck completion test. Args: test_case: Test case to run similarity_fn: Function (query, k) -> [(card, score), ...] tag_set_fn: Function to get functional tags for a card cmc_fn: Function to get CMC for a card price_fn: Function to get price for a card Returns: CompletionResult with quality metrics """ try: # Run completion config = CompletionConfig( game=test_case.game, target_main_size=test_case.target_size, method="fusion", # Use fusion for best results ) completed = greedy_complete( deck=test_case.incomplete_deck, similarity_fn=similarity_fn, config=config, tag_set_fn=tag_set_fn, price_fn=price_fn, ) # Load reference decks for quality assessment reference_decks = load_reference_decks( game=test_case.game, archetype=test_case.archetype, format=test_case.format, limit=20, ) # Assess quality if tag_set_fn and cmc_fn: quality = assess_deck_quality( deck=completed, game=test_case.game, tag_set_fn=tag_set_fn, cmc_fn=cmc_fn, reference_decks=reference_decks if reference_decks else None, archetype=test_case.archetype, ) else: # Create minimal quality metrics if tag_set_fn/cmc_fn not available from ..deck_building.deck_quality import DeckQualityMetrics quality = DeckQualityMetrics( mana_curve_score=0.5, tag_balance_score=0.5, synergy_score=0.5, overall_score=5.0, num_cards=sum( int(c.get("count", 0)) for p in completed.get("partitions", []) if p.get("name") == ("Main" if test_case.game == "magic" else "Main Deck") for c in p.get("cards", []) ), num_unique_tags=0, avg_tags_per_card=0.0, ) success = quality.overall_score >= test_case.expected_quality_min return CompletionResult( test_case=test_case, completed_deck=completed, quality_metrics=quality, success=success, ) except Exception as e: logger.error(f"Completion test failed: {e}", exc_info=True) return CompletionResult( test_case=test_case, completed_deck=test_case.incomplete_deck, quality_metrics=None, # type: ignore success=False, error=str(e), ) def validate_deck_completion( game: str = "magic", num_test_cases: int = 10, similarity_fn: Optional[Callable[[str, int], list[tuple[str, float]]]] = None, tag_set_fn: Optional[Callable[[str], set[str]]] = None, cmc_fn: Optional[Callable[[str], Optional[int]]] = None, price_fn: Optional[Callable[[str], Optional[float]]] = None, ) -> dict[str, Any]: """ Validate deck completion produces high-quality decks. Returns validation results with success rate and quality metrics. """ # Create test cases test_cases = create_test_cases(game=game, num_cases=num_test_cases) if not test_cases: return { "valid": False, "error": "No test cases created", "success_rate": 0.0, "results": [], } if similarity_fn is None: return { "valid": False, "error": "similarity_fn required", "success_rate": 0.0, "results": [], } # Run tests results = [] for test_case in test_cases: result = run_completion_test( test_case=test_case, similarity_fn=similarity_fn, tag_set_fn=tag_set_fn, cmc_fn=cmc_fn, price_fn=price_fn, ) results.append(result) # Compute statistics successful = sum(1 for r in results if r.success) success_rate = successful / len(results) if results else 0.0 quality_scores = [ r.quality_metrics.overall_score for r in results if r.quality_metrics is not None ] avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0.0 # Validation criteria valid = success_rate >= 0.7 and avg_quality >= 6.0 return { "valid": valid, "success_rate": success_rate, "avg_quality_score": avg_quality, "num_test_cases": len(test_cases), "num_successful": successful, "results": [ { "test_case": asdict(r.test_case), "quality_score": r.quality_metrics.overall_score if r.quality_metrics else None, "success": r.success, "error": r.error, } for r in results ], } def main() -> int: """CLI for deck quality validation.""" import argparse parser = argparse.ArgumentParser(description="Validate deck completion quality") parser.add_argument( "--game", type=str, default="magic", choices=["magic", "pokemon", "yugioh"], help="Game to test", ) parser.add_argument( "--num-cases", type=int, default=10, help="Number of test cases", ) parser.add_argument( "--embeddings", type=Path, help="Path to embeddings .wv file", ) parser.add_argument( "--pairs", type=Path, help="Path to pairs CSV file", ) parser.add_argument( "--method", type=str, default="fusion", choices=["embedding", "jaccard", "fusion"], help="Similarity method", ) parser.add_argument( "--output", type=Path, help="Output JSON file for validation results", ) args = parser.parse_args() # Load similarity function try: from .similarity_helper import ( create_similarity_function, create_similarity_function_from_defaults, create_similarity_function_from_env, ) # Try environment variables first, then explicit paths, then defaults try: similarity_fn = create_similarity_function_from_env() logger.info("Loaded similarity function from environment variables") except ValueError: if args.embeddings or args.pairs: similarity_fn = create_similarity_function( embeddings_path=args.embeddings, pairs_path=args.pairs, method=args.method, ) logger.info("Loaded similarity function from provided paths") else: similarity_fn = create_similarity_function_from_defaults(game=args.game) logger.info(f"Loaded similarity function from defaults for {args.game}") except Exception as e: logger.error(f"Failed to load similarity function: {e}") print(f"Error: {e}") print("\nOptions:") print(" 1. Set EMBEDDINGS_PATH and PAIRS_PATH environment variables") print(" 2. Use --embeddings and --pairs flags") print(" 3. Train embeddings first (default paths will be used)") return 1 # Load tag_set_fn and cmc_fn if available tag_set_fn = None cmc_fn = None try: from ..enrichment.card_functional_tagger import FunctionalTagger if args.game == "magic": tagger = FunctionalTagger() tag_set_fn = lambda card: set(tagger.tag_card(card).keys()) logger.info("Loaded functional tagger for MTG") except Exception: logger.warning("Functional tagger not available - quality metrics will be limited") try: from ..data.card_database import get_card_database card_db = get_card_database() card_db.load() if args.game == "magic": # Get CMC from card database def get_cmc(card_name: str) -> int | None: # This is a placeholder - would need actual CMC lookup return None cmc_fn = get_cmc except Exception: logger.warning("Card database not available - CMC lookup disabled") # Run validation output_path = args.output or PATHS.experiments / f"deck_quality_validation_{args.game}.json" results = validate_deck_completion( game=args.game, num_test_cases=args.num_cases, similarity_fn=similarity_fn, tag_set_fn=tag_set_fn, cmc_fn=cmc_fn, ) # Save results import json with open(output_path, "w") as f: json.dump(results, f, indent=2) # Print summary print(f"\nDeck Quality Validation: {args.game}") print("=" * 60) print(f"Test cases: {results['num_test_cases']}") print(f"Successful: {results['num_successful']}") print(f"Success rate: {results['success_rate'] * 100:.1f}%") print(f"Avg quality score: {results['avg_quality_score']:.2f}/10.0") print(f"\nStatus: {' VALID' if results['valid'] else 'Error: FAILED'}") if results.get("error"): print(f"\nError: {results['error']}") print(f"\nResults saved to: {output_path}") return 0 if results["valid"] else 1 if __name__ == "__main__": import sys sys.exit(main())
