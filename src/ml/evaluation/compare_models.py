#!/usr/bin/env python3 """ Model Comparison Framework Compare different embedding models and similarity methods: - Different dimensions (64, 128, 256) - Different algorithms (node2vec, DeepWalk, LINE) - Different p,q parameters - Different similarity metrics (cosine, euclidean, etc) Uses annotated test sets for rigorous evaluation. """ import argparse import json from collections.abc import Callable from pathlib import Path import numpy as np import pandas as pd from scipy.spatial.distance import cityblock, euclidean try: from gensim.models import KeyedVectors HAS_GENSIM = True except ImportError: HAS_GENSIM = False class SimilarityMethod: """Different methods to compute similarity from embeddings""" @staticmethod def cosine(wv, card1: str, card2: str) -> float: """Cosine similarity (default for word2vec)""" try: return wv.similarity(card1, card2) except KeyError: return 0.0 @staticmethod def euclidean(wv, card1: str, card2: str) -> float: """Euclidean distance (inverted to similarity)""" try: v1 = wv[card1] v2 = wv[card2] dist = euclidean(v1, v2) return 1.0 / (1.0 + dist) except KeyError: return 0.0 @staticmethod def manhattan(wv, card1: str, card2: str) -> float: """Manhattan distance (inverted)""" try: v1 = wv[card1] v2 = wv[card2] dist = cityblock(v1, v2) return 1.0 / (1.0 + dist) except KeyError: return 0.0 class ModelComparator: """Compare multiple embedding models""" def __init__(self, test_set_file: str): """Load annotated test set""" with open(test_set_file) as f: self.test_set = json.load(f) print(f" Loaded test set: {len(self.test_set)} queries") def evaluate_model( self, wv, similarity_fn: Callable = SimilarityMethod.cosine, k_values: list[int] | None = None, ) -> dict: """ Evaluate model on test set. Metrics: - Precision@K (graded: highly_relevant = 1.0, relevant = 0.75, etc) - NDCG@K (normalized discounted cumulative gain) - MRR (mean reciprocal rank) """ if k_values is None: k_values = [5, 10, 20] results = {f"P@{k}": [] for k in k_values} results.update({f"NDCG@{k}": [] for k in k_values}) results["MRR"] = [] for query, ground_truth in self.test_set.items(): if query not in wv: continue # Get all test cards for this query all_test_cards = ( ground_truth.get("highly_relevant", []) + ground_truth.get("relevant", []) + ground_truth.get("somewhat_relevant", []) + ground_truth.get("marginally_relevant", []) + ground_truth.get("irrelevant", []) ) # Compute similarity scores scored_cards = [] for card in all_test_cards: sim = similarity_fn(wv, query, card) scored_cards.append((card, sim)) # Sort by similarity scored_cards.sort(key=lambda x: x[1], reverse=True) # Compute metrics for k in k_values: # Precision@K (graded) p_at_k = self._precision_at_k(scored_cards[:k], ground_truth) results[f"P@{k}"].append(p_at_k) # NDCG@K ndcg = self._ndcg_at_k(scored_cards[:k], ground_truth, k) results[f"NDCG@{k}"].append(ndcg) # MRR mrr = self._mrr(scored_cards, ground_truth) results["MRR"].append(mrr) # Average metrics avg_results = {k: np.mean(v) if v else 0.0 for k, v in results.items()} return avg_results def _precision_at_k(self, predictions: list, ground_truth: dict) -> float: """Graded precision - weight by relevance level""" relevance_weights = { "highly_relevant": 1.0, "relevant": 0.75, "somewhat_relevant": 0.5, "marginally_relevant": 0.25, "irrelevant": 0.0, } score = 0.0 for card, _ in predictions: for level, weight in relevance_weights.items(): if card in ground_truth.get(level, []): score += weight break return score / len(predictions) if predictions else 0.0 def _ndcg_at_k(self, predictions: list, ground_truth: dict, k: int) -> float: """Normalized Discounted Cumulative Gain""" relevance_scores = { "highly_relevant": 4, "relevant": 3, "somewhat_relevant": 2, "marginally_relevant": 1, "irrelevant": 0, } # DCG dcg = 0.0 for i, (card, _) in enumerate(predictions[:k], 1): rel = 0 for level, score in relevance_scores.items(): if card in ground_truth.get(level, []): rel = score break dcg += rel / np.log2(i + 1) # IDCG (ideal ordering) all_rels = [] for level, cards in ground_truth.items(): score = relevance_scores.get(level, 0) all_rels.extend([score] * len(cards)) all_rels.sort(reverse=True) idcg = sum(rel / np.log2(i + 1) for i, rel in enumerate(all_rels[:k], 1)) return dcg / idcg if idcg > 0 else 0.0 def _mrr(self, predictions: list, ground_truth: dict) -> float: """Mean reciprocal rank - rank of first highly relevant""" highly_relevant = set(ground_truth.get("highly_relevant", [])) relevant = set(ground_truth.get("relevant", [])) target_set = highly_relevant | relevant for rank, (card, _) in enumerate(predictions, 1): if card in target_set: return 1.0 / rank return 0.0 def compare_models( self, models: dict[str, str], similarity_methods: dict[str, Callable] | None = None ) -> pd.DataFrame: """ Compare multiple models side-by-side. Args: models: {name: path_to_wv_file} similarity_methods: {name: similarity_function} Returns: DataFrame with comparison results """ if similarity_methods is None: similarity_methods = {"cosine": SimilarityMethod.cosine} results = [] for model_name, wv_path in models.items(): print(f"\n Evaluating {model_name}...") wv = KeyedVectors.load(wv_path) for sim_name, sim_fn in similarity_methods.items(): print(f" Using {sim_name} similarity...") metrics = self.evaluate_model(wv, sim_fn) results.append({"model": model_name, "similarity": sim_name, **metrics}) df = pd.DataFrame(results) return df def main(): parser = argparse.ArgumentParser(description="Compare embedding models") parser.add_argument("--test-set", type=str, required=True, help="Test set JSON") parser.add_argument("--models", nargs="+", required=True, help="Model .wv files") parser.add_argument( "--methods", nargs="+", choices=["cosine", "euclidean", "manhattan"], default=["cosine"], help="Similarity methods", ) parser.add_argument("--output", type=str, help="Output CSV") args = parser.parse_args() if not HAS_GENSIM: print("Error: gensim not installed") return 1 # Build model dict models = {} for model_path in args.models: name = Path(model_path).stem models[name] = model_path # Build similarity methods dict similarity_fns = {} for method in args.methods: if method == "cosine": similarity_fns["cosine"] = SimilarityMethod.cosine elif method == "euclidean": similarity_fns["euclidean"] = SimilarityMethod.euclidean elif method == "manhattan": similarity_fns["manhattan"] = SimilarityMethod.manhattan # Compare comparator = ModelComparator(args.test_set) results_df = comparator.compare_models(models, similarity_fns) # Display print("\n" + "=" * 80) print("MODEL COMPARISON RESULTS") print("=" * 80) # Sort by P@10 results_df = results_df.sort_values("P@10", ascending=False) print(results_df.to_string(index=False, float_format="%.4f")) # Save if args.output: results_df.to_csv(args.output, index=False) print(f"\nüíæ Saved results to {args.output}") # Print winner best = results_df.iloc[0] print(f"\nüèÜ Best model: {best['model']} ({best['similarity']})") print(f" P@10: {best['P@10']:.4f}") print(f" NDCG@10: {best['NDCG@10']:.4f}") print(f" MRR: {best['MRR']:.4f}") # Generate HTML report html_file = ( Path(args.output).parent / "comparison_report.html" if args.output else Path("comparison_report.html") ) generate_html_comparison(results_df, html_file, args) print(f"\nüìÑ HTML report: {html_file}") return 0 def generate_html_comparison(df, output_file, args): """Generate HTML comparison report""" from datetime import datetime best = df.iloc[0] html = f"""<!DOCTYPE html> <html> <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <meta name="color-scheme" content="light dark"> <title>Model Comparison Report</title> <style> :root {{ color-scheme: light dark; --bg: #ffffff; --fg: #1a1a1a; --fg-muted: #666666; --border: #e5e5e5; --accent: #0066cc; --code-bg: #f6f6f6; --success: #16a34a; }} @media (prefers-color-scheme: dark) {{ :root {{ --bg: #1a1a1a; --fg: #e0e0e0; --fg-muted: #999999; --border: #333333; --accent: #4a9eff; --code-bg: #2d2d2d; --success: #22c55e; }} }} * {{ margin: 0; padding: 0; box-sizing: border-box; }} body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; background: var(--bg); color: var(--fg); line-height: 1.6; padding: clamp(1rem, 3vw, 2rem); }} .container {{ max-width: 1400px; margin: 0 auto; }} header {{ border-bottom: 1px solid var(--border); padding-bottom: 1rem; margin-bottom: 2rem; }} h1 {{ font-size: clamp(1.5rem, 4vw, 2rem); font-weight: 600; margin-bottom: 0.25rem; letter-spacing: -0.02em; }} .timestamp {{ color: var(--fg-muted); font-size: 0.85rem; }} h2 {{ font-size: 1.3rem; font-weight: 600; margin: 2rem 0 0.75rem 0; letter-spacing: -0.01em; }} .winner {{ background: var(--code-bg); border: 2px solid var(--success); border-radius: 4px; padding: 1.5rem; margin: 1.5rem 0; }} .winner h2 {{ margin-top: 0; color: var(--success); font-size: 1.2rem; }} .winner p {{ margin: 0.5rem 0 0 0; }} table {{ width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.85rem; }} th, td {{ padding: 0.5rem; text-align: left; border: 1px solid var(--border); }} th {{ background: var(--code-bg); font-weight: 600; }} tr:hover {{ background: var(--code-bg); }} .metric {{ font-family: "SF Mono", Monaco, "Cascadia Code", "Roboto Mono", Consolas, monospace; font-size: 0.95em; }} .best {{ background: rgba(22, 163, 74, 0.1); }} .viz {{ margin: 1.5rem 0; }} .viz p {{ margin: 0.25rem 0; }} .bar {{ background: var(--accent); height: 18px; border-radius: 2px; display: inline-block; }} </style> </head> <body> <div class="container"> <header> <h1>Model Comparison Report</h1> <p class="timestamp">Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p> </header> <div class="winner"> <h2>Winner: {best["model"]} ({best["similarity"]})</h2> <p><span class="metric">P@10: {best["P@10"]:.4f}</span> | <span class="metric">NDCG@10: {best["NDCG@10"]:.4f}</span> | <span class="metric">MRR: {best["MRR"]:.4f}</span></p> </div> <h2>Full Results</h2> <table> <thead> <tr> <th>Rank</th> <th>Model</th> <th>Similarity</th> <th>P@5</th> <th>P@10</th> <th>P@20</th> <th>NDCG@5</th> <th>NDCG@10</th> <th>NDCG@20</th> <th>MRR</th> </tr> </thead> <tbody> """ for rank, (_, row) in enumerate(df.iterrows(), 1): row_class = ' class="best"' if rank == 1 else "" html += f""" <tr{row_class}> <td>{rank}</td> <td><strong>{row["model"]}</strong></td> <td>{row["similarity"]}</td> <td class="metric">{row.get("P@5", 0):.4f}</td> <td class="metric">{row.get("P@10", 0):.4f}</td> <td class="metric">{row.get("P@20", 0):.4f}</td> <td class="metric">{row.get("NDCG@5", 0):.4f}</td> <td class="metric">{row.get("NDCG@10", 0):.4f}</td> <td class="metric">{row.get("NDCG@20", 0):.4f}</td> <td class="metric">{row.get("MRR", 0):.4f}</td> </tr> """ html += """ </tbody> </table> <h2>P@10 Comparison (Visual)</h2> <div class="viz"> """ max_p10 = df["P@10"].max() for _, row in df.iterrows(): width = int((row["P@10"] / max_p10) * 300) html += f""" <p> <strong style="display: inline-block; width: 180px; font-size: 0.9em;">{row["model"]}</strong> <span class="bar" style="width: {width}px;"></span> <span class="metric" style="margin-left: 8px;">{row["P@10"]:.4f}</span> </p> """ html += "</div>" html += """ </div> </body> </html> """ with open(output_file, "w") as f: f.write(html) if __name__ == "__main__": import sys sys.exit(main())