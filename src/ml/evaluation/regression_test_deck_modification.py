#!/usr/bin/env python3 """ Regression Testing for Deck Modification System Uses ground truth annotations to test that the system maintains quality over time. Compares current API responses to annotated ground truth. Usage: python -m src.ml.evaluation.regression_test_deck_modification \ --annotations experiments/deck_modification_annotations.json \ --api-url http://localhost:8000 \ --output regression_results.json """ from __future__ import annotations import json import argparse from pathlib import Path from typing import Any from dataclasses import dataclass try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False @dataclass class RegressionResult: """Result of regression test for a single test case.""" test_case: str task: str # add|remove|replace|contextual metric: str # relevance|explanation_quality|archetype_match|role_fit expected_score: float # From annotations actual_score: float # From current API difference: float passed: bool # Within tolerance class DeckModificationRegressionTester: """Run regression tests against ground truth annotations.""" def __init__(self, api_url: str, tolerance: float = 0.5): self.api_url = api_url self.tolerance = tolerance # Allow 0.5 point difference def test_add_suggestions( self, test_case: dict[str, Any], annotations: dict[str, Any], ) -> list[RegressionResult]: """Test add suggestions against annotations.""" results = [] # Call API try: response = requests.post( f"{self.api_url}/v1/deck/suggest_actions", json={ "game": test_case["game"], "deck": test_case["deck"], "action_type": "add", "archetype": test_case.get("archetype"), "format": test_case.get("format"), "top_k": 10, }, timeout=30, ) if response.status_code != 200: return [RegressionResult( test_case=test_case["name"], task="add", metric="api_error", expected_score=0.0, actual_score=0.0, difference=0.0, passed=False, )] api_data = response.json() api_actions = {a["card"]: a for a in api_data.get("actions", [])} # Compare to annotations for judgment in annotations.get("judgments", {}).get("add", []): card = judgment["suggested_card"] expected_relevance = judgment.get("relevance", 2) if card in api_actions: # Card was suggested - check if it's in top results api_action = api_actions[card] actual_score = api_action.get("score", 0.0) # For now, just check if card appears (would need LLM to judge actual quality) results.append(RegressionResult( test_case=test_case["name"], task="add", metric="card_present", expected_score=1.0 if expected_relevance >= 3 else 0.0, actual_score=1.0, # Card is present difference=0.0, passed=True, )) else: # Card was not suggested - might be a regression if expected_relevance >= 3: # Should have been suggested results.append(RegressionResult( test_case=test_case["name"], task="add", metric="card_missing", expected_score=1.0, actual_score=0.0, difference=-1.0, passed=False, )) except Exception as e: results.append(RegressionResult( test_case=test_case["name"], task="add", metric="api_error", expected_score=0.0, actual_score=0.0, difference=0.0, passed=False, )) return results def test_contextual_discovery( self, test_case: dict[str, Any], annotations: dict[str, Any], ) -> list[RegressionResult]: """Test contextual discovery against annotations.""" results = [] try: params = { "game": test_case["game"], "top_k": 10, } if test_case.get("format"): params["format"] = test_case["format"] if test_case.get("archetype"): params["archetype"] = test_case["archetype"] response = requests.get( f"{self.api_url}/v1/cards/{test_case['card']}/contextual", params=params, timeout=30, ) if response.status_code != 200: return [RegressionResult( test_case=test_case["name"], task="contextual", metric="api_error", expected_score=0.0, actual_score=0.0, difference=0.0, passed=False, )] api_data = response.json() # Check each category for category in ["synergies", "alternatives", "upgrades", "downgrades"]: api_cards = {item["card"]: item for item in api_data.get(category, [])} expected_cards = set(test_case.get(f"expected_{category}", [])) for expected_card in expected_cards: if expected_card in api_cards: results.append(RegressionResult( test_case=test_case["name"], task="contextual", metric=f"{category}_present", expected_score=1.0, actual_score=1.0, difference=0.0, passed=True, )) else: results.append(RegressionResult( test_case=test_case["name"], task="contextual", metric=f"{category}_missing", expected_score=1.0, actual_score=0.0, difference=-1.0, passed=False, )) except Exception as e: results.append(RegressionResult( test_case=test_case["name"], task="contextual", metric="api_error", expected_score=0.0, actual_score=0.0, difference=0.0, passed=False, )) return results def run_regression_tests( self, annotations_path: Path, ) -> dict[str, Any]: """Run all regression tests.""" with open(annotations_path) as f: annotations_data = json.load(f) all_results = [] # Test deck modification for annotation in annotations_data: if "deck" in annotation: # Deck modification test case results = self.test_add_suggestions(annotation, annotation) all_results.extend(results) elif "card" in annotation: # Contextual discovery test case results = self.test_contextual_discovery(annotation, annotation) all_results.extend(results) # Aggregate results passed = sum(1 for r in all_results if r.passed) total = len(all_results) pass_rate = passed / total if total > 0 else 0.0 return { "total_tests": total, "passed": passed, "failed": total - passed, "pass_rate": pass_rate, "results": [ { "test_case": r.test_case, "task": r.task, "metric": r.metric, "expected": r.expected_score, "actual": r.actual_score, "difference": r.difference, "passed": r.passed, } for r in all_results ], } def main(): parser = argparse.ArgumentParser(description="Regression test deck modification system") parser.add_argument("--annotations", type=str, required=True, help="Path to annotations JSON") parser.add_argument("--api-url", type=str, required=True, help="API URL (e.g., http://localhost:8000)") parser.add_argument("--output", type=str, help="Output path for results") parser.add_argument("--tolerance", type=float, default=0.5, help="Tolerance for score differences") args = parser.parse_args() if not HAS_REQUESTS: print("Error: requests not installed") return 1 tester = DeckModificationRegressionTester(args.api_url, tolerance=args.tolerance) results = tester.run_regression_tests(Path(args.annotations)) if args.output: with open(args.output, "w") as f: json.dump(results, f, indent=2) print(f"âœ“ Results saved to {args.output}") else: print(json.dumps(results, indent=2)) print(f"\n Regression Test Summary:") print(f" Total: {results['total_tests']}") print(f" Passed: {results['passed']}") print(f" Failed: {results['failed']}") print(f" Pass Rate: {results['pass_rate']:.1%}") return 0 if results['pass_rate'] >= 0.8 else 1 if __name__ == "__main__": exit(main())