#!/usr/bin/env python3 """ Evaluation Framework for Card Embeddings Implements: - Train/validation/test splits - Precision@K, MRR, NDCG metrics - Baseline comparisons (random, degree centrality) - Experiment logging """ import argparse import json import random from collections import defaultdict from datetime import datetime from pathlib import Path import numpy as np import pandas as pd try: from gensim.models import KeyedVectors HAS_GENSIM = True except ImportError: HAS_GENSIM = False print("Install gensim: pip install gensim") class DeckSplitter: """Split decks by time for proper train/val/test separation""" def __init__(self, pairs_csv: str): self.df = pd.read_csv(pairs_csv) def split_by_count(self, train_frac=0.7, val_frac=0.15, test_frac=0.15): """Split edges randomly (baseline - not time-based)""" assert abs(train_frac + val_frac + test_frac - 1.0) < 0.001 n = len(self.df) indices = np.random.permutation(n) train_end = int(n * train_frac) val_end = train_end + int(n * val_frac) train_df = self.df.iloc[indices[:train_end]] val_df = self.df.iloc[indices[train_end:val_end]] test_df = self.df.iloc[indices[val_end:]] return train_df, val_df, test_df class BaselineModel: """Baseline similarity models for comparison""" def __init__(self, df: pd.DataFrame): self.df = df self._build_graph() def _build_graph(self): """Build adjacency lists""" self.adj = defaultdict(list) self.degree = defaultdict(int) for _, row in self.df.iterrows(): c1, c2, weight = row["NAME_1"], row["NAME_2"], row["COUNT_MULTISET"] self.adj[c1].append((c2, weight)) self.adj[c2].append((c1, weight)) self.degree[c1] += weight self.degree[c2] += weight def random_similarity(self, card: str, k: int = 10) -> list[tuple[str, float]]: """Random baseline - sample k random cards""" all_cards = list(self.degree.keys()) if card not in all_cards: return [] candidates = [c for c in all_cards if c != card] sampled = random.sample(candidates, min(k, len(candidates))) return [(c, random.random()) for c in sampled] def degree_similarity(self, card: str, k: int = 10) -> list[tuple[str, float]]: """Degree centrality baseline - cards with similar degree""" if card not in self.degree: return [] card_degree = self.degree[card] # Find cards with similar degree candidates = [] for c, deg in self.degree.items(): if c != card: similarity = 1.0 / (1.0 + abs(deg - card_degree)) candidates.append((c, similarity)) candidates.sort(key=lambda x: x[1], reverse=True) return candidates[:k] def jaccard_similarity(self, card: str, k: int = 10) -> list[tuple[str, float]]: """Jaccard similarity of co-occurrence sets""" if card not in self.adj: return [] card_neighbors = {c for c, _ in self.adj[card]} candidates = [] for c in self.degree: if c == card: continue other_neighbors = {n for n, _ in self.adj.get(c, [])} intersection = len(card_neighbors & other_neighbors) union = len(card_neighbors | other_neighbors) if union > 0: similarity = intersection / union candidates.append((c, similarity)) candidates.sort(key=lambda x: x[1], reverse=True) return candidates[:k] class Evaluator: """Evaluate embedding quality""" def __init__(self, test_df: pd.DataFrame): self.test_df = test_df self._build_test_graph() def _build_test_graph(self): """Build ground truth graph from test set""" self.true_neighbors = defaultdict(set) for _, row in self.test_df.iterrows(): c1, c2 = row["NAME_1"], row["NAME_2"] self.true_neighbors[c1].add(c2) self.true_neighbors[c2].add(c1) def precision_at_k(self, card: str, predictions: list[tuple[str, float]], k: int) -> float: """Precision@K - what fraction of top-k predictions are correct""" if card not in self.true_neighbors: return 0.0 top_k = [c for c, _ in predictions[:k]] true_set = self.true_neighbors[card] hits = sum(1 for c in top_k if c in true_set) return hits / k if k > 0 else 0.0 def mean_reciprocal_rank(self, card: str, predictions: list[tuple[str, float]]) -> float: """MRR - rank of first correct prediction""" if card not in self.true_neighbors: return 0.0 true_set = self.true_neighbors[card] for rank, (c, _) in enumerate(predictions, 1): if c in true_set: return 1.0 / rank return 0.0 def evaluate_model(self, model, cards: list[str], k_values=None) -> dict: """Evaluate model on all cards""" if k_values is None: k_values = [5, 10, 20] results = {f"P@{k}": [] for k in k_values} results["MRR"] = [] for card in cards: if callable(model): # Model is a function predictions = model(card) else: # Model is embeddings (KeyedVectors) try: similar = model.most_similar(card, topn=max(k_values)) predictions = [(c, float(sim)) for c, sim in similar] except KeyError: predictions = [] # Compute metrics for k in k_values: p_at_k = self.precision_at_k(card, predictions, k) results[f"P@{k}"].append(p_at_k) mrr = self.mean_reciprocal_rank(card, predictions) results["MRR"].append(mrr) # Average metrics avg_results = {k: np.mean(v) if v else 0.0 for k, v in results.items()} return avg_results class ExperimentLogger: """Log experiments to JSON for tracking""" def __init__(self, log_file: str = "experiments.jsonl"): self.log_file = Path(log_file) def log(self, experiment: dict): """Append experiment to log""" experiment["timestamp"] = datetime.now().isoformat() with open(self.log_file, "a") as f: f.write(json.dumps(experiment) + "\n") print(f"ðŸ“ Logged experiment to {self.log_file}") def load_experiments(self) -> list[dict]: """Load all experiments""" if not self.log_file.exists(): return [] experiments = [] with open(self.log_file) as f: for line in f: experiments.append(json.loads(line)) return experiments def best_experiment(self, metric: str = "P@10") -> dict: """Find best experiment by metric""" experiments = self.load_experiments() if not experiments: return {} best = max(experiments, key=lambda x: x.get("metrics", {}).get(metric, 0)) return best def main(): parser = argparse.ArgumentParser(description="Evaluate card embeddings") parser.add_argument("--pairs", type=str, required=True, help="Path to pairs.csv") parser.add_argument("--embeddings", type=str, help="Path to .wv embeddings file") parser.add_argument("--test-frac", type=float, default=0.15, help="Test set fraction") parser.add_argument("--seed", type=int, default=42, help="Random seed") parser.add_argument("--log", type=str, default="experiments.jsonl", help="Experiment log file") args = parser.parse_args() if not HAS_GENSIM: print("Error: gensim not installed") return 1 # Set seed for reproducibility random.seed(args.seed) np.random.seed(args.seed) print(" Evaluation Framework") print("=" * 60) # Split data print(f"\nðŸ“‚ Loading data from {args.pairs}...") splitter = DeckSplitter(args.pairs) train_df, val_df, test_df = splitter.split_by_count( train_frac=0.7, val_frac=0.15, test_frac=args.test_frac ) print(f" Train: {len(train_df):,} edges") print(f" Val: {len(val_df):,} edges") print(f" Test: {len(test_df):,} edges") # Get test cards test_cards = list(set(test_df["NAME_1"]) | set(test_df["NAME_2"])) print(f" Test cards: {len(test_cards):,}") # Initialize evaluator evaluator = Evaluator(test_df) # Evaluate baselines print("\n Baseline Evaluation") print("=" * 60) baseline = BaselineModel(train_df) baselines = { "random": lambda c: baseline.random_similarity(c, k=20), "degree": lambda c: baseline.degree_similarity(c, k=20), "jaccard": lambda c: baseline.jaccard_similarity(c, k=20), } baseline_results = {} for name, model in baselines.items(): print(f"\n{name.capitalize()} baseline:") results = evaluator.evaluate_model(model, test_cards[:100]) # Sample for speed baseline_results[name] = results for metric, value in results.items(): print(f" {metric}: {value:.4f}") # Evaluate embeddings if provided if args.embeddings: print("\n Embedding Evaluation") print("=" * 60) print(f"Loading {args.embeddings}...") wv = KeyedVectors.load(args.embeddings) print(f" Loaded {len(wv):,} embeddings") embedding_results = evaluator.evaluate_model(wv, test_cards) print("\nNode2Vec embeddings:") for metric, value in embedding_results.items(): print(f" {metric}: {value:.4f}") # Compare to best baseline best_baseline = max(baseline_results.items(), key=lambda x: x[1]["P@10"]) print(f"\nBest baseline: {best_baseline[0]} (P@10: {best_baseline[1]['P@10']:.4f})") print( f"Improvement: {(embedding_results['P@10'] / best_baseline[1]['P@10'] - 1) * 100:+.1f}%" ) # Log experiment logger = ExperimentLogger(args.log) logger.log( { "model": "node2vec", "embeddings": args.embeddings, "seed": args.seed, "metrics": embedding_results, "baseline_metrics": baseline_results, } ) print("\n Evaluation complete!") # Generate HTML report if args.embeddings: html_file = Path(args.log).parent / "evaluation_report.html" generate_html_report( baseline_results, embedding_results if args.embeddings else {}, html_file, args ) print(f"ðŸ“„ HTML report: {html_file}") return 0 def generate_html_report(baseline_results, embedding_results, output_file, args): """Generate HTML report of evaluation results""" html = f"""<!DOCTYPE html> <html> <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <meta name="color-scheme" content="light dark"> <title>Evaluation Report - {Path(args.embeddings).stem if args.embeddings else "Baselines"}</title> <style> :root {{ color-scheme: light dark; --bg: #ffffff; --fg: #1a1a1a; --fg-muted: #666666; --border: #e5e5e5; --accent: #0066cc; --code-bg: #f6f6f6; --success: #16a34a; }} @media (prefers-color-scheme: dark) {{ :root {{ --bg: #1a1a1a; --fg: #e0e0e0; --fg-muted: #999999; --border: #333333; --accent: #4a9eff; --code-bg: #2d2d2d; --success: #22c55e; }} }} * {{ margin: 0; padding: 0; box-sizing: border-box; }} body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; background: var(--bg); color: var(--fg); line-height: 1.6; padding: clamp(1rem, 3vw, 2rem); }} .container {{ max-width: 1200px; margin: 0 auto; }} header {{ border-bottom: 1px solid var(--border); padding-bottom: 1rem; margin-bottom: 2rem; }} h1 {{ font-size: clamp(1.5rem, 4vw, 2rem); font-weight: 600; margin-bottom: 0.25rem; letter-spacing: -0.02em; }} .timestamp {{ color: var(--fg-muted); font-size: 0.85rem; }} h2 {{ font-size: 1.3rem; font-weight: 600; margin: 2rem 0 0.75rem 0; letter-spacing: -0.01em; }} .params {{ background: var(--code-bg); border: 1px solid var(--border); border-radius: 4px; padding: 1rem; margin: 1.5rem 0; font-size: 0.9rem; }} .params p {{ margin: 0.25rem 0; }} .params strong {{ color: var(--fg); }} table {{ width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }} th, td {{ padding: 0.5rem 0.75rem; text-align: left; border: 1px solid var(--border); }} th {{ background: var(--code-bg); font-weight: 600; }} tr:hover {{ background: var(--code-bg); }} .metric {{ font-family: "SF Mono", Monaco, "Cascadia Code", "Roboto Mono", Consolas, monospace; font-size: 0.95em; }} .best {{ background: rgba(22, 163, 74, 0.1); }} .improvement {{ font-size: 1.5rem; font-weight: 600; margin: 1rem 0; }} .improvement.positive {{ color: var(--success); }} .improvement.negative {{ color: #dc2626; }} </style> </head> <body> <div class="container"> <header> <h1>Evaluation Report</h1> <p class="timestamp">Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p> </header> <div class="params"> <p><strong>Test Fraction:</strong> {args.test_frac}</p> <p><strong>Random Seed:</strong> {args.seed}</p> {f"<p><strong>Embeddings:</strong> {args.embeddings}</p>" if args.embeddings else ""} </div> """ # Baseline results html += """ <h2>Baseline Models</h2> <table> <thead> <tr> <th>Model</th> <th>P@5</th> <th>P@10</th> <th>P@20</th> <th>MRR</th> </tr> </thead> <tbody> """ for name, results in sorted( baseline_results.items(), key=lambda x: x[1].get("P@10", 0), reverse=True ): best_class = ( ' class="best"' if name == max(baseline_results.items(), key=lambda x: x[1].get("P@10", 0))[0] else "" ) html += f""" <tr{best_class}> <td><strong>{name}</strong></td> <td class="metric">{results.get("P@5", 0):.4f}</td> <td class="metric">{results.get("P@10", 0):.4f}</td> <td class="metric">{results.get("P@20", 0):.4f}</td> <td class="metric">{results.get("MRR", 0):.4f}</td> </tr> """ html += """ </tbody> </table> """ # Embedding results if embedding_results: best_baseline_p10 = max(baseline_results.items(), key=lambda x: x[1].get("P@10", 0))[1][ "P@10" ] embed_p10 = embedding_results.get("P@10", 0) improvement = (embed_p10 / best_baseline_p10 - 1) * 100 is_positive = improvement > 0 html += """ <h2>Embedding Model</h2> <table> <thead> <tr> <th>Model</th> <th>P@5</th> <th>P@10</th> <th>P@20</th> <th>MRR</th> </tr> </thead> <tbody> """ html += f""" <tr class="best"> <td><strong>Node2Vec</strong></td> <td class="metric">{embedding_results.get("P@5", 0):.4f}</td> <td class="metric">{embed_p10:.4f}</td> <td class="metric">{embedding_results.get("P@20", 0):.4f}</td> <td class="metric">{embedding_results.get("MRR", 0):.4f}</td> </tr> </tbody> </table> <h2>Improvement vs Best Baseline</h2> <p>Best baseline P@10: <span class="metric">{best_baseline_p10:.4f}</span></p> <p>Embedding P@10: <span class="metric">{embed_p10:.4f}</span></p> <p class="improvement {"positive" if is_positive else "negative"}"> {improvement:+.1f}% </p> """ html += """ </div> </body> </html> """ with open(output_file, "w") as f: f.write(html) if __name__ == "__main__": import sys sys.exit(main())
