#!/usr/bin/env python3 """ Confidence Intervals Integration Integrates confidence interval computation into evaluation scripts. Part of T0.1 foundation refinement. """ from __future__ import annotations from pathlib import Path from typing import Any from .test_set_validation import ( bootstrap_confidence_interval, compute_metric_confidence_intervals, normal_confidence_interval, ) def add_confidence_intervals_to_evaluation_results( evaluation_results: dict[str, Any], per_query_metrics: dict[str, dict[str, float]] | None = None, confidence: float = 0.95, ) -> dict[str, Any]: """ Add confidence intervals to evaluation results. Args: evaluation_results: Dict with metrics like {"p_at_10": 0.08, "mrr": 0.12, ...} per_query_metrics: Optional dict mapping query -> metric_name -> value confidence: Confidence level (default 0.95) Returns: Updated evaluation_results with confidence intervals """ results_with_ci = evaluation_results.copy() # If per_query_metrics provided, compute CIs from per-query data if per_query_metrics: intervals = compute_metric_confidence_intervals( per_query_metrics, confidence=confidence, ) # Add intervals to results for metric_name, ci in intervals.items(): results_with_ci[f"{metric_name}_ci"] = { "value": ci.value, "lower": ci.lower, "upper": ci.upper, "confidence": ci.confidence, "n": ci.n, } # If only aggregate metrics, estimate CI from single value (less reliable) else: # For metrics that might have per-query data, add placeholder for metric_name in ["p_at_10", "p_at_5", "mrr", "ndcg"]: if metric_name in results_with_ci: value = results_with_ci[metric_name] # Estimate CI assuming n=100 queries (conservative) # This is less reliable than actual per-query CIs results_with_ci[f"{metric_name}_ci_note"] = ( "Confidence interval estimated. " "For accurate CIs, provide per_query_metrics." ) return results_with_ci def format_metric_with_ci( metric_name: str, value: float, ci: dict[str, Any] | None = None, ) -> str: """ Format metric with confidence interval for display. Returns string like "0.08 (95% CI: 0.06-0.10, n=100)" """ if ci: return ( f"{value:.3f} " f"({int(ci['confidence']*100)}% CI: {ci['lower']:.3f}-{ci['upper']:.3f}, " f"n={ci['n']})" ) return f"{value:.3f}" def main() -> int: """CLI for adding confidence intervals to evaluation results.""" import argparse import json parser = argparse.ArgumentParser( description="Add confidence intervals to evaluation results" ) parser.add_argument( "--evaluation-results", type=Path, required=True, help="Path to evaluation results JSON", ) parser.add_argument( "--per-query-metrics", type=Path, help="Path to per-query metrics JSON (optional)", ) parser.add_argument( "--output", type=Path, help="Output path (default: overwrite input)", ) parser.add_argument( "--confidence", type=float, default=0.95, help="Confidence level (default: 0.95)", ) args = parser.parse_args() # Load evaluation results with open(args.evaluation_results) as f: results = json.load(f) # Load per-query metrics if provided per_query_metrics = None if args.per_query_metrics and args.per_query_metrics.exists(): with open(args.per_query_metrics) as f: per_query_metrics = json.load(f) # Add confidence intervals results_with_ci = add_confidence_intervals_to_evaluation_results( evaluation_results=results, per_query_metrics=per_query_metrics, confidence=args.confidence, ) # Save output_path = args.output or args.evaluation_results with open(output_path, "w") as f: json.dump(results_with_ci, f, indent=2) print(f" Added confidence intervals to: {output_path}") # Print summary for metric_name in ["p_at_10", "p_at_5", "mrr", "ndcg"]: if metric_name in results_with_ci: value = results_with_ci[metric_name] ci_key = f"{metric_name}_ci" ci = results_with_ci.get(ci_key) print(f" {metric_name}: {format_metric_with_ci(metric_name, value, ci)}") return 0 if __name__ == "__main__": import sys sys.exit(main())
