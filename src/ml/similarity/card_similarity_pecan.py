#!/usr/bin/env python3 """ Card Similarity using PecanPy (node2vec) https://github.com/krishnanlab/PecanPy PecanPy is a fast, parallelized, memory-efficient implementation of node2vec optimized for graphs of different sizes and densities. Published in: Liu R, Krishnan A (2021) Bioinformatics """ import argparse from pathlib import Path # import matplotlib.pyplot as plt # moved to lazy import in visualize_embeddings import numpy as np import pandas as pd from sklearn.manifold import TSNE try: from gensim.models import Word2Vec from pecanpy.pecanpy import DenseOTF, PreComp, SparseOTF HAS_PECANPY = True except ImportError: HAS_PECANPY = False print("PecanPy not installed. Install with:") print(" pip install pecanpy") def prepare_edgelist(csv_file: str, output_edg: str, min_cooccurrence: int = 2): """ Convert DeckSage pairs.csv to PecanPy edgelist format. Warning: DATA LEAKAGE WARNING: If pairs CSV includes test period data, embeddings will leak. Recommendation: Filter pairs by timestamp before training: uv run python -m ml.scripts.filter_pairs_by_timestamp """ print(f" Loading graph from {csv_file}...") # Check if this looks like filtered pairs if "train_val_only" not in csv_file and "filtered" not in csv_file.lower(): print("Warning: WARNING: Pairs CSV may include test period ‚Üí potential data leakage") print(" Recommendation: Use filtered pairs (train/val only)") print(" Run: uv run python -m ml.scripts.filter_pairs_by_timestamp") df = pd.read_csv(csv_file) # Filter by minimum co-occurrence df = df[df["COUNT_SET"] >= min_cooccurrence] print(f" Filtered to {len(df):,} pairs (min co-occurrence: {min_cooccurrence})") # Write to edgelist format: node1 node2 weight with open(output_edg, "w") as f: for _, row in df.iterrows(): f.write(f"{row['NAME_1']}\t{row['NAME_2']}\t{row['COUNT_MULTISET']}\n") num_nodes = len(set(df["NAME_1"]) | set(df["NAME_2"])) print(f" Graph: {num_nodes:,} nodes, {len(df):,} edges") print(f" Saved edgelist to {output_edg}") return num_nodes, len(df) def train_pecanpy( edgelist_file: str, output_file: str, dim: int = 128, walk_length: int = 80, num_walks: int = 10, window_size: int = 10, p: float = 1.0, q: float = 1.0, workers: int = 4, mode: str = "SparseOTF", extend: bool = True, # Use node2vec+ for weighted graphs ): """Train node2vec embeddings using PecanPy""" print(f"\n Training PecanPy node2vec{'+' if extend else ''}...") print(f" Mode: {mode} (optimized for this graph type)") print(f" Dimension: {dim}") print(f" Walk length: {walk_length}") print(f" Num walks: {num_walks}") print(f" Window: {window_size}") print(f" p={p}, q={q}") print(f" Workers: {workers}") print(f" node2vec+: {extend}") # Initialize PecanPy graph based on mode if mode == "PreComp": g = PreComp(p=p, q=q, workers=workers, verbose=True, extend=extend) elif mode == "SparseOTF": g = SparseOTF(p=p, q=q, workers=workers, verbose=True, extend=extend) elif mode == "DenseOTF": g = DenseOTF(p=p, q=q, workers=workers, verbose=True, extend=extend) else: raise ValueError(f"Unknown mode: {mode}") # Read graph print("\nüìñ Reading graph...") g.read_edg(edgelist_file, weighted=True, directed=False) # Generate walks print("\nüö∂ Generating random walks...") walks = g.simulate_walks(num_walks=num_walks, walk_length=walk_length) # Train Word2Vec print("\nüß† Training Word2Vec...") model = Word2Vec( walks, vector_size=dim, window=window_size, min_count=0, sg=1, # Skip-gram workers=workers, epochs=1, ) # Save model.wv.save(output_file) print(f"‚úì Saved embeddings to {output_file}") return model.wv def find_similar_cards(wv, card_name: str, top_k: int = 10): """Find most similar cards""" try: similar = wv.most_similar(card_name, topn=top_k) return [(card, float(sim)) for card, sim in similar] except KeyError: print(f"Card '{card_name}' not found in vocabulary") return [] def visualize_embeddings(wv, output_file: str, max_cards: int = 500): """Create t-SNE visualization""" print("\nüé® Creating t-SNE visualization...") # Lazy import to avoid hard dependency try: import matplotlib.pyplot as plt # type: ignore except Exception: print(" Matplotlib not installed; skipping visualization.") return # Get all embeddings cards = list(wv.index_to_key) embeddings = np.array([wv[card] for card in cards]) # Sample if too many if len(cards) > max_cards: indices = np.random.choice(len(cards), max_cards, replace=False) cards_sample = [cards[i] for i in indices] embeddings_sample = embeddings[indices] else: cards_sample = cards embeddings_sample = embeddings # t-SNE print(f" Running t-SNE on {len(cards_sample):,} cards...") tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(cards_sample) - 1)) embeddings_2d = tsne.fit_transform(embeddings_sample) # Plot plt.figure(figsize=(20, 16)) plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.4, s=30, c="steelblue") # Annotate sample cards for i in range(min(80, len(cards_sample))): if i % 4 == 0: # Every 4th card plt.annotate( cards_sample[i], (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=7, alpha=0.7 ) plt.title("MTG Card Embedding Space (PecanPy node2vec+ & t-SNE)", fontsize=16) plt.xlabel("t-SNE dimension 1", fontsize=12) plt.ylabel("t-SNE dimension 2", fontsize=12) plt.tight_layout() plt.savefig(output_file, dpi=200, bbox_inches="tight") print(f" Saved to {output_file}") def demo_similarity_search(wv, query_cards): """Demonstrate similarity search""" print("\nüîç Card Similarity Search Results\n") print("=" * 80) for card in query_cards: similar = find_similar_cards(wv, card, top_k=10) if similar: print(f"\nüìå Cards similar to '{card}':") for i, (sim_card, score) in enumerate(similar, 1): bar = "‚ñà" * int(score * 20) print(f" {i:2d}. {sim_card:50s} {bar} {score:.3f}") print("\n" + "=" * 80) def main(): parser = argparse.ArgumentParser( description="Card Similarity with PecanPy node2vec", formatter_class=argparse.RawDescriptionHelpFormatter, epilog=""" Examples: # Fast training with default settings python card_similarity_pecan.py --input ../../data/processed/pairs.csv # High quality embeddings python card_similarity_pecan.py --input ../../data/processed/pairs.csv \\ --dim 256 --walk-length 100 --num-walks 20 --epochs 10 # Query specific cards python card_similarity_pecan.py --input ../../data/processed/pairs.csv \\ --query "Lightning Bolt" "Brainstorm" "Dark Ritual" """, ) parser.add_argument("--input", type=str, required=True, help="Path to pairs.csv") parser.add_argument("--dim", type=int, default=128, help="Embedding dimension") parser.add_argument("--walk-length", type=int, default=80, help="Random walk length") parser.add_argument("--num-walks", type=int, default=10, help="Number of walks per node") parser.add_argument("--window", type=int, default=10, help="Context window size") parser.add_argument("--p", type=float, default=1.0, help="Return parameter") parser.add_argument("--q", type=float, default=1.0, help="In-out parameter") parser.add_argument("--workers", type=int, default=4, help="Number of workers") parser.add_argument( "--mode", type=str, default="SparseOTF", choices=["PreComp", "SparseOTF", "DenseOTF"], help="PecanPy mode (SparseOTF for large sparse graphs)", ) parser.add_argument("--min-cooccur", type=int, default=2, help="Min co-occurrence") parser.add_argument("--query", type=str, nargs="+", help="Cards to query") parser.add_argument("--visualize", action="store_true", help="Create visualization") parser.add_argument("--output", type=str, default="magic", help="Output prefix") args = parser.parse_args() if not HAS_PECANPY: print("\nError: Error: PecanPy not installed") print("Install with: pip install pecanpy") return 1 # Output to data directories graphs_dir = Path(__file__).parent.parent.parent / "data" / "graphs" embeddings_dir = Path(__file__).parent.parent.parent / "data" / "embeddings" graphs_dir.mkdir(parents=True, exist_ok=True) embeddings_dir.mkdir(parents=True, exist_ok=True) # Prepare edgelist edg_file = graphs_dir / f"{args.output}_graph.edg" num_nodes, num_edges = prepare_edgelist(args.input, str(edg_file), args.min_cooccur) # Recommend mode based on graph size if num_nodes < 10000: recommended_mode = "PreComp" elif num_edges / (num_nodes * (num_nodes - 1) / 2) < 0.2: recommended_mode = "SparseOTF" else: recommended_mode = "DenseOTF" if args.mode != recommended_mode: print(f"\n Recommendation: {recommended_mode} might be faster for this graph") print(f" (You're using: {args.mode})") # Train embeddings wv_file = embeddings_dir / f"{args.output}_pecanpy.wv" wv = train_pecanpy( str(edg_file), str(wv_file), dim=args.dim, walk_length=args.walk_length, num_walks=args.num_walks, window_size=args.window, p=args.p, q=args.q, workers=args.workers, mode=args.mode, extend=True, # Use node2vec+ for weighted graphs ) print(f"\nüíæ Embeddings saved to {wv_file}") # Demo similarity search if args.query: demo_similarity_search(wv, args.query) else: # Try classic MTG cards demo_cards = [ "Lightning Bolt", "Brainstorm", "Dark Ritual", "Sol Ring", "Counterspell", "Path to Exile", "Swords to Plowshares", ] available = [c for c in demo_cards if c in wv] if available: demo_similarity_search(wv, available[:3]) # Visualization if args.visualize: experiments_dir = Path(__file__).parent.parent.parent / "experiments" experiments_dir.mkdir(parents=True, exist_ok=True) viz_file = experiments_dir / f"{args.output}_pecanpy_tsne.png" visualize_embeddings(wv, str(viz_file)) print("\n Experiment complete!") print("\nüìö To load embeddings later:") print(" from gensim.models import KeyedVectors") print(f" wv = KeyedVectors.load('{wv_file}')") print(" similar = wv.most_similar('Lightning Bolt', topn=10)") print("\nüìñ Citation:") print(" Liu R, Krishnan A (2021) PecanPy: a fast, efficient, and") print(" parallelized Python implementation of node2vec. Bioinformatics") print(" https://doi.org/10.1093/bioinformatics/btab202") return 0 if __name__ == "__main__": import sys sys.exit(main())
