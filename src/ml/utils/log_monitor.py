
"""
Unified log monitoring for runctl and local training jobs. This module provides structured log parsing and monitoring that works with: - runctl-managed AWS instances (via `runctl aws logs` command) - Local training scripts (via log file monitoring) - S3 progress files (training_metrics.jsonl, training_progress.json) It parses the structured log prefixes ([PROGRESS], [CHECKPOINT], [STAGE], [METRIC]) and correlation IDs for unified tracking across distributed systems. Architecture: - Infrastructure (runctl): SSH/SSM log tailing via `runctl aws logs` - Domain logic (Python): Structured parsing, correlation IDs, ML progress tracking Usage: # Monitor runctl instance (uses runctl internally) monitor = RunctlLogMonitor(instance_id="i-123") status = monitor.get_status() # Monitor local log monitor = LocalLogMonitor(log_path="training.log") status = monitor.get_status() 
"""

from __future__ import annotations import json import re import subprocess import time from dataclasses import dataclass, field from datetime import datetime, timezone from pathlib import Path from typing import Any try: from .logging_config import get_logger logger = get_logger(__name__) except ImportError: import logging logger = logging.getLogger(__name__) @dataclass class LogEvent: 
"""
Parsed log event with structured information.
"""
 timestamp: datetime level: str prefix: str | None # [PROGRESS], [CHECKPOINT], etc. correlation_id: str | None message: str raw_line: str metadata: dict[str, Any] = field(default_factory=dict) @dataclass class TrainingStatus: 
"""
Current training status from logs. Includes all parsed metadata from structured logs, including: - Progress information (stage, current, total, percentage) - Component tracking (component name) - Resource metrics (CPU, memory, GPU if available) - Context fields (experiment, dataset_version, model_architecture, git_commit) - Timing information (elapsed, eta, throughput) 
"""
 correlation_id: str | None = None stage: str | None = None progress: str | None = None last_checkpoint: str | None = None last_metric: dict[str, Any] | None = None errors: list[str] = field(default_factory=list) last_update: datetime | None = None is_complete: bool = False # Context fields experiment_name: str | None = None hostname: str | None = None pid: int | None = None dataset_version: str | None = None model_architecture: str | None = None git_commit: str | None = None # Component tracking component: str | None = None # Resource metrics cpu_percent: float | None = None memory_mb: float | None = None gpu_utilization: int | None = None # Parsed progress fields progress_current: float | None = None progress_total: float | None = None progress_percentage: float | None = None elapsed_seconds: int | None = None # Additional metadata metadata: dict[str, Any] = field(default_factory=dict) class LogParser: 
"""
Parse structured logs with parseable prefixes. Optimized for performance with compiled regex patterns and efficient parsing. Handles edge cases: malformed logs, missing fields, partial lines. 
"""
 # Compiled regex patterns (reused across all parsing operations) PREFIX_PATTERN = re.compile(r'\[(PROGRESS|CHECKPOINT|STAGE|METRIC)\]') # Updated to match new format: corr_id=hex8 or [hex8] (backward compatible) CORRELATION_PATTERN = re.compile(r'(?:corr_id=([a-f0-9]{8})|\[([a-f0-9]{8})\])') # New structured format: stage=value progress=X/Y pct=Z% (also supports old format) PROGRESS_PATTERN_NEW = re.compile(r'stage=(\w+)(?:\s+progress=([\d.]+)/([\d.]+))?(?:\s+pct=([\d.]+)%)?') PROGRESS_PATTERN_OLD = re.compile(r'\[PROGRESS\]\s+(\w+):\s+([\d.]+)/([\d.]+)\s+\(([\d.]+)%\)') CHECKPOINT_PATTERN_NEW = re.compile(r'name=([^\s]+)(?:\s+path=([^\s]+))?') CHECKPOINT_PATTERN_OLD = re.compile(r'\[CHECKPOINT\]\s+(\w+)(?:\s+saved to\s+(.+))?') METRIC_PATTERN = re.compile(r'\[METRIC\]\s+(.+)') STAGE_PATTERN = re.compile(r'\[STAGE\]\s+(.+)') # Log level patterns (more flexible to handle variations) # Handles formats like: # "2024-01-15 10:30:45 - INFO - module - message" # "2024-01-15 10:30:45,123 - INFO - module - func:line - message" LEVEL_PATTERN = re.compile( r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}(?:[.,]\d+)?)\s+-\s+(\w+)\s+-\s+(\w+)\s+-\s+(?:[\w:\.]+\s+-\s+)?(.+)' ) # Alternative timestamp formats ALT_TIMESTAMP_PATTERNS = [ re.compile(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?)'), # ISO format re.compile(r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})'), # Simple format ] @classmethod def parse_line(cls, line: str) -> LogEvent | None: 
"""
Parse a single log line into a structured event. Handles edge cases: - Empty or whitespace-only lines - Malformed timestamps - Missing log levels - Partial/corrupted log entries 
"""
 if not line or not line.strip(): return None # Try to extract timestamp and level (standard format) timestamp = datetime.now(timezone.utc) # Default to now level = "INFO" # Default level message = line.strip() timestamp_match = cls.LEVEL_PATTERN.match(line) if timestamp_match: timestamp_str, name, level_str, msg = timestamp_match.groups() level = level_str.upper() if level_str else "INFO" message = msg if msg else line # Try to parse timestamp (handle multiple formats) try: # Standard format: "2024-01-15 10:30:45" or "2024-01-15 10:30:45,123" (comma for microseconds) if ',' in timestamp_str: # Handle comma-separated microseconds (Python logging format) timestamp = datetime.strptime(timestamp_str.split(',')[0], "%Y-%m-%d %H:%M:%S") else: timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S") except ValueError: try: # ISO format: "2024-01-15T10:30:45" or "2024-01-15T10:30:45.123Z" if 'T' in timestamp_str: # Remove timezone if present (split on + or - before timezone offset) if '+' in timestamp_str: ts_clean = timestamp_str.split('+')[0] elif timestamp_str.endswith('Z'): ts_clean = timestamp_str.rstrip('Z') else: ts_clean = timestamp_str # Remove fractional seconds if present if '.' in ts_clean: ts_clean = ts_clean.split('.')[0] timestamp = datetime.strptime(ts_clean, "%Y-%m-%dT%H:%M:%S") else: timestamp = datetime.now(timezone.utc) except ValueError: # Try alternative patterns for pattern in cls.ALT_TIMESTAMP_PATTERNS: alt_match = pattern.search(line) if alt_match: try: ts_str = alt_match.group(1) if 'T' in ts_str: # Remove timezone and fractional seconds if '+' in ts_str: ts_clean = ts_str.split('+')[0] elif ts_str.endswith('Z'): ts_clean = ts_str.rstrip('Z') else: ts_clean = ts_str if '.' in ts_clean: ts_clean = ts_clean.split('.')[0] timestamp = datetime.strptime(ts_clean, "%Y-%m-%dT%H:%M:%S") break except ValueError: continue # If all parsing fails, use current time timestamp = datetime.now(timezone.utc) else: # No standard format, try to extract just timestamp for pattern in cls.ALT_TIMESTAMP_PATTERNS: match = pattern.search(line) if match: try: ts_str = match.group(1) if 'T' in ts_str: # Remove timezone and fractional seconds if '+' in ts_str: ts_clean = ts_str.split('+')[0] elif ts_str.endswith('Z'): ts_clean = ts_str.rstrip('Z') else: ts_clean = ts_str if '.' in ts_clean: ts_clean = ts_clean.split('.')[0] timestamp = datetime.strptime(ts_clean, "%Y-%m-%dT%H:%M:%S") else: timestamp = datetime.strptime(ts_str, "%Y-%m-%d %H:%M:%S") break except ValueError: continue # Extract prefix prefix_match = cls.PREFIX_PATTERN.search(message) prefix = prefix_match.group(1) if prefix_match else None # Extract correlation ID (handle both new format corr_id=hex8 and old format [hex8]) corr_match = cls.CORRELATION_PATTERN.search(message) if corr_match: # Group 1 is corr_id=hex8, group 2 is [hex8] correlation_id = corr_match.group(1) or corr_match.group(2) else: correlation_id = None # Extract all key=value pairs from structured format (for both old and new) # Pattern: key=value (handles unquoted values, stops at space) # Updated to handle values with spaces (stops at next key= or end) kv_pattern = re.compile(r'(\w+)=([^\s=]+(?:\s+[^\s=]+)*?)(?=\s+\w+=|\s*$)') kv_pairs = {} for kv_match in kv_pattern.finditer(message): key, value = kv_match.groups() # Skip context fields (already extracted, but we'll still parse them for TrainingStatus) # Don't skip - we want them in metadata for TrainingStatus extraction # Clean up value (remove trailing spaces) value = value.strip() # Try to parse as number or structured format try: # Handle progress format X/Y if '/' in value and value.count('/') == 1: try: current_str, total_str = value.split('/') kv_pairs[f"{key}_current"] = float(current_str) kv_pairs[f"{key}_total"] = float(total_str) kv_pairs[key] = value # Keep original format too except ValueError: kv_pairs[key] = value # Handle time format HH:MM:SS elif ':' in value and value.count(':') == 2: try: parts = value.split(':') if len(parts) == 3: hours, mins, secs = map(int, parts) total_seconds = hours * 3600 + mins * 60 + secs kv_pairs[f"{key}_seconds"] = total_seconds kv_pairs[key] = value # Keep original format too else: kv_pairs[key] = value except (ValueError, TypeError): kv_pairs[key] = value # Handle percentage values (remove %) elif value.endswith('%'): try: kv_pairs[key] = float(value[:-1]) kv_pairs[f"{key}_pct"] = float(value[:-1]) # Also store as _pct except ValueError: kv_pairs[key] = value # Try numeric elif '.' in value: kv_pairs[key] = float(value) else: kv_pairs[key] = int(value) except ValueError: kv_pairs[key] = value # Extract metadata based on prefix (with error handling) metadata: dict[str, Any] = {} try: if prefix == "PROGRESS": # Try new structured format first progress_match = cls.PROGRESS_PATTERN_NEW.search(message) if progress_match: stage, current, total, pct = progress_match.groups() metadata = {"stage": stage} try: if current and total: metadata["current"] = float(current) metadata["total"] = float(total) if pct: metadata["percentage"] = float(pct) except (ValueError, TypeError): if current: metadata["current"] = current if total: metadata["total"] = total else: # Fall back to old format progress_match = cls.PROGRESS_PATTERN_OLD.search(message) if progress_match: stage, current, total, pct = progress_match.groups() try: metadata = { "stage": stage, "current": float(current), "total": float(total), "percentage": float(pct), } except (ValueError, TypeError): metadata = {"stage": stage, "current": current, "total": total} # Merge in any additional key=value pairs found metadata.update(kv_pairs) elif prefix == "CHECKPOINT": # Try new structured format first checkpoint_match = cls.CHECKPOINT_PATTERN_NEW.search(message) if checkpoint_match: name, path = checkpoint_match.groups() metadata = {"name": name or "unknown"} if path: metadata["path"] = path else: # Fall back to old format checkpoint_match = cls.CHECKPOINT_PATTERN_OLD.search(message) if checkpoint_match: name, path = checkpoint_match.groups() metadata = {"name": name or "unknown", "path": path} # Merge in any additional key=value pairs found metadata.update(kv_pairs) elif prefix == "METRIC": # Extract key=value pairs from metric message metric_match = cls.METRIC_PATTERN.search(message) if metric_match: metric_str = metric_match.group(1) # Try to parse key=value pairs (handle various formats) for pair in metric_str.split(", "): pair = pair.strip() if "=" in pair: try: key, value = pair.split("=", 1) key = key.strip() value = value.strip() # Try to parse as number, fall back to string try: # Try int first, then float if '.' in value: metadata[key] = float(value) else: metadata[key] = int(value) except ValueError: metadata[key] = value except ValueError: # Skip malformed pairs continue except Exception as e: # Log but don't fail - return event with partial metadata logger.debug(f"Error extracting metadata from log line: {e}") return LogEvent( timestamp=timestamp, level=level, prefix=prefix, correlation_id=correlation_id, message=message, raw_line=line, metadata=metadata, ) @classmethod def parse_file(cls, log_path: Path | str, last_n_lines: int = 100) -> list[LogEvent]: 
"""
Parse last N lines from a log file. Handles: - Large files (efficiently reads only last N lines) - Corrupted/partial lines - Encoding issues - File locking 
"""
 log_path = Path(log_path) if not log_path.exists(): return [] events: list[LogEvent] = [] try: # For large files, read from end (more efficient) with open(log_path, 'rb') as f: # Seek to end f.seek(0, 2) file_size = f.tell() # Read backwards to get last N lines if file_size == 0: return [] # Estimate bytes needed (assume ~100 chars per line) bytes_to_read = min(file_size, last_n_lines * 200) f.seek(max(0, file_size - bytes_to_read)) # Read and decode content = f.read().decode('utf-8', errors='replace') # Replace invalid chars lines = content.splitlines() # Take last N lines for line in lines[-last_n_lines:]: event = cls.parse_line(line) if event: events.append(event) except PermissionError: logger.warning(f"Permission denied reading log file: {log_path}") except UnicodeDecodeError as e: logger.warning(f"Encoding error in log file {log_path}: {e}") # Try with different encoding try: with open(log_path, encoding='latin-1', errors='replace') as f: lines = f.readlines() for line in lines[-last_n_lines:]: event = cls.parse_line(line) if event: events.append(event) except Exception: pass except Exception as e: logger.warning(f"Error parsing log file {log_path}: {e}") return events class RunctlLogMonitor: 
"""
Monitor logs from runctl-managed instances. Uses runctl for infrastructure (SSH/SSM log tailing) and Python for domain logic (structured log parsing, ML progress tracking). 
"""
 def __init__( self, instance_id: str, log_path: str | Path | None = None, runctl_bin: Path | str | None = None, use_ssm: bool = True, # Kept for backward compatibility, but runctl handles this ): 
"""
Initialize monitor for runctl instance. Args: instance_id: EC2 instance ID log_path: Path to log file on instance (default: auto-detect via runctl) runctl_bin: Path to runctl binary use_ssm: Deprecated - runctl handles SSM/SSH automatically 
"""
 self.instance_id = instance_id self.log_path = log_path # Find runctl binary if runctl_bin: self.runctl_bin = Path(runctl_bin) else: project_root = Path(__file__).parent.parent.parent.parent self.runctl_bin = project_root.parent / "runctl" / "target" / "release" / "runctl" if not self.runctl_bin.exists(): # Try alternative locations alt_paths = [ Path.home() / ".cargo" / "bin" / "runctl", Path("/usr/local/bin/runctl"), Path("/usr/bin/runctl"), ] found = False for alt_path in alt_paths: if alt_path.exists(): self.runctl_bin = alt_path found = True break if not found: raise FileNotFoundError( f"runctl not found. Expected at {self.runctl_bin}.\n" "Build with: cd ../runctl && cargo build --release" ) def _get_instance_info(self) -> dict[str, Any]: 
"""
Get instance information using runctl (infrastructure layer). Note: This is kept for backward compatibility but runctl handles instance info internally. Consider removing in favor of runctl commands. 
"""
 import subprocess import json # Try to use runctl first (if it has an info command in the future) # For now, fall back to AWS CLI try: result = subprocess.run( ["aws", "ec2", "describe-instances", "--instance-ids", self.instance_id, "--query", "Reservations[0].Instances[0].[KeyName,PublicIpAddress,State.Name,PrivateIpAddress]", "--output", "json"], capture_output=True, text=True, timeout=10, ) if result.returncode == 0: data = json.loads(result.stdout) if data and len(data) > 0 and data[0]: key_name, public_ip, state, private_ip = data[0] if len(data[0]) >= 4 else [None, None, None, None] # Use private IP if public IP not available (for VPC-only instances) ip_address = public_ip if public_ip and public_ip != "None" else private_ip return { "key_name": key_name if key_name and key_name != "None" else None, "public_ip": ip_address, "state": state if state else "unknown", "has_ssh_key": key_name is not None and key_name != "None" and key_name != "", } except (subprocess.TimeoutExpired, json.JSONDecodeError, KeyError, IndexError) as e: logger.debug(f"Error getting instance info: {e}") except Exception as e: logger.warning(f"Unexpected error getting instance info: {e}") return {"has_ssh_key": False, "state": "unknown", "public_ip": None} def tail_logs(self, lines: int = 50) -> list[str]: 
"""
Tail logs from instance using runctl (infrastructure layer). Uses runctl for SSH/SSM log tailing, keeping Python focused on parsing. Falls back to S3 if runctl unavailable. 
"""
 import subprocess # Use runctl for log tailing (infrastructure) try: cmd = [ str(self.runctl_bin), "aws", "logs", self.instance_id, "--tail", f"--lines={lines}", ] # Add log path if specified if self.log_path: cmd.extend(["--path", str(self.log_path)]) else: # Auto-detect if path not provided cmd.append("--auto-detect") result = subprocess.run( cmd, capture_output=True, text=True, timeout=30, ) if result.returncode == 0 and result.stdout: return result.stdout.strip().split("\n") # If runctl fails, log and fall back logger.debug(f"runctl logs failed (exit {result.returncode}): {result.stderr}") except FileNotFoundError: logger.debug(f"runctl not found at {self.runctl_bin}, falling back to S3") except subprocess.TimeoutExpired: logger.debug("runctl logs command timed out, falling back to S3") except Exception as e: logger.debug(f"Error calling runctl logs: {e}, falling back to S3") # Fallback to S3 if runctl unavailable or fails return self._get_logs_from_s3() def _get_logs_from_s3(self) -> list[str]: 
"""
Try to get logs from S3 if they're synced there.
"""
 import subprocess import tempfile # Check if logs are synced to S3 (common pattern) s3_log_path = f"s3://games-collections/logs/{self.instance_id}.log" try: with tempfile.NamedTemporaryFile(mode='w+', delete=False) as tmp: result = subprocess.run( ["aws", "s3", "cp", s3_log_path, tmp.name], capture_output=True, timeout=10, ) if result.returncode == 0: with open(tmp.name) as f: lines = f.readlines() return [line.strip() for line in lines[-50:]] # Last 50 lines except Exception: pass return [] def get_status(self, use_live_logs: bool = True, max_errors: int = 10) -> TrainingStatus: 
"""
Get current training status from instance logs. Args: use_live_logs: Try to tail live logs from instance (slower but more accurate) max_errors: Maximum number of errors to track (prevents unbounded growth) 
"""
 status = TrainingStatus() # Try to get logs from instance if use_live_logs: try: log_lines = self.tail_logs(lines=100, retry=True) if log_lines: # Parse log lines (with error handling) events: list[LogEvent] = [] parse_errors = 0 for line in log_lines: try: event = LogParser.parse_line(line) if event: events.append(event) except Exception as e: parse_errors += 1 if parse_errors <= 3: # Log first few parse errors logger.debug(f"Error parsing log line: {e}") if events: # Process events to build status (most recent first) for event in reversed(events): if not status.correlation_id and event.correlation_id: status.correlation_id = event.correlation_id if event.prefix == "PROGRESS": if event.metadata: status.stage = event.metadata.get("stage") current = event.metadata.get("current") total = event.metadata.get("total") if current is not None and total is not None: try: status.progress = f"{int(current)}/{int(total)}" except (ValueError, TypeError): status.progress = f"{current}/{total}" status.last_update = event.timestamp elif event.prefix == "CHECKPOINT": if event.metadata: status.last_checkpoint = event.metadata.get("name") status.last_update = event.timestamp elif event.prefix == "METRIC": if event.metadata: status.last_metric = event.metadata status.last_update = event.timestamp elif event.level == "ERROR": if len(status.errors) < max_errors: status.errors.append(event.message[:200]) # Truncate long errors if not status.last_update: status.last_update = event.timestamp # Check for completion (multiple indicators) msg_lower = event.message.lower() if any(keyword in msg_lower for keyword in ["complete", "finished", "done", "success", "training complete"]): status.is_complete = True if status.last_update: return status except Exception as e: logger.debug(f"Error getting live logs: {e}, falling back to S3") # Fallback to S3 progress files return self._get_status_from_s3() def _get_status_from_s3(self) -> TrainingStatus: 
"""
Get status from S3 progress files (fallback).
"""
 import tempfile status = TrainingStatus() # Check S3 for progress files progress_dir = "s3://games-collections/training_progress" try: # Check training_progress.json with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp: result = subprocess.run( ["aws", "s3", "cp", f"{progress_dir}/training_progress.json", tmp.name], capture_output=True, timeout=10, ) if result.returncode == 0: try: with open(tmp.name) as f: progress_data = json.load(f) status.last_update = datetime.now(timezone.utc) status.stage = progress_data.get("current_stage") last_epoch = progress_data.get("last_epoch", "?") total_epochs = progress_data.get("total_epochs", "?") if last_epoch != "?" and total_epochs != "?": status.progress = f"{last_epoch}/{total_epochs}" else: status.progress = progress_data.get("progress", None) # Extract correlation ID if available status.correlation_id = progress_data.get("correlation_id") except (json.JSONDecodeError, IOError) as e: logger.debug(f"Error parsing training_progress.json: {e}") else: # Clean up temp file if download failed try: Path(tmp.name).unlink() except Exception: pass # Check latest metrics with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.jsonl') as tmp: result = subprocess.run( ["aws", "s3", "cp", f"{progress_dir}/training_metrics.jsonl", tmp.name], capture_output=True, timeout=10, ) if result.returncode == 0: try: with open(tmp.name) as f: lines = f.readlines() if lines: # Get last non-empty line for line in reversed(lines): line = line.strip() if line: latest = json.loads(line) status.last_metric = latest.get("metrics", {}) # Extract correlation ID if available if not status.correlation_id: status.correlation_id = latest.get("correlation_id") break except (json.JSONDecodeError, IOError) as e: logger.debug(f"Error parsing training_metrics.jsonl: {e}") else: # Clean up temp file if download failed try: Path(tmp.name).unlink() except Exception: pass except subprocess.TimeoutExpired: logger.debug("Timeout reading S3 progress files") except Exception as e: logger.debug(f"Error reading S3 progress files: {e}") return status class LocalLogMonitor: 
"""
Monitor logs from local training scripts.
"""
 def __init__(self, log_path: Path | str): 
"""
Initialize monitor for local log file. Args: log_path: Path to log file 
"""
 self.log_path = Path(log_path) def get_status(self, last_n_lines: int = 100) -> TrainingStatus: 
"""
Get current training status from log file.
"""
 events = LogParser.parse_file(self.log_path, last_n_lines=last_n_lines) status = TrainingStatus() # Process events in reverse (most recent first) for event in reversed(events): if not status.correlation_id and event.correlation_id: status.correlation_id = event.correlation_id # Extract context fields from metadata if not status.experiment_name and "experiment" in event.metadata: status.experiment_name = str(event.metadata["experiment"]) if not status.hostname and "host" in event.metadata: status.hostname = str(event.metadata["host"]) if not status.pid and "pid" in event.metadata: try: status.pid = int(event.metadata["pid"]) except (ValueError, TypeError): pass if not status.dataset_version and "dataset_version" in event.metadata: status.dataset_version = str(event.metadata["dataset_version"]) if not status.model_architecture and "model_architecture" in event.metadata: status.model_architecture = str(event.metadata["model_architecture"]) if not status.git_commit and "git_commit" in event.metadata: status.git_commit = str(event.metadata["git_commit"]) if not status.component and "component" in event.metadata: status.component = str(event.metadata["component"]) # Extract resource metrics if status.cpu_percent is None and "cpu_percent" in event.metadata: try: status.cpu_percent = float(event.metadata["cpu_percent"]) except (ValueError, TypeError): pass if status.memory_mb is None and "memory_mb" in event.metadata: try: status.memory_mb = float(event.metadata["memory_mb"]) except (ValueError, TypeError): pass if status.gpu_utilization is None and "gpu_utilization" in event.metadata: try: status.gpu_utilization = int(event.metadata["gpu_utilization"]) except (ValueError, TypeError): pass # Extract parsed progress fields if status.progress_current is None and "progress_current" in event.metadata: try: status.progress_current = float(event.metadata["progress_current"]) except (ValueError, TypeError): pass if status.progress_total is None and "progress_total" in event.metadata: try: status.progress_total = float(event.metadata["progress_total"]) except (ValueError, TypeError): pass if status.progress_percentage is None: # Try percentage from pct or percentage field if "pct" in event.metadata: try: status.progress_percentage = float(event.metadata["pct"]) except (ValueError, TypeError): pass elif "percentage" in event.metadata: try: status.progress_percentage = float(event.metadata["percentage"]) except (ValueError, TypeError): pass if status.elapsed_seconds is None and "elapsed_seconds" in event.metadata: try: status.elapsed_seconds = int(event.metadata["elapsed_seconds"]) except (ValueError, TypeError): pass # Store all metadata for access status.metadata.update(event.metadata) if event.prefix == "PROGRESS": if event.metadata: status.stage = event.metadata.get("stage") current = event.metadata.get("current") or status.progress_current total = event.metadata.get("total") or status.progress_total if current and total: status.progress = f"{current}/{total}" status.last_update = event.timestamp elif event.prefix == "CHECKPOINT": if event.metadata: status.last_checkpoint = event.metadata.get("name") status.last_update = event.timestamp elif event.prefix == "METRIC": if event.metadata: status.last_metric = event.metadata status.last_update = event.timestamp elif event.level == "ERROR": status.errors.append(event.message) if not status.last_update: status.last_update = event.timestamp # Check for completion if "complete" in event.message.lower() or "finished" in event.message.lower(): status.is_complete = True return status def tail(self, callback: Any, follow: bool = True) -> None: 
"""
Tail log file and call callback for each new event. Args: callback: Function to call with each new LogEvent follow: Continue tailing (like tail -f) 
"""
 if not self.log_path.exists(): logger.warning(f"Log file does not exist: {self.log_path}") return # Read existing lines first with open(self.log_path) as f: f.seek(0, 2) # Seek to end # Then follow for new lines if follow: while True: line = f.readline() if line: event = LogParser.parse_line(line) if event: callback(event) else: time.sleep(0.1) def monitor_training( instance_id: str | None = None, log_path: Path | str | None = None, use_runctl: bool = True, check_interval: int = 30, use_live_logs: bool = True, ) -> TrainingStatus: 
"""
Unified monitoring function for training jobs. Works with both runctl instances and local logs. Includes error handling and retry logic for robust operation. Args: instance_id: EC2 instance ID (for runctl monitoring) log_path: Local log file path (for local monitoring) use_runctl: Use runctl monitoring (requires instance_id) check_interval: Seconds between checks (for future use in loops) use_live_logs: Try to tail live logs (slower but more accurate) Returns: Current training status (may be empty if no data available) 
"""
 try: if use_runctl and instance_id: monitor = RunctlLogMonitor(instance_id) return monitor.get_status(use_live_logs=use_live_logs) elif log_path: monitor = LocalLogMonitor(log_path) return monitor.get_status() else: raise ValueError("Either instance_id (for runctl) or log_path (for local) must be provided") except Exception as e: logger.warning(f"Error in monitor_training: {e}") # Return empty status rather than crashing return TrainingStatus() def format_status(status: TrainingStatus, verbose: bool = False) -> str: 
"""
Format training status for human-readable display. Args: status: TrainingStatus to format verbose: Include more details (all errors, full metrics) 
"""
 lines = [] if status.correlation_id: lines.append(f"Correlation ID: {status.correlation_id}") if status.stage: lines.append(f"Stage: {status.stage}") if status.progress: lines.append(f"Progress: {status.progress}") if status.last_checkpoint: lines.append(f"Last Checkpoint: {status.last_checkpoint}") if status.last_metric: # Format metrics nicely metric_items = list(status.last_metric.items()) if verbose or len(metric_items) <= 5: metric_str = ", ".join(f"{k}={v:.4f}" if isinstance(v, float) else f"{k}={v}" for k, v in metric_items) else: # Show top 5 most important metrics important_keys = ['loss', 'p10', 'mrr', 'ndcg', 'accuracy'] shown = [] for key in important_keys: if key in status.last_metric: v = status.last_metric[key] shown.append(f"{key}={v:.4f}" if isinstance(v, float) else f"{key}={v}") if len(shown) < len(metric_items): shown.append(f"... ({len(metric_items) - len(shown)} more)") metric_str = ", ".join(shown) lines.append(f"Latest Metrics: {metric_str}") if status.errors: error_count = len(status.errors) lines.append(f"Errors: {error_count}") # Show last 3 errors (or all if verbose) errors_to_show = status.errors if verbose else status.errors[-3:] for error in errors_to_show: # Truncate very long errors display_error = error[:100] + "..." if len(error) > 100 else error lines.append(f" - {display_error}") if not verbose and error_count > 3: lines.append(f" ... ({error_count - 3} more errors)") # Status with age calculation if status.is_complete: lines.append("Status: COMPLETE") elif status.last_update: # Ensure both datetimes are timezone-aware now = datetime.now(timezone.utc) last_update = status.last_update if last_update.tzinfo is None: # Assume UTC if naive last_update = last_update.replace(tzinfo=timezone.utc) age_seconds = (now - last_update).total_seconds() if age_seconds < 60: age_str = f"{int(age_seconds)}s" elif age_seconds < 3600: age_str = f"{age_seconds / 60:.1f}m" else: age_str = f"{age_seconds / 3600:.1f}h" lines.append(f"Status: RUNNING (last update: {age_str} ago)") # Warn if stale if age_seconds > 300: # 5 minutes lines.append("Warning: Warning: No updates in 5+ minutes (may be stuck)") else: lines.append("Status: ‚è≥ UNKNOWN (no log data found)") return "\n".join(lines)