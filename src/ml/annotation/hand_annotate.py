#!/usr/bin/env python3 """ Hand Annotation Tool for Test Set Expansion Creates annotation batches for manual hand annotation and grading. Supports expanding test sets to 100+ queries with proper statistical rigor. Usage: # Generate annotation batch for MTG python hand_annotate.py generate --game magic --target 50 --current 38 # Grade completed annotations python hand_annotate.py grade --input annotations/hand_batch_001_magic.yaml # Merge into canonical test set python hand_annotate.py merge --input annotations/hand_batch_001_magic.yaml --output experiments/test_set_canonical_magic.json """ from __future__ import annotations import argparse import json import random from collections import defaultdict from pathlib import Path from typing import Any import pandas as pd import yaml try: from gensim.models import KeyedVectors HAS_GENSIM = True except ImportError: HAS_GENSIM = False from ..utils.paths import PATHS class QueryGenerator: """Generate diverse query cards for annotation""" def __init__(self, pairs_csv: str | Path, embeddings_path: str | Path | None = None): self.df = pd.read_csv(pairs_csv) self.embeddings = None if embeddings_path and HAS_GENSIM: try: self.embeddings = KeyedVectors.load(str(embeddings_path)) except Exception: pass self._build_graph() def _build_graph(self): """Build adjacency list and degree stats""" self.adj = defaultdict(list) self.degree = defaultdict(int) self.total_weight = defaultdict(float) for _, row in self.df.iterrows(): c1, c2, weight = row["NAME_1"], row["NAME_2"], row.get("COUNT_MULTISET", 1) self.adj[c1].append((c2, weight)) self.adj[c2].append((c1, weight)) self.degree[c1] += 1 self.degree[c2] += 1 self.total_weight[c1] += weight self.total_weight[c2] += weight def sample_queries( self, n: int, strategy: str = "stratified", exclude: set[str] | None = None, seed: int = 42, ) -> list[str]: """ Sample query cards for annotation. Strategies: - stratified: Mix of high/medium/low degree cards (best for coverage) - random: Uniform random - popular: High degree only (well-known cards) - diverse: Maximize diversity using embeddings (if available) """ random.seed(seed) exclude = exclude or set() cards = [c for c in self.degree.keys() if c not in exclude] if strategy == "random": return random.sample(cards, min(n, len(cards))) elif strategy == "popular": sorted_cards = sorted(cards, key=lambda c: self.degree[c], reverse=True) return sorted_cards[:n] elif strategy == "stratified": # Divide into high/medium/low degree sorted_cards = sorted(cards, key=lambda c: self.degree[c], reverse=True) n_third = max(1, n // 3) high = random.sample( sorted_cards[: len(sorted_cards) // 3], min(n_third, len(sorted_cards) // 3) ) mid = random.sample( sorted_cards[len(sorted_cards) // 3 : 2 * len(sorted_cards) // 3], min(n_third, len(sorted_cards) // 3), ) low = random.sample( sorted_cards[2 * len(sorted_cards) // 3 :], min(n - 2 * n_third, len(sorted_cards) // 3), ) return high + mid + low elif strategy == "diverse" and self.embeddings: # Use embeddings to maximize diversity # Start with highest degree card, then iteratively add most diverse sorted_cards = sorted(cards, key=lambda c: self.degree[c], reverse=True) selected = [sorted_cards[0]] for _ in range(min(n - 1, len(sorted_cards) - 1)): best_card = None best_score = -1 for candidate in sorted_cards: if candidate in selected: continue # Compute minimum similarity to already selected min_sim = 1.0 for sel in selected: try: sim = self.embeddings.similarity(candidate, sel) min_sim = min(min_sim, abs(sim)) except KeyError: min_sim = 0.0 if min_sim < best_score or best_score == -1: best_score = min_sim best_card = candidate if best_card: selected.append(best_card) else: break return selected else: raise ValueError(f"Unknown strategy: {strategy}") class CandidateGenerator: """Generate candidate cards for annotation""" def __init__( self, pairs_csv: str | Path, embeddings_path: str | Path | None = None, graph_adj: dict[str, set[str]] | None = None, ): self.pairs_csv = pairs_csv self.embeddings = None if embeddings_path and HAS_GENSIM: try: self.embeddings = KeyedVectors.load(str(embeddings_path)) except Exception: pass # Build graph adjacency if not provided if graph_adj: self.adj = graph_adj else: self.adj = self._build_adj_from_csv() def _build_adj_from_csv(self) -> dict[str, set[str]]: """Build adjacency list from pairs CSV""" df = pd.read_csv(self.pairs_csv) adj = defaultdict(set) for _, row in df.iterrows(): c1, c2 = row["NAME_1"], row["NAME_2"] adj[c1].add(c2) adj[c2].add(c1) return dict(adj) def generate_candidates(self, query: str, k: int = 20, min_embedding_score: float = 0.3) -> list[dict[str, Any]]: """ Generate candidate cards from multiple sources with improved quality filtering. Args: query: Query card name k: Number of candidates to return (reduced default from 30 to 20) min_embedding_score: Minimum embedding similarity score (filters low-quality matches) Returns list of candidates with source attribution and scores. """ # Filter list: Cards that are too generic to be useful similarity results GENERIC_FILTER = { # Basic lands "Plains", "Island", "Swamp", "Mountain", "Forest", # Common fetches (too generic) "Arid Mesa", "Scalding Tarn", "Polluted Delta", "Verdant Catacombs", "Marsh Flats", "Misty Rainforest", "Bloodstained Mire", "Wooded Foothills", "Flooded Strand", "Windswept Heath", # Shocklands (too common) "Sacred Foundry", "Steam Vents", "Watery Grave", "Overgrown Tomb", "Temple Garden", "Godless Shrine", "Stomping Ground", "Blood Crypt", "Breeding Pool", "Hallowed Fountain", # Generic utility lands "Command Tower", "Path of Ancestry", "City of Brass", "Mana Confluence", "Exotic Orchard", "Reflecting Pool", "Forbidden Orchard", } candidates = {} # From embeddings (prioritize high-quality matches) embedding_matches = 0 if self.embeddings: try: # Try exact match first if query in self.embeddings: similar = self.embeddings.most_similar(query, topn=k * 2) # Get more, filter later for card, score in similar: # Filter low-quality embedding matches if score < min_embedding_score: continue # Filter generic cards if card in GENERIC_FILTER or card == query: continue if card not in candidates: candidates[card] = { "card": card, "sources": [], "scores": {}, } candidates[card]["sources"].append("embedding") candidates[card]["scores"]["embedding"] = float(score) embedding_matches += 1 else: # Try case-insensitive match vocab_lower = {k.lower(): k for k in self.embeddings.key_to_index.keys()} query_lower = query.lower() if query_lower in vocab_lower: actual_name = vocab_lower[query_lower] similar = self.embeddings.most_similar(actual_name, topn=k * 2) for card, score in similar: if score < min_embedding_score: continue # Filter generic cards if card in GENERIC_FILTER or card == query: continue if card not in candidates: candidates[card] = { "card": card, "sources": [], "scores": {}, } candidates[card]["sources"].append("embedding") candidates[card]["scores"]["embedding"] = float(score) embedding_matches += 1 except (KeyError, AttributeError) as e: import logging logging.getLogger(__name__).debug(f"Could not use embeddings for '{query}': {e}") # From graph co-occurrence (prioritize high-weight edges) if query in self.adj: # Get neighbors, but limit to top k to avoid too many low-quality matches neighbors = list(self.adj[query])[:k * 2] # Get more, filter later for card in neighbors: # Filter generic cards and self if card in GENERIC_FILTER or card == query: continue if card not in candidates: candidates[card] = { "card": card, "sources": [], "scores": {}, } candidates[card]["sources"].append("cooccurrence") candidates[card]["scores"]["cooccurrence"] = 1.0 # Sort by quality: prioritize candidates with multiple sources, then by highest score candidate_list = list(candidates.values()) candidate_list.sort( key=lambda x: ( len(x["sources"]), # Multiple sources = higher confidence max(x["scores"].values()) if x["scores"] else 0, # Higher score = better match ), reverse=True, ) # Return top k candidates return candidate_list[:k] def create_annotation_batch( game: str, target_queries: int, current_queries: int, pairs_csv: str | Path, embeddings_path: str | Path | None = None, output_path: str | Path | None = None, existing_test_set: dict[str, Any] | None = None, seed: int = 42, candidates_per_query: int = 20, min_embedding_score: float = 0.3, ) -> Path: """ Create annotation batch for hand annotation. Args: game: 'magic', 'pokemon', or 'yugioh' target_queries: Target number of queries (e.g., 50 for MTG) current_queries: Current number in test set pairs_csv: Path to pairs CSV embeddings_path: Optional path to embeddings output_path: Output YAML path existing_test_set: Existing test set to exclude queries from seed: Random seed Returns: Path to created annotation file """ # Determine how many new queries needed n_new = max(0, target_queries - current_queries) if n_new == 0: print(f"âœ“ Already have {current_queries} queries, no expansion needed") return Path(output_path) if output_path else Path("annotations/hand_batch_empty.yaml") # Load existing queries to exclude exclude = set() if existing_test_set: queries = existing_test_set.get("queries", existing_test_set) exclude = set(queries.keys()) # Generate new queries print(f"ðŸ“‹ Generating {n_new} new queries for {game}...") query_gen = QueryGenerator(pairs_csv, embeddings_path) new_queries = query_gen.sample_queries(n_new, strategy="stratified", exclude=exclude, seed=seed) # Generate candidates for each query print(f"ðŸ” Generating candidates for {len(new_queries)} queries...") cand_gen = CandidateGenerator(pairs_csv, embeddings_path) # Try to load card attributes for context card_attrs = {} attrs_path = PATHS.processed / "card_attributes_enriched.csv" if attrs_path.exists(): try: attrs_df = pd.read_csv(attrs_path) name_col = None for col in ["NAME", "name", "card_name", "Card"]: if col in attrs_df.columns: name_col = col break if name_col: for _, row in attrs_df.iterrows(): card_name = str(row[name_col]).strip() if card_name: card_attrs[card_name] = { "type": str(row.get("type", "")), "mana_cost": str(row.get("mana_cost", "")), "cmc": str(row.get("cmc", "")), "oracle_text": str(row.get("oracle_text", ""))[:200], # Truncate for readability } except Exception as e: import logging logging.getLogger(__name__).debug(f"Could not load card attributes: {e}") tasks = [] total_embedding_candidates = 0 total_cooccurrence_candidates = 0 queries_with_embeddings = 0 for query in new_queries: candidates = cand_gen.generate_candidates(query, k=candidates_per_query, min_embedding_score=min_embedding_score) # Track source statistics has_embedding = False for c in candidates: if "embedding" in c["sources"]: total_embedding_candidates += 1 has_embedding = True if "cooccurrence" in c["sources"]: total_cooccurrence_candidates += 1 if has_embedding: queries_with_embeddings += 1 # Add query context if available query_context = card_attrs.get(query, {}) task = { "query": query, "game": game, } # Add query context to help annotator if query_context: task["query_context"] = { "type": query_context.get("type", ""), "mana_cost": query_context.get("mana_cost", ""), "cmc": query_context.get("cmc", ""), } if query_context.get("oracle_text"): task["query_context"]["oracle_text"] = query_context["oracle_text"] # Add candidates with context candidate_list = [] for c in candidates: cand_card = c["card"] cand_context = card_attrs.get(cand_card, {}) cand_dict = { "card": cand_card, "sources": c["sources"], "scores": c.get("scores", {}), # Include scores for transparency "relevance": None, # To be filled by annotator (0-4) "similarity_type": None, # functional|synergy|archetype|manabase|unrelated "is_substitute": None, # True if can replace query in most decks "role_match": None, # True if serves same role/function (for downstream tasks) "archetype_context": None, # Which archetypes is this valid in? (comma-separated or "universal") "format_context": None, # Which formats? (comma-separated or "universal") "substitution_quality": None, # upgrade|downgrade|sidegrade (if is_substitute=True) "notes": "", # Free-form notes, especially for edge cases } # Add candidate context if available (helps annotator compare) if cand_context: cand_dict["context"] = { "type": cand_context.get("type", ""), "mana_cost": cand_context.get("mana_cost", ""), "cmc": cand_context.get("cmc", ""), } candidate_list.append(cand_dict) task["candidates"] = candidate_list tasks.append(task) # Create output directory if output_path: output_file = Path(output_path) else: output_dir = Path("annotations") output_dir.mkdir(exist_ok=True) batch_num = len(list(output_dir.glob("hand_batch_*.yaml"))) + 1 output_file = output_dir / f"hand_batch_{batch_num:03d}_{game}.yaml" # Write YAML with open(output_file, "w") as f: yaml.dump( { "metadata": { "game": game, "batch_id": output_file.stem, "num_queries": len(tasks), "created": pd.Timestamp.now().isoformat(), "target_total": target_queries, "current_total": current_queries, }, "instructions": { "relevance_scale": { 4: "Extremely similar (near substitutes, same function)", 3: "Very similar (often seen together, similar role)", 2: "Somewhat similar (related function or archetype)", 1: "Marginally similar (loose connection)", 0: "Irrelevant (different function, color, or archetype)", }, "grading_guidelines": [ "Focus on functional similarity (can they replace each other?)", "Consider archetype context (do they appear in same decks?)", "Consider mana cost and card type", "Add notes for edge cases or interesting patterns", ], "downstream_task_support": { "role_match": "Does candidate serve the same role/function as query? (removal, ramp, threat, etc.)", "is_substitute": "Can candidate replace query in most decks? (for substitution task)", "archetype_context": "Which archetypes is this similarity valid in? (e.g., 'Burn, RDW' or 'universal')", "format_context": "Which formats? (e.g., 'Modern, Legacy' or 'universal')", "substitution_quality": "If substitutable: upgrade (better), downgrade (worse), or sidegrade (equal)?", "similarity_type": "functional (same role), synergy (work together), archetype (same deck), manabase, or unrelated", }, "iaa_tracking": { "annotator_id": "Set this to your annotator ID (e.g., 'annotator_1', 'expert_reviewer')", "annotation_date": "Date of annotation (ISO format)", "confidence": "Your confidence in this annotation (1-5 scale, optional)", }, }, "tasks": tasks, }, f, default_flow_style=False, sort_keys=False, ) total_candidates = sum(len(t['candidates']) for t in tasks) print(f"âœ“ Created annotation batch: {output_file}") print(f" Queries: {len(tasks)}") print(f" Total candidates: {total_candidates}") if total_embedding_candidates > 0 or total_cooccurrence_candidates > 0: print(f" Source breakdown: {total_embedding_candidates} from embeddings, {total_cooccurrence_candidates} from co-occurrence") print(f" Queries with embedding matches: {queries_with_embeddings}/{len(tasks)}") print(f"\nðŸ“ Next steps:") print(f" 1. Open {output_file} in text editor") print(f" 2. For each candidate, set relevance (0-4) and add notes") print(f" 3. Run: python hand_annotate.py grade --input {output_file}") return output_file def grade_annotations(input_path: str | Path) -> dict[str, Any]: """ Grade and validate completed annotations. Returns statistics and validation results. """ input_path = Path(input_path) with open(input_path) as f: data = yaml.safe_load(f) tasks = data.get("tasks", []) stats = { "total_queries": len(tasks), "total_candidates": 0, "graded_candidates": 0, "ungraded_candidates": 0, "relevance_distribution": defaultdict(int), "queries_with_notes": 0, "validation_errors": [], } for task in tasks: query = task.get("query", "unknown") candidates = task.get("candidates", []) stats["total_candidates"] += len(candidates) query_has_notes = False for cand in candidates: relevance = cand.get("relevance") if relevance is None: stats["ungraded_candidates"] += 1 stats["validation_errors"].append(f"{query}: Candidate '{cand.get('card')}' not graded") else: stats["graded_candidates"] += 1 try: rel_int = int(relevance) if rel_int < 0 or rel_int > 4: stats["validation_errors"].append( f"{query}: Invalid relevance {rel_int} (must be 0-4)" ) else: stats["relevance_distribution"][rel_int] += 1 except (ValueError, TypeError): stats["validation_errors"].append( f"{query}: Non-numeric relevance '{relevance}'" ) if cand.get("notes", "").strip(): query_has_notes = True if query_has_notes: stats["queries_with_notes"] += 1 stats["completion_rate"] = ( stats["graded_candidates"] / stats["total_candidates"] if stats["total_candidates"] > 0 else 0.0 ) return stats def merge_to_test_set( annotation_path: str | Path, existing_test_set_path: str | Path, output_path: str | Path | None = None, ) -> Path: """ Merge completed annotations into canonical test set. Converts YAML annotations to test set JSON format. """ annotation_path = Path(annotation_path) existing_test_set_path = Path(existing_test_set_path) # Load existing test set with open(existing_test_set_path) as f: existing = json.load(f) # Load annotations with open(annotation_path) as f: annotations = yaml.safe_load(f) # Extract queries from existing (handle both formats) existing_queries = existing.get("queries", existing) # Convert annotations to test set format for task in annotations.get("tasks", []): query = task["query"] if query in existing_queries: print(f"Warning: Query '{query}' already exists, skipping...") continue # Build relevance buckets buckets = { "highly_relevant": [], "relevant": [], "somewhat_relevant": [], "marginally_relevant": [], "irrelevant": [], } for cand in task.get("candidates", []): relevance = cand.get("relevance") if relevance is None: continue try: rel_int = int(relevance) card = cand["card"] if rel_int == 4: buckets["highly_relevant"].append(card) elif rel_int == 3: buckets["relevant"].append(card) elif rel_int == 2: buckets["somewhat_relevant"].append(card) elif rel_int == 1: buckets["marginally_relevant"].append(card) elif rel_int == 0: buckets["irrelevant"].append(card) except (ValueError, TypeError): continue existing_queries[query] = buckets # Update metadata if "version" not in existing: existing["version"] = "merged" if "queries" not in existing: existing["queries"] = existing_queries else: existing["queries"] = existing_queries existing["num_queries"] = len(existing_queries) # Write output if output_path: output_file = Path(output_path) else: output_file = existing_test_set_path with open(output_file, "w") as f: json.dump(existing, f, indent=2) print(f"âœ“ Merged annotations into: {output_file}") print(f" Total queries: {existing['num_queries']}") return output_file def main() -> int: parser = argparse.ArgumentParser(description="Hand annotation tool for test set expansion") subparsers = parser.add_subparsers(dest="command", help="Command") # Generate annotation batch gen = subparsers.add_parser("generate", help="Generate annotation batch") gen.add_argument("--game", choices=["magic", "pokemon", "yugioh"], required=True) gen.add_argument("--target", type=int, required=True, help="Target number of queries") gen.add_argument("--current", type=int, required=True, help="Current number of queries") gen.add_argument("--pairs", type=str, help="Path to pairs CSV (default: from PATHS)") gen.add_argument("--embeddings", type=str, help="Path to embeddings file") gen.add_argument("--output", type=str, help="Output YAML path") gen.add_argument("--test-set", type=str, help="Existing test set to exclude queries from") gen.add_argument("--seed", type=int, default=42) gen.add_argument("--candidates-per-query", type=int, default=20, help="Number of candidates per query (default: 20)") gen.add_argument("--min-embedding-score", type=float, default=0.3, help="Minimum embedding similarity score (default: 0.3)") # Grade annotations grade = subparsers.add_parser("grade", help="Grade and validate annotations") grade.add_argument("--input", type=str, required=True, help="Annotation YAML file") # Merge to test set merge = subparsers.add_parser("merge", help="Merge annotations into test set") merge.add_argument("--input", type=str, required=True, help="Annotation YAML file") merge.add_argument("--test-set", type=str, required=True, help="Existing test set JSON") merge.add_argument("--output", type=str, help="Output test set JSON (default: overwrite input)") args = parser.parse_args() if args.command == "generate": # Determine pairs CSV path if args.pairs: pairs_csv = Path(args.pairs) else: pairs_csv = PATHS.pairs_large if not pairs_csv.exists(): pairs_csv = PATHS.pairs_500 if not pairs_csv.exists(): print(f"Error: Pairs CSV not found: {pairs_csv}") return 1 # Load existing test set if provided existing_test_set = None if args.test_set: with open(args.test_set) as f: existing_test_set = json.load(f) create_annotation_batch( game=args.game, target_queries=args.target, current_queries=args.current, pairs_csv=pairs_csv, embeddings_path=Path(args.embeddings) if args.embeddings else None, output_path=Path(args.output) if args.output else None, existing_test_set=existing_test_set, seed=args.seed, candidates_per_query=getattr(args, "candidates_per_query", 20), min_embedding_score=getattr(args, "min_embedding_score", 0.3), ) elif args.command == "grade": stats = grade_annotations(args.input) print("\n Annotation Statistics:") print(f" Total queries: {stats['total_queries']}") print(f" Total candidates: {stats['total_candidates']}") print(f" Graded: {stats['graded_candidates']}") print(f" Ungraded: {stats['ungraded_candidates']}") print(f" Completion rate: {stats['completion_rate']:.1%}") print("\nðŸ“ˆ Relevance Distribution:") for rel in sorted(stats["relevance_distribution"].keys()): count = stats["relevance_distribution"][rel] print(f" {rel}: {count} candidates") if stats["validation_errors"]: print(f"\nWarning: Validation Errors ({len(stats['validation_errors'])}):") for error in stats["validation_errors"][:10]: print(f" - {error}") if len(stats["validation_errors"]) > 10: print(f" ... and {len(stats['validation_errors']) - 10} more") if stats["ungraded_candidates"] > 0: print(f"\nError: Incomplete: {stats['ungraded_candidates']} candidates still need grading") return 1 elif stats["validation_errors"]: print(f"\nWarning: Has validation errors, but all candidates graded") return 1 else: print(f"\nâœ“ All annotations complete and valid!") return 0 elif args.command == "merge": merge_to_test_set(args.input, args.test_set, args.output) else: parser.print_help() return 1 return 0 if __name__ == "__main__": import sys sys.exit(main())