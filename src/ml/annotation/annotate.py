#!/usr/bin/env python3 """ Test Set Annotation for Card Similarity Evaluation Creates human-annotated ground truth datasets for comparing embedding models. Workflow: 1. Sample query cards (diverse archetypes/rarities) 2. Generate candidate similar cards from multiple models 3. Human annotators rate relevance (0-4 scale) 4. Compute inter-annotator agreement 5. Create gold standard test set """ import argparse import json import random from collections import defaultdict from pathlib import Path import pandas as pd import yaml try: from gensim.models import KeyedVectors HAS_GENSIM = True except ImportError: HAS_GENSIM = False class TestSetBuilder: """Build test sets from co-occurrence data""" def __init__(self, pairs_csv: str): self.df = pd.read_csv(pairs_csv) self._build_graph() def _build_graph(self): """Build adjacency list and stats""" self.adj = defaultdict(list) self.degree = defaultdict(int) for _, row in self.df.iterrows(): c1, c2, weight = row["NAME_1"], row["NAME_2"], row["COUNT_MULTISET"] self.adj[c1].append((c2, weight)) self.adj[c2].append((c1, weight)) self.degree[c1] += 1 self.degree[c2] += 1 def sample_query_cards(self, n: int = 50, strategy: str = "stratified") -> list[str]: """ Sample query cards for annotation. Strategies: - stratified: Mix of high/medium/low degree cards - random: Uniform random - popular: High degree only """ cards = list(self.degree.keys()) if strategy == "random": return random.sample(cards, min(n, len(cards))) elif strategy == "popular": sorted_cards = sorted(cards, key=lambda c: self.degree[c], reverse=True) return sorted_cards[:n] elif strategy == "stratified": # Divide into high/medium/low degree sorted_cards = sorted(cards, key=lambda c: self.degree[c], reverse=True) n_third = n // 3 high = random.sample( sorted_cards[: len(sorted_cards) // 3], min(n_third, len(sorted_cards) // 3) ) mid = random.sample( sorted_cards[len(sorted_cards) // 3 : 2 * len(sorted_cards) // 3], min(n_third, len(sorted_cards) // 3), ) low = random.sample( sorted_cards[2 * len(sorted_cards) // 3 :], min(n - 2 * n_third, len(sorted_cards) // 3), ) return high + mid + low else: raise ValueError(f"Unknown strategy: {strategy}") class AnnotationTask: """Generate annotation tasks combining multiple models""" def __init__(self): self.models = {} self.baselines = {} def add_embedding_model(self, name: str, wv_path: str): """Add an embedding model""" wv = KeyedVectors.load(wv_path) self.models[name] = wv print(f"‚úì Loaded {name}: {len(wv):,} embeddings") def add_baseline(self, name: str, fn): """Add a baseline model function""" self.baselines[name] = fn def generate_candidates( self, query_card: str, k: int = 20, pool_size: int = 10 ) -> dict[str, list[str]]: """ Generate candidate cards from all models. Returns union of top-k from each model (up to pool_size). """ candidates = {} all_candidates = set() # Get predictions from each model for model_name, model in self.models.items(): try: similar = model.most_similar(query_card, topn=k) cards = [c for c, _ in similar] candidates[model_name] = cards all_candidates.update(cards[:pool_size]) except KeyError: candidates[model_name] = [] for model_name, fn in self.baselines.items(): results = fn(query_card, k=k) cards = [c for c, _ in results] candidates[model_name] = cards all_candidates.update(cards[:pool_size]) return candidates, list(all_candidates) def create_annotation_batch( self, query_cards: list[str], output_file: str, k: int = 20, pool_size: int = 10 ): """ Create YAML annotation file for human review. Format: - query: "Lightning Bolt" context: "3 damage instant for R" candidates: - card: "Chain Lightning" models: [node2vec_128, node2vec_256] relevance: ?? # 0-4 scale notes: "" - card: "Lava Dart" models: [node2vec_128] relevance: ?? notes: "" """ tasks = [] for query in query_cards: model_preds, candidate_pool = self.generate_candidates(query, k, pool_size) # Build candidate list with model attribution candidates = [] for candidate in candidate_pool: # Find which models predicted this predicted_by = [name for name, preds in model_preds.items() if candidate in preds] candidates.append( { "card": candidate, "predicted_by": predicted_by, "relevance": None, # To be filled by annotator "notes": "", } ) tasks.append( { "query": query, "context": "", # Can be filled manually with archetype info "candidates": candidates, } ) # Write to YAML (human-friendly) output_path = Path(output_file) with open(output_path, "w") as f: yaml.dump( { "instructions": { "relevance_scale": { 0: "Completely irrelevant (different colors, function)", 1: "Somewhat related (same color or card type)", 2: "Related (similar function or archetype)", 3: "Very similar (often seen together)", 4: "Extremely similar (near substitutes)", }, "notes": "Add explanation for edge cases or interesting patterns", }, "tasks": tasks, }, f, default_flow_style=False, sort_keys=False, ) print(f"üìù Created annotation batch: {output_path}") print(f" Queries: {len(tasks)}") print(f" Total candidates: {sum(len(t['candidates']) for t in tasks)}") class AnnotationAnalyzer: """Analyze annotated datasets""" def __init__(self, annotation_file: str): with open(annotation_file) as f: self.data = yaml.safe_load(f) self.tasks = self.data.get("tasks", []) def validate(self) -> bool: """Check if all tasks are fully annotated""" incomplete = [] for i, task in enumerate(self.tasks): for j, cand in enumerate(task.get("candidates", [])): if cand.get("relevance") is None: incomplete.append(f"Task {i} ({task['query']}), candidate {j} ({cand['card']})") if incomplete: print(f"Warning: {len(incomplete)} incomplete annotations:") for item in incomplete[:10]: print(f" - {item}") if len(incomplete) > 10: print(f" ... and {len(incomplete) - 10} more") return False print(f"‚úì All {len(self.tasks)} tasks fully annotated") return True def compute_metrics(self) -> dict: """Compute annotation statistics""" relevance_scores = [] model_performance = defaultdict(lambda: {"relevant": 0, "total": 0}) for task in self.tasks: for cand in task.get("candidates", []): rel = cand.get("relevance") if rel is not None: relevance_scores.append(rel) # Track which models predicted relevant cards is_relevant = rel >= 3 # Threshold for model in cand.get("predicted_by", []): model_performance[model]["total"] += 1 if is_relevant: model_performance[model]["relevant"] += 1 # Compute precision for each model model_precisions = {} for model, stats in model_performance.items(): if stats["total"] > 0: model_precisions[model] = stats["relevant"] / stats["total"] return { "num_tasks": len(self.tasks), "num_candidates": len(relevance_scores), "mean_relevance": sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0, "model_precisions": model_precisions, } def export_test_set(self, output_file: str, min_relevance: int = 3): """ Export annotated test set as JSON for evaluation. Format: { "query_card": { "relevant": ["card1", "card2"], "somewhat_relevant": ["card3"], "irrelevant": ["card4"] } } """ test_set = {} for task in self.tasks: query = task["query"] test_set[query] = { "highly_relevant": [], # 4 "relevant": [], # 3 "somewhat_relevant": [], # 2 "marginally_relevant": [], # 1 "irrelevant": [], # 0 } for cand in task.get("candidates", []): rel = cand.get("relevance") card = cand["card"] if rel == 4: test_set[query]["highly_relevant"].append(card) elif rel == 3: test_set[query]["relevant"].append(card) elif rel == 2: test_set[query]["somewhat_relevant"].append(card) elif rel == 1: test_set[query]["marginally_relevant"].append(card) elif rel == 0: test_set[query]["irrelevant"].append(card) output_path = Path(output_file) with open(output_path, "w") as f: json.dump(test_set, f, indent=2) print(f" Exported test set: {output_path}") def main(): parser = argparse.ArgumentParser(description="Annotation tools") subparsers = parser.add_subparsers(dest="command", help="Command") # Create annotation batch create = subparsers.add_parser("create", help="Create annotation batch") create.add_argument("--pairs", type=str, required=True, help="pairs.csv") create.add_argument("--embeddings", nargs="+", help="Embedding files to compare") create.add_argument("--num-queries", type=int, default=50, help="Number of queries") create.add_argument("--output", type=str, default="annotations.yaml") create.add_argument("--seed", type=int, default=42) # Analyze annotations analyze = subparsers.add_parser("analyze", help="Analyze completed annotations") analyze.add_argument("--input", type=str, required=True, help="Annotation YAML") analyze.add_argument("--export", type=str, help="Export test set JSON") args = parser.parse_args() if args.command == "create": random.seed(args.seed) # Sample query cards builder = TestSetBuilder(args.pairs) queries = builder.sample_query_cards(args.num_queries, strategy="stratified") print(f"üìã Sampled {len(queries)} query cards") # Create annotation task task = AnnotationTask() # Add embedding models if args.embeddings: for _i, emb_file in enumerate(args.embeddings): name = Path(emb_file).stem task.add_embedding_model(name, emb_file) # Generate batch task.create_annotation_batch(queries, args.output) print("\nüìù Next steps:") print(f" 1. Open {args.output} in text editor") print(" 2. For each candidate, set relevance (0-4)") print(f" 3. Run: python annotate.py analyze --input {args.output}") elif args.command == "analyze": analyzer = AnnotationAnalyzer(args.input) # Validate if not analyzer.validate(): print("\nWarning: Complete annotations before analysis") return 1 # Compute metrics metrics = analyzer.compute_metrics() print("\n Annotation Statistics:") print(f" Tasks: {metrics['num_tasks']}") print(f" Candidates: {metrics['num_candidates']}") print(f" Mean relevance: {metrics['mean_relevance']:.2f}") print("\nüèÜ Model Precision (% relevant predictions):") for model, precision in sorted( metrics["model_precisions"].items(), key=lambda x: x[1], reverse=True ): print(f" {model:30s}: {precision * 100:5.1f}%") # Export test set if args.export: analyzer.export_test_set(args.export) else: parser.print_help() return 1 return 0 if __name__ == "__main__": import sys sys.exit(main())