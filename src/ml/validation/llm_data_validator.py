#!/usr/bin/env python3 """ LLM Data Quality Auditor (OPTIONAL BATCH VALIDATION) Uses LLMs for ABSTRACT/SEMANTIC validation that deterministic rules can't catch: 1. Deck metadata consistency (archetype matches cards) 2. Card relationships (do co-occurring cards make sense?) 3. Semantic anomalies and outliers Warning: DO NOT USE THIS FOR: - Ban list checking (use validators.legality instead - LLMs hallucinate) - Format legality (use deterministic rules) - Deck construction rules (use Pydantic models) âœ“ USE THIS FOR: - Post-hoc data quality audits - Finding mislabeled archetypes - Discovering suspicious card pairs - Quality scoring for human review This is EXPENSIVE and SLOW - run as optional batch process, not in ML pipeline. """ import asyncio import json import logging import os from collections import Counter, defaultdict from datetime import datetime from pathlib import Path logger = logging.getLogger(__name__) __all__ = ["DataQualityValidator", "HAS_PYDANTIC_AI"] # Auto-load .env to pick up provider keys try: from dotenv import load_dotenv # type: ignore load_dotenv() except ImportError: # dotenv not critical, can work without it if env vars set pass try: from pydantic import BaseModel, Field from pydantic_ai import Agent HAS_PYDANTIC_AI = True except ImportError: HAS_PYDANTIC_AI = False print("Install pydantic-ai: pip install pydantic-ai") from ..utils.paths import PATHS # ============================================================================ # Validation Models # ============================================================================ class ArchetypeValidation(BaseModel): """LLM validation of archetype label.""" deck_id: str claimed_archetype: str top_cards: list[str] is_consistent: bool = Field(description="Does archetype match cards?") confidence: float = Field(ge=0.0, le=1.0, description="Confidence 0-1") issues: list[str] = Field(default_factory=list, description="Problems found") suggested_archetype: str | None = Field(None, description="Better label if wrong") reasoning: str = Field(description="Why consistent or not") class CardRelationshipValidation(BaseModel): """LLM validation of card pair relationship.""" card1: str card2: str archetype_context: str makes_sense: bool = Field(description="Do these cards logically co-occur?") confidence: float = Field(ge=0.0, le=1.0) relationship_type: str = Field(description="synergy|manabase|staples|suspicious") reasoning: str class DeckCoherenceValidation(BaseModel): """LLM validation of deck coherence (do cards work together?).""" deck_id: str archetype: str format: str sample_cards: list[str] is_coherent: bool = Field(description="Do cards form a coherent strategy?") confidence: float = Field(ge=0.0, le=1.0) issues: list[str] = Field(default_factory=list, description="Coherence problems") reasoning: str class DeckQualityAssessment(BaseModel): """Overall deck quality assessment.""" deck_id: str archetype: str format: str quality_score: float = Field(ge=0.0, le=1.0, description="Overall quality 0-1") is_tournament_viable: bool is_coherent: bool = Field(description="Cards work together?") issues: list[str] = Field(default_factory=list) recommendations: list[str] = Field(default_factory=list) # ============================================================================ # Validation Agents # ============================================================================ if HAS_PYDANTIC_AI: from ..utils.pydantic_ai_helpers import make_agent VAL_MODEL_ARCH = os.getenv("VALIDATOR_MODEL_ARCHETYPE", "anthropic/claude-4.5-sonnet") VAL_MODEL_REL = os.getenv("VALIDATOR_MODEL_RELATIONSHIP", "anthropic/claude-4.5-sonnet") VAL_MODEL_COH = os.getenv("VALIDATOR_MODEL_COHERENCE", "anthropic/claude-4.5-sonnet") # Archetype validator (using OpenRouter) archetype_agent = make_agent( VAL_MODEL_ARCH, ArchetypeValidation, """You are an expert TCG judge validating deck archetypes. Your task: Check if the claimed archetype matches the actual cards in the deck. Common archetypes: - Aggro: Fast, creature-heavy, low curve - Control: Counters, removal, card draw, finishers - Combo: Cards that win with specific interactions - Midrange: Efficient threats and answers - Ramp: Mana acceleration into big spells Be strict but fair. Flag inconsistencies like: - "Aggro" with no creatures - "Control" with no interaction - "Combo" with no combo pieces - Generic names that don't match strategy Consider format context (Modern, Legacy, Pauper have different staples).""", ) # Card relationship validator (using OpenRouter) relationship_agent = make_agent( VAL_MODEL_REL, CardRelationshipValidation, """You are an expert TCG judge validating card relationships. Your task: Assess if two cards logically appear together in decks. Valid reasons to co-occur: - Synergy: Cards that combo or enhance each other - Manabase: Lands that support same colors - Staples: Both are format staples - Archetype: Both fit same strategy Suspicious patterns: - Completely different strategies (aggro + control) - Color conflicts (RR cost + UUU cost with no fixing) - Different eras/formats - Random junk (might be data error) Consider the archetype context. Cards that seem odd together might make sense in specific decks.""", ) # Deck coherence validator (using OpenRouter) coherence_agent = make_agent( VAL_MODEL_COH, DeckCoherenceValidation, """You are an expert TCG deck analyst. Your task: Assess if a deck's cards form a coherent, logical strategy. Good signs: - Cards have clear synergies - Mana curve makes sense for archetype - Removal/interaction fits strategy - Win conditions are present Bad signs: - Random, unrelated cards - No clear game plan - Mismatch between aggressive and defensive cards - Color fixing insufficient for mana costs This is NOT about format legality (that's checked elsewhere). Focus on whether the deck makes strategic sense.""", ) # ============================================================================ # Batch Validators # ============================================================================ class DataQualityValidator: """Orchestrates LLM validation at scale.""" def __init__(self, output_dir: Path | None = None): if not HAS_PYDANTIC_AI: raise ImportError("pydantic-ai required") self.output_dir = output_dir or PATHS.experiments / "data_quality" self.output_dir.mkdir(exist_ok=True, parents=True) # Load decks self.decks = self._load_decks() logger.info(f"Loaded {len(self.decks)} decks for validation") def _load_decks(self) -> list[dict]: """Load decks with metadata and validation.""" from .validators.loader import load_decks_lenient validated_decks = load_decks_lenient( PATHS.decks_with_metadata, game="auto", check_legality=False, verbose=False, ) # Convert to dict for LLM processing return [d.model_dump() for d in validated_decks] async def validate_archetype_sample(self, sample_size: int = 50) -> list[ArchetypeValidation]: """Validate archetype labels on random sample.""" import random sample = random.sample(self.decks, min(sample_size, len(self.decks))) logger.info(f"Validating {len(sample)} archetypes...") results = [] for i, deck in enumerate(sample, 1): if i % 10 == 0: logger.debug(f" {i}/{len(sample)}...") # Get top cards (most characteristic) cards = [c["name"] for c in deck.get("cards", [])] top_cards = cards[:15] # First 15 (usually most important) prompt = f""" Deck ID: {deck["deck_id"]} Claimed Archetype: {deck.get("archetype", "Unknown")} Format: {deck.get("format", "Unknown")} Top Cards: {", ".join(top_cards)} Does this archetype label accurately describe these cards? """ try: result = await archetype_agent.run(prompt) results.append(result.output) except (KeyError, ValueError, TypeError, RuntimeError) as e: print(f" Error on {deck['deck_id']}: {e}") continue return results async def validate_card_relationships( self, archetype: str, sample_size: int = 30 ) -> list[CardRelationshipValidation]: """Validate card pairs within an archetype.""" # Get decks of this archetype arch_decks = [d for d in self.decks if d.get("archetype") == archetype] if not arch_decks: print(f"No decks found for archetype: {archetype}") return [] print(f"\nValidating {sample_size} card relationships in {archetype}...") # Find common pairs pair_counts = Counter() for deck in arch_decks: cards = [c["name"] for c in deck.get("cards", [])] for i, c1 in enumerate(cards): for c2 in cards[i + 1 :]: pair = tuple(sorted([c1, c2])) pair_counts[pair] += 1 # Sample frequent pairs common_pairs = pair_counts.most_common(sample_size) results = [] for i, ((card1, card2), count) in enumerate(common_pairs, 1): if i % 5 == 0: print(f" {i}/{len(common_pairs)}...") prompt = f""" Card 1: {card1} Card 2: {card2} Archetype Context: {archetype} Co-occurrence: {count} decks Do these cards logically appear together in {archetype} decks? What's the relationship? """ try: result = await relationship_agent.run(prompt) results.append(result.output) except Exception as e: # Catch all - network, API, validation errors logger.error(f"Error on {card1} + {card2}: {e}") continue return results async def validate_deck_coherence( self, sample_size: int = 50 ) -> list[DeckCoherenceValidation]: """Check if decks have coherent strategies.""" import random sample = random.sample(self.decks, min(sample_size, len(self.decks))) print(f"\nValidating {len(sample)} decks for strategic coherence...") results = [] for i, deck in enumerate(sample, 1): if i % 10 == 0: print(f" {i}/{len(sample)}...") cards = [c["name"] for c in deck.get("cards", [])] sample_cards = cards[:20] # First 20 cards as representative sample prompt = f""" Deck ID: {deck["deck_id"]} Archetype: {deck.get("archetype", "Unknown")} Format: {deck.get("format", "Unknown")} Sample Cards: {", ".join(sample_cards)} Does this deck have a coherent strategy? Do the cards work together logically? Consider: synergies, game plan, curve, interaction. """ try: result = await coherence_agent.run(prompt) results.append(result.output) except Exception as e: # Catch all - network, API, validation errors logger.error(f"Error on {deck['deck_id']}: {e}") continue return results def generate_report( self, archetype_results: list[ArchetypeValidation], relationship_results: dict[str, list[CardRelationshipValidation]], coherence_results: list[DeckCoherenceValidation], ): """Generate comprehensive quality report.""" timestamp = datetime.now().strftime("%Y%m%d_%H%M%S") report_file = self.output_dir / f"quality_report_{timestamp}.json" # Analyze archetype validation archetype_stats = { "total": len(archetype_results), "consistent": sum(1 for r in archetype_results if r.is_consistent), "inconsistent": sum(1 for r in archetype_results if not r.is_consistent), "avg_confidence": sum(r.confidence for r in archetype_results) / len(archetype_results) if archetype_results else 0, "issues_by_archetype": defaultdict(list), } for result in archetype_results: if not result.is_consistent: archetype_stats["issues_by_archetype"][result.claimed_archetype].append( { "deck_id": result.deck_id, "issues": result.issues, "suggested": result.suggested_archetype, } ) # Analyze card relationships relationship_stats = {} for archetype, results in relationship_results.items(): relationship_stats[archetype] = { "total_pairs": len(results), "makes_sense": sum(1 for r in results if r.makes_sense), "suspicious": sum(1 for r in results if not r.makes_sense), "avg_confidence": sum(r.confidence for r in results) / len(results) if results else 0, "suspicious_pairs": [ {"cards": [r.card1, r.card2], "reason": r.reasoning} for r in results if not r.makes_sense ], } # Analyze deck coherence coherence_stats = { "total": len(coherence_results), "coherent": sum(1 for r in coherence_results if r.is_coherent), "incoherent": sum(1 for r in coherence_results if not r.is_coherent), "avg_confidence": sum(r.confidence for r in coherence_results) / len(coherence_results) if coherence_results else 0, "issues": [ {"deck_id": r.deck_id, "problems": r.issues} for r in coherence_results if not r.is_coherent ], } report = { "timestamp": timestamp, "dataset": {"total_decks": len(self.decks), "decks_validated": len(archetype_results)}, "archetype_validation": archetype_stats, "card_relationships": relationship_stats, "deck_coherence": coherence_stats, "summary": { "data_quality_score": self._compute_quality_score( archetype_stats, relationship_stats, coherence_stats ), "critical_issues": self._identify_critical_issues( archetype_results, relationship_results, coherence_results ), "recommendations": self._generate_recommendations( archetype_stats, relationship_stats, coherence_stats ), }, "note": "This is a SEMANTIC audit - use validators.legality for ban lists", } with open(report_file, "w") as f: json.dump(report, f, indent=2, default=str) print(f"\n{'=' * 60}") print("DATA QUALITY REPORT") print("=" * 60) print("\nArchetype Validation:") print( f" Consistent: {archetype_stats['consistent']}/{archetype_stats['total']} ({archetype_stats['consistent'] / archetype_stats['total'] * 100:.1f}%)" ) print(f" Avg Confidence: {archetype_stats['avg_confidence']:.3f}") print("\nCard Relationships:") for arch, stats in relationship_stats.items(): print( f" {arch}: {stats['makes_sense']}/{stats['total_pairs']} valid ({stats['makes_sense'] / stats['total_pairs'] * 100:.1f}%)" ) print("\nDeck Coherence:") print( f" Coherent: {coherence_stats['coherent']}/{coherence_stats['total']} ({coherence_stats['coherent'] / coherence_stats['total'] * 100:.1f}%)" ) print(f" Avg Confidence: {coherence_stats['avg_confidence']:.3f}") print(f"\nQuality Score: {report['summary']['data_quality_score']:.2f}/1.0") print(f"\nReport saved: {report_file}") return report def _compute_quality_score(self, arch_stats, rel_stats, coh_stats) -> float: """Compute overall data quality score.""" scores = [] # Archetype consistency if arch_stats["total"] > 0: scores.append(arch_stats["consistent"] / arch_stats["total"]) # Relationship validity for stats in rel_stats.values(): if stats["total_pairs"] > 0: scores.append(stats["makes_sense"] / stats["total_pairs"]) # Deck coherence if coh_stats["total"] > 0: scores.append(coh_stats["coherent"] / coh_stats["total"]) return sum(scores) / len(scores) if scores else 0.0 def _identify_critical_issues(self, arch_res, rel_res, coh_res) -> list[str]: """Identify critical data quality issues.""" issues = [] # High rate of archetype mislabeling total = len(arch_res) inconsistent = sum(1 for r in arch_res if not r.is_consistent) if inconsistent / total > 0.2: issues.append( f"High archetype mislabeling: {inconsistent}/{total} ({inconsistent / total * 100:.0f}%)" ) # Deck coherence issues if coh_res: incoherent = sum(1 for r in coh_res if not r.is_coherent) if incoherent / len(coh_res) > 0.2: issues.append( f"High incoherence rate: {incoherent}/{len(coh_res)} ({incoherent / len(coh_res) * 100:.0f}%)" ) return issues def _generate_recommendations(self, arch_stats, rel_stats, coh_stats) -> list[str]: """Generate actionable recommendations.""" recs = [] # Archetype cleanup if arch_stats["inconsistent"] > 10: recs.append("Review and relabel inconsistent archetypes") recs.append(f"Focus on: {list(arch_stats['issues_by_archetype'].keys())[:3]}") # Relationship validation for arch, stats in rel_stats.items(): if stats["suspicious"] > 5: recs.append(f"Investigate suspicious pairs in {arch}") # Deck coherence if coh_stats["incoherent"] > 10: recs.append("Investigate incoherent decks - may indicate scraping errors") return recs # ============================================================================ # CLI # ============================================================================ async def main(): """Run validation pipeline.""" if not HAS_PYDANTIC_AI: print("Error: pydantic-ai not installed") print("Install with: pip install pydantic-ai") return if not os.getenv("OPENROUTER_API_KEY"): print("Error: OPENROUTER_API_KEY not set") print("Set in .env file or: export OPENROUTER_API_KEY=your-key") return # Load .env if it exists from pathlib import Path env_file = Path(__file__).parent.parent.parent / ".env" if env_file.exists(): from dotenv import load_dotenv load_dotenv(env_file) validator = DataQualityValidator() # 1. Validate archetypes (50 random samples) print("\n" + "=" * 60) print("PHASE 1: Archetype Validation") print("=" * 60) archetype_results = await validator.validate_archetype_sample(sample_size=50) # 2. Validate card relationships (top 3 archetypes) print("\n" + "=" * 60) print("PHASE 2: Card Relationship Validation") print("=" * 60) # Get top archetypes arch_counts = Counter(d.get("archetype") for d in validator.decks) top_archetypes = [arch for arch, _ in arch_counts.most_common(3)] relationship_results = {} for archetype in top_archetypes: results = await validator.validate_card_relationships(archetype, sample_size=20) relationship_results[archetype] = results # 3. Validate deck coherence print("\n" + "=" * 60) print("PHASE 3: Deck Coherence Validation") print("=" * 60) coherence_results = await validator.validate_deck_coherence(sample_size=30) # 4. Generate report print("\n" + "=" * 60) print("PHASE 4: Generating Report") print("=" * 60) validator.generate_report(archetype_results, relationship_results, coherence_results) print("\n Semantic audit complete!") print("\nWarning: Note: This was a SEMANTIC audit. Run validators.legality for ban lists.") if __name__ == "__main__": asyncio.run(main())
