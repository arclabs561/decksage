# Next Steps - Focused Action Plan

**Generated:** October 1, 2025  
**Status:** B+ (8.5/10) - Solid foundation, ready to validate

## ğŸ¯ Core Questions (Answer These First)

### 1. **Primary Use Case**
What problem are we solving?

**Options:**
- **A)** Deck building aid (suggest alternatives/upgrades)
- **B)** Meta analysis tool (track archetype trends)  
- **C)** Budget optimizer (find cheaper alternatives)
- **D)** Learning tool (understand card relationships)

**Why it matters:** Defines success metrics and evaluation strategy

---

### 2. **Success Metric**
How do we know it works?

**Options:**
- **A)** Expert validation (3+ MTG experts agree â‰¥70% of top-5)
- **B)** Click-through rate (users click â‰¥50% of recommendations)
- **C)** Deck win-rate (suggested cards improve deck performance)
- **D)** Precision@10 â‰¥ 0.7 (vs annotated ground truth)

**Why it matters:** Without this, we're guessing if quality is good

---

### 3. **Scope: Single vs Multi-Game**
MTG-only or actually multi-game?

**Current state:**
- Architecture: âœ… Multi-game ready
- Data: âš ï¸ Only MTG decks (YGO/Pokemon have cards, no decks)

**Options:**
- **A)** Focus MTG, make it excellent (simpler, faster to market)
- **B)** Add YGO/Pokemon deck scraping (validate multi-game claim)

**Why it matters:** Affects data collection priorities

---

## ğŸ”´ Immediate Actions (Do Today)

### Action 1: Validate End-to-End Pipeline
```bash
# Training is running in background...
# Once complete, test the full flow:

cd src/ml
source .venv/bin/activate

# Check if training succeeded
ls -lh ../../data/embeddings/magic_production_pecanpy.wv

# Test API
python api.py --embeddings ../../data/embeddings/magic_production_pecanpy.wv &
sleep 2

# Test similarity search
curl -X POST http://localhost:8000/similar \
  -H "Content-Type: application/json" \
  -d '{"query": "Lightning Bolt", "top_k": 5}'

# Open debug page
open ../debug/similarity-demo.html
```

**Success criteria:** 
- âœ… Training completes without errors
- âœ… API returns results
- âœ… Recommendations look reasonable

---

### Action 2: Manual Quality Check (YOU Review)

**Open:** `debug/similarity-demo.html`

**Test queries:**
1. Lightning Bolt â†’ Should find: Lava Spike, Chain Lightning, Rift Bolt
2. Counterspell â†’ Should find: Mana Leak, Force of Will, Archmage's Charm
3. Dark Ritual â†’ Should find: Cabal Ritual, Lotus Petal, Dark Petition
4. Brainstorm â†’ Should find: Ponder, Preordain, Portent

**What to check:**
- âŒ **Bad:** Totally unrelated cards (different colors/function)
- âš ï¸ **Questionable:** Same color but different role
- âœ… **Good:** Functional similar or often played together
- âœ… **Excellent:** Near-substitutes or archetype staples

**Document findings** â†’ Informs if we need better data/different model

---

### Action 3: Answer The Three Questions

Create `VISION.md`:
```markdown
# DeckSage Vision

## Primary Use Case
[Your answer: A/B/C/D]

## Success Metric
[Your answer: A/B/C/D]

## Scope
[Your answer: A/B]

## Target Users
[Write 2-3 user stories]
```

---

## ğŸŸ¡ This Week (Once Basics Work)

### If Quality is Good (recommendations make sense):

**4. Create first annotation batch**
```bash
python annotate.py create \
  --pairs ../../data/processed/pairs.csv \
  --embeddings ../../data/embeddings/magic_production_pecanpy.wv \
  --num-queries 20 \
  --output ../../experiments/annotations/batch1.yaml
```

**5. Annotate yourself** (or recruit MTG player)
- Open `batch1.yaml`
- Rate each recommendation 0-4
- Takes ~30-60 minutes

**6. Measure metrics**
```bash
python compare_models.py \
  --test-set ../../experiments/batch1.json \
  --models ../../data/embeddings/*.wv \
  --output ../../experiments/results.csv
```

### If Quality is Bad (weird recommendations):

**4a. Diagnose data issues**
- Check format distribution (Modern vs Legacy vs Pauper)
- Look for edge contamination
- Verify key cards are present

**4b. Extract more balanced data**
```bash
cd src/backend
go run cmd/dataset/main.go extract magic mtgtop8 --section Modern --limit 100
go run cmd/dataset/main.go transform magic pairs
```

**4c. Re-train with better data**

---

## ğŸŸ¢ Next 2 Weeks (Production Path)

**7. Add Python tests**
```bash
mkdir src/ml/tests
# test_api.py, test_evaluate.py, test_annotate.py
pytest src/ml/tests/
```

**8. Build frontend integration**
- Connect React app to API
- Card search with autocomplete
- Display similarity results

**9. Deploy**
- Docker compose (API + Redis)
- Simple frontend hosting
- Domain setup

**10. Monitor & iterate**
- Track which searches are used
- Measure click-through rates
- Refine based on usage

---

## ğŸ¯ Decision Tree

```
START
  â†“
Run end-to-end test â†’ Does it work?
  â”œâ”€ NO â†’ Fix dependencies/paths â†’ Retry
  â””â”€ YES â†’ Are recommendations good?
       â”œâ”€ NO â†’ Extract better data â†’ Retrain
       â””â”€ YES â†’ Create annotations â†’ Measure metrics
              â†“
              Metrics â‰¥ target?
              â”œâ”€ NO â†’ Tune hyperparameters â†’ Retry
              â””â”€ YES â†’ Add tests â†’ Deploy â†’ Monitor
```

---

## ğŸ“Š Current Status vs Vision

| Component | Status | Vision Gap |
|-----------|--------|------------|
| **Scraping** | âœ… Working (MTG) | âš ï¸ YGO/Pokemon decks missing |
| **Graph** | âœ… Co-occurrence | â“ Other graph types? |
| **Embeddings** | âœ… Node2Vec | â“ Try alternatives? (GNN, transformers) |
| **Evaluation** | âœ… Metrics ready | âš ï¸ No ground truth yet |
| **API** | âœ… Code ready | âš ï¸ Not tested |
| **Frontend** | âš ï¸ React shell | âŒ No integration |
| **Multi-game** | âœ… Architecture | âš ï¸ MTG data only |

---

## ğŸš€ The Path Forward

### Today (2-3 hours):
1. âœ… Training running (background)
2. â³ Wait for results
3. ğŸ” Manual quality check via HTML page
4. ğŸ“ Answer 3 core questions

### This Week (5-10 hours):
5. Create first annotation batch
6. Annotate + measure metrics
7. Decide: Good enough? Need better data?

### Next 2 Weeks (20-30 hours):
8. Add tests
9. Frontend integration
10. Deploy MVP

---

## ğŸ’¡ Key Insight

**Stop here if embeddings look bad.** No point building infrastructure on broken foundation.

**The HTML demo page is your quality gate** - if recommendations don't make sense to you (a human with domain knowledge), no amount of metrics will save it.

**Next immediate step:** Wait for training to complete (~5-10 minutes), then open `debug/similarity-demo.html` and judge for yourself.

---

## ğŸ“‹ Checklist Before Moving Forward

- [ ] Training completes successfully
- [ ] API serves requests
- [ ] You review 5-10 similarity searches
- [ ] Recommendations make intuitive sense
- [ ] You define primary use case
- [ ] You define success metric

**Only proceed if all checkboxes pass.**

